---
issue: 47
agent: PLAN
date: 2026-02-14
complexity: COMPLEX
stack: backend
package: ingestkit-pdf
---

# PLAN: Issue #47 â€” Integration Test Suite and Benchmark Report

## Executive Summary

Implement a three-file deliverable for ingestkit-pdf Phase 2 gate: (1) integration tests exercising the full PDFRouter.process() pipeline with real backends, (2) benchmark tests with SLO assertions from SPEC section 25, and (3) a standalone benchmark script producing JSON reports. All tests reuse the programmatic PDF fixtures already in conftest.py and skip gracefully when backends (Qdrant, Ollama) are unavailable. The router currently imports backends from `ingestkit_excel.backends` (see `router.py:918-923`), so integration tests must use those same concrete backends.

---

## Investigation Findings

### What Exists
- **conftest.py**: 5 programmatic PDF fixtures (`text_native_pdf`, `scanned_pdf`, `complex_pdf`, `encrypted_pdf`, `garbled_pdf`) + 4 mock backends + factory helpers
- **pyproject.toml**: `integration` marker already registered; no `benchmark` marker yet
- **router.py**: `PDFRouter.process()` is the main entry point; `create_default_router()` factory imports from `ingestkit_excel.backends` (Qdrant, SQLite, Ollama)
- **SPEC section 22.5**: Phase 2 Gate requires integration tests pass + benchmark report meets throughput targets
- **SPEC section 25**: Full SLO definitions including per-stage latency budgets and throughput targets

### What Does NOT Exist
- `test_integration.py` -- needs creation
- `test_benchmark.py` -- needs creation
- `scripts/benchmark.py` -- needs creation; `scripts/` directory does not exist
- `benchmark` pytest marker -- not registered

### Key Architecture Notes
- `create_default_router()` imports from `ingestkit_excel.backends` -- this is a coupling point. Integration tests should construct backends directly rather than relying on this factory.
- Path A = TextExtractor (text_native_pdf fixture), Path B = OCRProcessor (scanned_pdf fixture), Path C = ComplexProcessor (complex_pdf fixture) -- but `_complex_processor` is `None` in current router, so Path C will return error.
- Real backends: `QdrantVectorStore(url="http://localhost:6333")`, `SQLiteStructuredDB(":memory:")`, `OllamaLLM(base_url="http://localhost:11434")`, `OllamaEmbedding(base_url="http://localhost:11434")`
- SLO targets (SPEC 25.2): Path A >= 50 pages/sec, Path B >= 10 pages/sec on laptop hardware
- Per-stage latency budgets (SPEC 25.4): security < 100ms, profile < 2s, Tier 1 < 200ms

---

## Files to Create/Modify

### File 1: `packages/ingestkit-pdf/tests/test_integration.py` (NEW)

**Purpose**: Full-pipeline integration tests with real backends.

**Dependencies**: `qdrant-client`, `httpx`, `ingestkit_excel.backends`

**Backend Availability Helpers** (module-level):

```python
def _qdrant_available() -> bool:
    """Check if Qdrant is reachable on localhost:6333."""
    try:
        from qdrant_client import QdrantClient
        client = QdrantClient(url="http://localhost:6333", timeout=2)
        client.get_collections()
        return True
    except Exception:
        return False

def _ollama_available() -> bool:
    """Check if Ollama is reachable on localhost:11434."""
    try:
        import httpx
        resp = httpx.get("http://localhost:11434/api/tags", timeout=2)
        return resp.status_code == 200
    except Exception:
        return False

requires_qdrant = pytest.mark.skipif(
    not _qdrant_available(), reason="Qdrant not available on localhost:6333"
)
requires_ollama = pytest.mark.skipif(
    not _ollama_available(), reason="Ollama not available on localhost:11434"
)
requires_backends = pytest.mark.skipif(
    not (_qdrant_available() and _ollama_available()),
    reason="Qdrant and/or Ollama not available"
)
```

**Fixtures** (session-scoped for backend instances):

```python
@pytest.fixture(scope="session")
def real_vector_store():
    """Session-scoped Qdrant backend. Creates test collection, cleans up after."""
    from ingestkit_excel.backends import QdrantVectorStore
    store = QdrantVectorStore(url="http://localhost:6333")
    yield store
    # Cleanup: delete test collections created during tests

@pytest.fixture(scope="session")
def real_structured_db():
    """Session-scoped in-memory SQLite backend."""
    from ingestkit_excel.backends import SQLiteStructuredDB
    return SQLiteStructuredDB(":memory:")

@pytest.fixture(scope="session")
def real_llm():
    """Session-scoped Ollama LLM backend."""
    from ingestkit_excel.backends import OllamaLLM
    return OllamaLLM(base_url="http://localhost:11434")

@pytest.fixture(scope="session")
def real_embedder():
    """Session-scoped Ollama embedding backend."""
    from ingestkit_excel.backends import OllamaEmbedding
    return OllamaEmbedding(base_url="http://localhost:11434", model="nomic-embed-text")

@pytest.fixture()
def integration_router(real_vector_store, real_structured_db, real_llm, real_embedder):
    """PDFRouter wired to real backends with test config."""
    config = PDFProcessorConfig(
        tenant_id="integration-test",
        default_collection="test_integration",
    )
    return PDFRouter(
        vector_store=real_vector_store,
        structured_db=real_structured_db,
        llm=real_llm,
        embedder=real_embedder,
        config=config,
    )
```

**Test Class: `TestPathAIntegration`** (5 tests):

| Test | Fixture | Assertions |
|------|---------|------------|
| `test_path_a_text_native_full_pipeline` | `text_native_pdf` | `errors == []`, `chunks_created > 0`, `classification.pdf_type == PDFType.TEXT_NATIVE`, `classification.confidence > 0`, `ingest_key` is 64-char hex, `processing_time_seconds > 0`, `ingestion_method == TEXT_EXTRACTION` |
| `test_path_a_written_artifacts` | `text_native_pdf` | `written.vector_point_ids` non-empty, `written.vector_collection == "test_integration"` |
| `test_path_a_stage_results` | `text_native_pdf` | `parse_result.pages_extracted == 3`, `parse_result.extraction_method == "pymupdf"`, `classification_result.tier_used` is valid enum |
| `test_path_a_idempotency` | `text_native_pdf` | Two calls produce same `ingest_key` |
| `test_path_a_tenant_propagation` | `text_native_pdf` | `result.tenant_id == "integration-test"` |

**Test Class: `TestPathBIntegration`** (3 tests):

| Test | Fixture | Assertions |
|------|---------|------------|
| `test_path_b_scanned_full_pipeline` | `scanned_pdf` | `errors == []` (or acceptable warnings only), `chunks_created > 0`, `classification.pdf_type == PDFType.SCANNED`, `ingestion_method == OCR_PIPELINE`, `ocr_result is not None` |
| `test_path_b_ocr_stage_result` | `scanned_pdf` | `ocr_result.pages_ocrd == 2`, `ocr_result.engine_used` is valid OCREngine |
| `test_path_b_written_artifacts` | `scanned_pdf` | `written.vector_point_ids` non-empty |

Note: Path B requires Tesseract installed. Add `@pytest.mark.ocr` alongside `@pytest.mark.integration`.

**Test Class: `TestPathCIntegration`** (1 test):

| Test | Fixture | Assertions |
|------|---------|------------|
| `test_path_c_complex_not_available` | `complex_pdf` | Since `ComplexProcessor` is None, assert error result: `"ComplexProcessor not available" in result.errors`. This documents the current state; update when ComplexProcessor is implemented. |

**Test Class: `TestEdgeCaseIntegration`** (3 tests):

| Test | Fixture | Assertions |
|------|---------|------------|
| `test_encrypted_pdf_handling` | `encrypted_pdf` | Should return security error/warning (password-protected), `chunks_created == 0` |
| `test_garbled_pdf_handling` | `garbled_pdf` | Should complete (may have warnings), verify quality-related warnings emitted |
| `test_can_handle_filter` | N/A | `router.can_handle("test.pdf") is True`, `router.can_handle("test.xlsx") is False` |

All tests decorated with:
```python
@pytest.mark.integration
@requires_backends
```

Path B tests additionally decorated with `@pytest.mark.ocr`.

**Cleanup fixture**:
```python
@pytest.fixture(autouse=True)
def _cleanup_qdrant(real_vector_store):
    """Delete test collection after each test to avoid cross-test pollution."""
    yield
    try:
        from qdrant_client import QdrantClient
        client = QdrantClient(url="http://localhost:6333", timeout=2)
        # Delete collections starting with "test_"
        collections = client.get_collections().collections
        for c in collections:
            if c.name.startswith("test_"):
                client.delete_collection(c.name)
    except Exception:
        pass
```

---

### File 2: `packages/ingestkit-pdf/tests/test_benchmark.py` (NEW)

**Purpose**: Timing-based benchmark tests with SLO assertions. Uses `time.monotonic()` -- no `pytest-benchmark` dependency.

**Approach**: Mark all tests as `@pytest.mark.integration` + `@pytest.mark.benchmark` (new marker). Use real backends when available, mock backends for per-stage latency tests that don't need real services.

**Test Class: `TestPerStageBenchmarks`** (3 tests, unit-level with mocks):

These test per-stage latency budgets from SPEC 25.4 using mock backends (no external services needed).

| Test | Target (SPEC 25.4) | Method |
|------|---------------------|--------|
| `test_security_scan_latency` | < 100ms (target), < 500ms (max) | Time `PDFSecurityScanner.scan()` on `text_native_pdf`; assert < 500ms |
| `test_profile_extraction_latency` | < 2s (target), < 5s (max) | Time `PDFRouter._build_document_profile()` on 3-page `text_native_pdf`; assert < 5s |
| `test_tier1_classification_latency` | < 200ms (target), < 500ms (max) | Time `PDFInspector.classify()` on a profile; assert < 500ms |

These are `@pytest.mark.unit` + `@pytest.mark.benchmark` (no external services).

**Test Class: `TestThroughputBenchmarks`** (2 tests, integration-level):

These require real backends and test end-to-end throughput.

| Test | SLO (SPEC 25.2) | Method |
|------|------------------|--------|
| `test_path_a_throughput_slo` | >= 50 pages/sec | Process `text_native_pdf` (3 pages) N=5 times, compute pages/sec = (3*5) / total_time. Assert >= 50. Use `pytest.warns` or just assert with descriptive message. |
| `test_path_b_throughput_slo` | >= 10 pages/sec | Process `scanned_pdf` (2 pages) N=3 times, compute pages/sec = (2*3) / total_time. Assert >= 10. |

Throughput tests decorated with `@pytest.mark.integration` + `@pytest.mark.benchmark` + `@requires_backends`.
Path B additionally `@pytest.mark.ocr`.

**Helper function**:
```python
def _measure_throughput(
    router: PDFRouter,
    pdf_path: Path,
    page_count: int,
    iterations: int,
) -> dict:
    """Run process() N times and compute throughput stats."""
    times = []
    for _ in range(iterations):
        start = time.monotonic()
        result = router.process(str(pdf_path))
        elapsed = time.monotonic() - start
        times.append(elapsed)
        assert not result.errors, f"Processing failed: {result.errors}"

    total_pages = page_count * iterations
    total_time = sum(times)
    return {
        "pages_per_sec": total_pages / total_time,
        "avg_time_sec": total_time / iterations,
        "min_time_sec": min(times),
        "max_time_sec": max(times),
        "total_pages": total_pages,
        "iterations": iterations,
    }
```

---

### File 3: `packages/ingestkit-pdf/scripts/benchmark.py` (NEW)

**Purpose**: Standalone CLI script that runs the full benchmark suite and outputs `benchmark-report-<date>.json`.

**Structure**:
- Uses `argparse` for CLI: `--iterations N` (default 5), `--output-dir DIR` (default `.`), `--paths` (A, B, or both)
- Creates backends via direct construction (same as integration tests)
- Checks backend availability before running
- Runs Path A and Path B benchmarks
- Collects per-stage timing from `ProcessingResult` fields:
  - `parse_result.parse_duration_seconds`
  - `classification_result.classification_duration_seconds`
  - `processing_time_seconds` (total)
  - `ocr_result.ocr_duration_seconds` (Path B only)
  - `embed_result.embed_duration_seconds` (if present)
- Computes percentiles (p50, p95, max) using `statistics` module
- Generates `benchmark-report-<date>.json`

**Output JSON schema**:
```json
{
  "timestamp": "2026-02-14T12:00:00Z",
  "platform": {"os": "...", "python": "...", "cpu_count": N},
  "config": {"iterations": 5, "...": "..."},
  "path_a": {
    "throughput_pages_per_sec": 65.2,
    "slo_target": 50,
    "slo_pass": true,
    "per_stage_latency": {
      "security_scan": {"p50": 0.01, "p95": 0.02, "max": 0.03},
      "profile_extraction": {"p50": 0.5, "p95": 0.8, "max": 1.1},
      "tier1_classification": {"p50": 0.05, "p95": 0.1, "max": 0.15},
      "text_extraction": {"p50": 1.5, "p95": 2.0, "max": 2.5},
      "embedding": {"p50": 0.3, "p95": 0.5, "max": 0.8},
      "total": {"p50": 2.5, "p95": 3.5, "max": 4.0}
    },
    "iterations": 5,
    "total_pages": 15
  },
  "path_b": {
    "throughput_pages_per_sec": 12.1,
    "slo_target": 10,
    "slo_pass": true,
    "per_stage_latency": {"...": "..."},
    "iterations": 5,
    "total_pages": 10
  },
  "phase2_gate": {
    "integration_tests_pass": null,
    "benchmark_targets_met": true,
    "path_a_slo": true,
    "path_b_slo": true
  }
}
```

**Key implementation details**:
- Use `from ingestkit_excel.backends import QdrantVectorStore, SQLiteStructuredDB, OllamaLLM, OllamaEmbedding` (same import path as `router.py:918-923`)
- Generate test PDFs programmatically using the same reportlab code from conftest.py (extracted into helper functions to avoid importing pytest fixtures)
- The script must be runnable standalone: `python packages/ingestkit-pdf/scripts/benchmark.py --iterations 10`
- Print summary table to stdout in addition to JSON file
- Exit code 0 if all SLOs pass, 1 if any SLO fails

**PDF generation helpers** (extracted from conftest.py patterns):
```python
def _create_text_native_pdf(path: Path) -> int:
    """Create a 3-page text-native PDF. Returns page count."""
    # Same logic as conftest.text_native_pdf fixture
    ...
    return 3

def _create_scanned_pdf(path: Path) -> int:
    """Create a 2-page scanned PDF. Returns page count."""
    # Same logic as conftest.scanned_pdf fixture
    ...
    return 2
```

---

### File 4: `packages/ingestkit-pdf/pyproject.toml` (MODIFY)

**Changes**:
1. Add `benchmark` marker to `[tool.pytest.ini_options]` markers list:
   ```toml
   "benchmark: Benchmark tests with SLO assertions",
   ```

---

## Implementation Order

1. **pyproject.toml** -- Add `benchmark` marker (1 line change)
2. **test_integration.py** -- Integration tests (backend checks + fixtures + 12 tests)
3. **test_benchmark.py** -- Benchmark tests (3 per-stage + 2 throughput = 5 tests)
4. **scripts/benchmark.py** -- Standalone benchmark runner

---

## Acceptance Criteria

- [ ] `test_integration.py` contains `@pytest.mark.integration` tests for Path A, Path B, Path C, and edge cases
- [ ] All integration tests skip gracefully when Qdrant or Ollama are unavailable (no failures, only skips)
- [ ] Path B tests additionally require `@pytest.mark.ocr` (Tesseract)
- [ ] `test_benchmark.py` contains per-stage latency tests and throughput SLO tests
- [ ] Throughput SLO assertions: Path A >= 50 pages/sec, Path B >= 10 pages/sec (from SPEC 25.2)
- [ ] Per-stage latency assertions match SPEC 25.4 max values: security < 500ms, profile < 5s, Tier 1 < 500ms
- [ ] `scripts/benchmark.py` runs standalone and outputs `benchmark-report-<date>.json`
- [ ] JSON report includes throughput, per-stage latency (p50/p95/max), and SLO pass/fail
- [ ] `benchmark` marker registered in pyproject.toml
- [ ] No binary PDF fixtures committed -- all PDFs generated programmatically
- [ ] Tests use existing conftest.py fixtures (text_native_pdf, scanned_pdf, complex_pdf, encrypted_pdf, garbled_pdf)
- [ ] Qdrant test collections cleaned up after tests (no state leakage)
- [ ] `pytest -m unit` still passes with zero regressions
- [ ] `pytest -m integration` skips cleanly when backends are down
- [ ] `pytest -m benchmark` runs benchmark subset

## Verification Gates (for PROVE)

```bash
# Unit tests still pass (no regressions)
cd packages/ingestkit-pdf && python -m pytest tests -m unit -q

# Integration tests skip gracefully without backends
cd packages/ingestkit-pdf && python -m pytest tests/test_integration.py -v --co

# Benchmark tests discoverable
cd packages/ingestkit-pdf && python -m pytest tests/test_benchmark.py -v --co

# Script is runnable (--help check)
python packages/ingestkit-pdf/scripts/benchmark.py --help

# Lint check
cd packages/ingestkit-pdf && ruff check tests/test_integration.py tests/test_benchmark.py scripts/benchmark.py
```

---

AGENT_RETURN: .agents/outputs/plan-47-021426.md
