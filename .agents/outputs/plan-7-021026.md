# PLAN: Issue #7 -- Tier 2/3 LLM Classifier with Schema Validation

**Issue:** #7
**Date:** 2026-02-10
**MAP artifact:** `/home/jjob/projects/ingestkit/.agents/outputs/map-7-021026.md`

---

## Files to Create/Modify

| File | Action | Purpose |
|------|--------|---------|
| `src/ingestkit_excel/llm_classifier.py` | **CREATE** | LLM classifier with validation pipeline |
| `tests/test_llm_classifier.py` | **CREATE** | Comprehensive tests with mock LLM backend |
| `src/ingestkit_excel/__init__.py` | **MODIFY** | Add `LLMClassifier` to imports and `__all__` |

---

## 1. File: `src/ingestkit_excel/llm_classifier.py`

### 1.1 Imports (exact)

```python
from __future__ import annotations

import json
import logging
import os
import re
from typing import Any, Literal

from pydantic import BaseModel, Field, ValidationError

from ingestkit_excel.config import ExcelProcessorConfig
from ingestkit_excel.errors import ErrorCode, IngestError
from ingestkit_excel.models import (
    ClassificationResult,
    ClassificationTier,
    FileProfile,
    FileType,
    SheetProfile,
)
from ingestkit_excel.protocols import LLMBackend

logger = logging.getLogger("ingestkit_excel")
```

### 1.2 Class: `LLMClassificationResponse(BaseModel)`

Pydantic model for validating raw LLM JSON output. This is an internal validation schema, NOT a pipeline output model.

```python
class LLMClassificationResponse(BaseModel):
    """Schema for validating LLM classification output.

    The ``type`` field uses Literal string values matching the ``FileType``
    enum values.  Confidence bounds are checked manually (not via Field
    constraints) to allow clamping instead of rejection.
    """

    type: Literal["tabular_data", "formatted_document", "hybrid"]
    confidence: float  # NO ge/le constraints -- checked manually to allow clamping
    reasoning: str = Field(min_length=1)
    sheet_types: dict[str, Literal["tabular_data", "formatted_document"]] | None = None
```

**ENUM_VALUE compliance:** The `Literal` values are the **string values** `"tabular_data"`, `"formatted_document"`, `"hybrid"` -- NOT Python enum names like `"TABULAR_DATA"`.

**Design decision (MAP section 7, Option A):** `confidence` field has NO `ge`/`le` constraints. Out-of-bounds values are caught manually after Pydantic validation so we can **clamp** (not reject). This avoids a `ValidationError` for OOB confidence, which would trigger an unnecessary retry.

### 1.3 Helper: `_infer_cell_type(value: str) -> str`

Module-level helper for converting cell string values to type labels in the structural summary.

```python
def _infer_cell_type(value: str) -> str:
    """Infer the structural type of a cell value for the LLM summary."""
    if not value or value.strip() == "":
        return "empty"
    try:
        int(value)
        return "int"
    except ValueError:
        pass
    try:
        float(value)
        return "float"
    except ValueError:
        pass
    return "str"
```

### 1.4 Class: `LLMClassifier`

#### 1.4.1 Constructor

```python
class LLMClassifier:
    """Tier 2/3 LLM-based file classifier with schema validation.

    Generates a structural summary from a FileProfile, sends it to an LLM
    backend via a classification prompt, validates the response against a
    Pydantic schema, and returns a ClassificationResult.

    The classifier does NOT handle tier escalation logic -- that is the
    responsibility of the router.  It classifies at the specific tier
    requested.
    """

    def __init__(self, llm: LLMBackend, config: ExcelProcessorConfig) -> None:
        self._llm = llm
        self._config = config
```

**Fields stored:**
- `self._llm: LLMBackend` -- the backend protocol instance
- `self._config: ExcelProcessorConfig` -- full config for accessing all LLM-related settings

#### 1.4.2 Method: `classify`

```python
def classify(
    self,
    profile: FileProfile,
    tier: ClassificationTier,
) -> ClassificationResult:
```

**Parameters:**
- `profile: FileProfile` -- the structural profile of the Excel file
- `tier: ClassificationTier` -- which tier to run (`LLM_BASIC` or `LLM_REASONING`)

**Returns:** `ClassificationResult`

**Pseudocode:**

```
def classify(self, profile, tier):
    # 1. Validate tier parameter
    if tier == ClassificationTier.RULE_BASED:
        raise ValueError("LLMClassifier does not handle rule_based tier")

    # 2. Select model based on tier
    if tier == ClassificationTier.LLM_BASIC:
        model = self._config.classification_model    # "qwen2.5:7b"
    else:  # LLM_REASONING
        model = self._config.reasoning_model         # "deepseek-r1:14b"

    # 3. Generate structural summary (PII-safe by default)
    summary = self._generate_structural_summary(profile)

    # 4. Build classification prompt
    prompt = self._build_classification_prompt(summary, tier)

    # 5. Attempt classification with retry loop
    errors: list[IngestError] = []
    max_attempts = 2  # 1 original + 1 retry

    for attempt in range(max_attempts):
        if attempt > 0:
            # Record retry warning
            errors.append(IngestError(
                code=ErrorCode.W_LLM_RETRY,
                message=f"Retrying LLM classification (attempt {attempt + 1}/{max_attempts})",
                stage="classify",
                recoverable=True,
            ))

        # 5a. Call LLM backend
        try:
            raw_dict = self._llm.classify(
                prompt=prompt,
                model=model,
                temperature=self._config.llm_temperature,
                timeout=self._config.backend_timeout_seconds,
            )
        except json.JSONDecodeError as exc:
            # Backend failed to parse JSON
            errors.append(IngestError(
                code=ErrorCode.E_LLM_MALFORMED_JSON,
                message=f"LLM returned unparseable JSON: {exc}",
                stage="classify",
                recoverable=True,
            ))
            # Append correction hint to prompt for retry
            prompt = prompt + "\n\nIMPORTANT: Your previous response was not valid JSON. Respond with ONLY a JSON object."
            continue
        except TimeoutError:
            errors.append(IngestError(
                code=ErrorCode.E_LLM_TIMEOUT,
                message=f"LLM backend timed out after {self._config.backend_timeout_seconds}s",
                stage="classify",
                recoverable=True,
            ))
            continue
        except Exception as exc:
            errors.append(IngestError(
                code=ErrorCode.E_LLM_MALFORMED_JSON,
                message=f"LLM backend error: {exc}",
                stage="classify",
                recoverable=True,
            ))
            prompt = prompt + "\n\nIMPORTANT: Your previous response was not valid JSON. Respond with ONLY a JSON object."
            continue

        # 5b. Optionally log LLM prompt/response at DEBUG
        if self._config.log_llm_prompts:
            logger.debug("LLM prompt:\n%s", self._redact(prompt))
            logger.debug("LLM response: %s", self._redact(str(raw_dict)))

        # 5c. Validate and parse response
        parsed, validation_errors = self._validate_and_parse_response(raw_dict, profile)
        errors.extend(validation_errors)

        if parsed is None:
            # Validation failed -- append correction hint and retry
            prompt = prompt + "\n\nIMPORTANT: Your previous response had schema errors. Ensure 'type' is one of \"tabular_data\", \"formatted_document\", \"hybrid\". Ensure 'confidence' is a float. Ensure 'reasoning' is a non-empty string. Respond with ONLY a JSON object."
            continue

        # 5d. Convert to ClassificationResult and return
        return self._to_classification_result(parsed, tier)

    # 6. All attempts exhausted -- fail closed
    errors.append(IngestError(
        code=ErrorCode.E_CLASSIFY_INCONCLUSIVE,
        message="LLM classification failed after all retry attempts",
        stage="classify",
        recoverable=False,
    ))

    # Return an inconclusive result with zero confidence
    return ClassificationResult(
        file_type=FileType.TABULAR_DATA,  # arbitrary; confidence=0.0 signals failure
        confidence=0.0,
        tier_used=tier,
        reasoning="LLM classification failed after exhausting retries. Fail-closed.",
        per_sheet_types=None,
        signals=None,
    )
```

**Key design points:**
- Maximum 2 total attempts (1 original + 1 retry) -- matches `backend_max_retries=2` semantics but here we hardcode 2 attempts for the validation retry loop. The backend itself may do its own transport-level retries.
- On JSON parse error from backend, we catch `json.JSONDecodeError` AND generic `Exception` (since backends may wrap errors differently).
- On timeout, we catch `TimeoutError` (backends are expected to raise this or a subclass).
- Correction hints are appended to the prompt on retry, not replacing it.
- Fail-closed: returns `confidence=0.0` with `file_type=FileType.TABULAR_DATA` (arbitrary -- the router should check confidence, not file_type, to determine if the result is usable).

#### 1.4.3 Method: `_generate_structural_summary`

```python
def _generate_structural_summary(self, profile: FileProfile) -> str:
```

**Parameters:**
- `profile: FileProfile` -- the file profile to summarize

**Returns:** `str` -- the structural summary text

**Pseudocode:**

```
def _generate_structural_summary(self, profile):
    # Extract filename only from file_path (no filesystem info leakage)
    filename = os.path.basename(profile.file_path)

    lines = []
    lines.append(f"File: {filename}")
    lines.append(f"Sheets: {profile.sheet_count} ({', '.join(profile.sheet_names)})")

    if profile.has_password_protected_sheets:
        lines.append("Note: File contains password-protected sheets.")
    if profile.has_chart_only_sheets:
        lines.append("Note: File contains chart-only sheets.")

    lines.append("")  # blank line

    for sheet in profile.sheets:
        lines.append(f'Sheet "{sheet.name}":')
        lines.append(f"- Rows: {sheet.row_count}, Columns: {sheet.col_count}")
        lines.append(f"- Merged cells: {sheet.merged_cell_count} (ratio: {sheet.merged_cell_ratio:.3f})")

        if sheet.header_values:
            lines.append(f"- Headers: [{', '.join(sheet.header_values)}]")
        else:
            lines.append("- Headers: [none detected]")

        if sheet.has_formulas:
            lines.append("- Contains formulas: yes")
        if sheet.is_hidden:
            lines.append("- Hidden sheet: yes")

        # Sample rows section
        max_rows = min(self._config.max_sample_rows, len(sheet.sample_rows))
        if max_rows > 0:
            if self._config.log_sample_data:
                # Include actual values (with redaction)
                lines.append("- Sample rows (values):")
                for i, row in enumerate(sheet.sample_rows[:max_rows]):
                    redacted_row = [self._redact(cell) for cell in row]
                    lines.append(f"  Row {i + 1}: [{', '.join(redacted_row)}]")
            else:
                # Structure-only: show types, never raw values
                lines.append("- Sample rows (structure only):")
                for i, row in enumerate(sheet.sample_rows[:max_rows]):
                    types = [_infer_cell_type(cell) for cell in row]
                    lines.append(f"  Row {i + 1}: [{', '.join(types)}]")

        lines.append("")  # blank line between sheets

    return "\n".join(lines)
```

**PII safety rules:**
- Default (`log_sample_data=False`): Only data TYPES (`str`, `float`, `int`, `empty`) appear in sample rows. NO actual cell values.
- Opt-in (`log_sample_data=True`): Actual values from `SheetProfile.sample_rows` are included, but `redact_patterns` are applied.
- Header values are ALWAYS included (they are structural metadata, not PII).
- Filename is extracted from `file_path` using `os.path.basename()` -- full path is NOT sent to the LLM.

#### 1.4.4 Method: `_build_classification_prompt`

```python
def _build_classification_prompt(
    self,
    summary: str,
    tier: ClassificationTier,
) -> str:
```

**Parameters:**
- `summary: str` -- the structural summary from `_generate_structural_summary`
- `tier: ClassificationTier` -- the tier (unused in base prompt but available for tier-specific prompt adjustments)

**Returns:** `str` -- the full prompt to send to the LLM

**Pseudocode:**

```
def _build_classification_prompt(self, summary, tier):
    prompt = (
        "You are classifying an Excel file for a document ingestion system.\n"
        "Based on the structural summary below, classify this file as one of:\n"
        "\n"
        '- "tabular_data": Rows are records, columns are fields. Consistent structure. Suitable for SQL database import.\n'
        '- "formatted_document": Excel used as a layout/formatting tool. Merged cells, irregular structure, text-heavy. Suitable for text extraction.\n'
        '- "hybrid": Mix of tabular and document-formatted sections. Different sheets or regions serve different purposes.\n'
        "\n"
        "Respond with JSON only:\n"
        "{\n"
        '  "type": "tabular_data" | "formatted_document" | "hybrid",\n'
        '  "confidence": <float between 0.0 and 1.0>,\n'
        '  "reasoning": "brief explanation",\n'
        '  "sheet_types": {"sheet_name": "type", ...}  // only if hybrid\n'
        "}\n"
        "\n"
        "Structural summary:\n"
        f"{summary}"
    )
    return prompt
```

**ENUM_VALUE compliance:** The prompt explicitly shows `"tabular_data"`, `"formatted_document"`, `"hybrid"` as the valid values -- matching the `FileType` enum string values exactly.

#### 1.4.5 Method: `_validate_and_parse_response`

```python
def _validate_and_parse_response(
    self,
    raw: dict,
    profile: FileProfile,
) -> tuple[LLMClassificationResponse | None, list[IngestError]]:
```

**Parameters:**
- `raw: dict` -- the parsed JSON dict from the LLM backend
- `profile: FileProfile` -- for cross-referencing sheet names in `sheet_types`

**Returns:** `tuple[LLMClassificationResponse | None, list[IngestError]]`
- First element: the validated response, or `None` if validation failed
- Second element: list of errors/warnings encountered during validation

**Pseudocode:**

```
def _validate_and_parse_response(self, raw, profile):
    errors: list[IngestError] = []

    # Step 1: Pydantic schema validation
    try:
        response = LLMClassificationResponse(**raw)
    except ValidationError as exc:
        errors.append(IngestError(
            code=ErrorCode.E_LLM_SCHEMA_INVALID,
            message=f"LLM response failed schema validation: {exc}",
            stage="classify",
            recoverable=True,
        ))
        return None, errors

    # Step 2: Confidence bounds check (manual, since Field has no ge/le)
    if response.confidence < 0.0 or response.confidence > 1.0:
        original = response.confidence
        clamped = max(0.0, min(1.0, response.confidence))
        errors.append(IngestError(
            code=ErrorCode.E_LLM_CONFIDENCE_OOB,
            message=f"Confidence {original} outside [0.0, 1.0], clamped to {clamped}",
            stage="classify",
            recoverable=True,
        ))
        # Create new response with clamped confidence (Pydantic models are immutable by default)
        response = response.model_copy(update={"confidence": clamped})

    # Step 3: Validate sheet_types keys match actual sheet names (if hybrid)
    if response.type == "hybrid" and response.sheet_types is not None:
        known_sheets = set(profile.sheet_names)
        unknown_sheets = set(response.sheet_types.keys()) - known_sheets
        if unknown_sheets:
            # Warning only -- do not reject
            logger.warning(
                "LLM returned sheet_types for unknown sheets: %s", unknown_sheets
            )

    return response, errors
```

**Key design points:**
- Pydantic `ValidationError` catches: wrong `type` value, missing required fields, empty `reasoning`.
- Confidence is NOT checked by Pydantic (no `ge`/`le` on field). Instead, we clamp manually and record `E_LLM_CONFIDENCE_OOB` as a warning.
- `sheet_types` keys are validated against `profile.sheet_names` but mismatches are warnings, not errors (the LLM might use slightly different names).
- Returns `None` on schema failure so the caller knows to retry.
- Returns the validated response on confidence OOB (clamped, not rejected).

#### 1.4.6 Method: `_to_classification_result`

```python
def _to_classification_result(
    self,
    response: LLMClassificationResponse,
    tier: ClassificationTier,
) -> ClassificationResult:
```

**Parameters:**
- `response: LLMClassificationResponse` -- the validated LLM response
- `tier: ClassificationTier` -- the tier that produced this result

**Returns:** `ClassificationResult`

**Pseudocode:**

```
def _to_classification_result(self, response, tier):
    # Convert string type to FileType enum
    file_type = FileType(response.type)  # FileType("tabular_data") -> FileType.TABULAR_DATA

    # Convert sheet_types strings to FileType enums (if present)
    per_sheet_types: dict[str, FileType] | None = None
    if response.sheet_types is not None:
        per_sheet_types = {
            name: FileType(st) for name, st in response.sheet_types.items()
        }

    return ClassificationResult(
        file_type=file_type,
        confidence=response.confidence,
        tier_used=tier,
        reasoning=response.reasoning,
        per_sheet_types=per_sheet_types,
        signals=None,  # LLM tiers do not produce signal breakdowns
    )
```

**ENUM_VALUE compliance:**
- `FileType(response.type)` works because `FileType` is `str, Enum` and `response.type` is one of `"tabular_data"`, `"formatted_document"`, `"hybrid"` -- matching the enum VALUES (not names).
- Do NOT use `FileType[response.type]` -- that looks up by NAME, not value.

#### 1.4.7 Method: `_redact`

```python
def _redact(self, text: str) -> str:
```

**Parameters:**
- `text: str` -- text to apply redaction patterns to

**Returns:** `str` -- redacted text

**Pseudocode:**

```
def _redact(self, text):
    result = text
    for pattern in self._config.redact_patterns:
        result = re.sub(pattern, "[REDACTED]", result)
    return result
```

---

## 2. Enum Values Reference (Exact Strings Used in Code)

### FileType enum values (used in Literal types and FileType() constructor)
| Python Member | String Value | Where Used |
|---|---|---|
| `FileType.TABULAR_DATA` | `"tabular_data"` | LLM response `type` field, prompt text |
| `FileType.FORMATTED_DOCUMENT` | `"formatted_document"` | LLM response `type` field, prompt text |
| `FileType.HYBRID` | `"hybrid"` | LLM response `type` field, prompt text |

### ClassificationTier enum values
| Python Member | String Value | Where Used |
|---|---|---|
| `ClassificationTier.RULE_BASED` | `"rule_based"` | Rejected by LLMClassifier (not its responsibility) |
| `ClassificationTier.LLM_BASIC` | `"llm_basic"` | Tier 2: uses `config.classification_model` |
| `ClassificationTier.LLM_REASONING` | `"llm_reasoning"` | Tier 3: uses `config.reasoning_model` |

### ErrorCode enum values (used via enum member, not string)
| Python Member | String `.value` | When Constructed |
|---|---|---|
| `ErrorCode.E_LLM_TIMEOUT` | `"E_LLM_TIMEOUT"` | Backend raises `TimeoutError` |
| `ErrorCode.E_LLM_MALFORMED_JSON` | `"E_LLM_MALFORMED_JSON"` | Backend raises `json.JSONDecodeError` or generic exception |
| `ErrorCode.E_LLM_SCHEMA_INVALID` | `"E_LLM_SCHEMA_INVALID"` | Pydantic `ValidationError` on LLM dict |
| `ErrorCode.E_LLM_CONFIDENCE_OOB` | `"E_LLM_CONFIDENCE_OOB"` | Confidence outside [0.0, 1.0] after Pydantic passes |
| `ErrorCode.E_CLASSIFY_INCONCLUSIVE` | `"E_CLASSIFY_INCONCLUSIVE"` | All retry attempts exhausted |
| `ErrorCode.W_LLM_RETRY` | `"W_LLM_RETRY"` | Each retry attempt (warning, non-fatal) |

**Construction pattern:** Always use the enum member directly:
```python
IngestError(code=ErrorCode.E_LLM_TIMEOUT, message="...", stage="classify", recoverable=True)
```

---

## 3. File: `tests/test_llm_classifier.py`

### 3.1 Test File Structure

Follow the existing patterns from `tests/test_inspector.py`:
- Module-level docstring
- Imports from submodules directly (not `__init__.py`)
- Helper functions prefixed with `_`
- Fixtures using `@pytest.fixture()`
- Test classes organized by concern
- Test methods: `test_<what>_<expected_outcome>` naming
- Type annotations returning `-> None`

### 3.2 Imports

```python
"""Tests for the LLMClassifier Tier 2/3 LLM-based classifier.

Covers schema validation, malformed JSON retry, confidence clamping,
tier escalation, fail-closed behavior, structural summary generation,
prompt correctness, and the mock LLM backend.
"""

from __future__ import annotations

import json

import pytest

from ingestkit_excel.config import ExcelProcessorConfig
from ingestkit_excel.errors import ErrorCode, IngestError
from ingestkit_excel.llm_classifier import (
    LLMClassificationResponse,
    LLMClassifier,
    _infer_cell_type,
)
from ingestkit_excel.models import (
    ClassificationResult,
    ClassificationTier,
    FileProfile,
    FileType,
    ParserUsed,
    SheetProfile,
)
```

### 3.3 Mock LLM Backend Design

```python
class MockLLMBackend:
    """Mock LLM backend for testing LLMClassifier.

    Supports configurable responses via a list of return values or
    exceptions. Each call to ``classify()`` pops the next item from
    the response queue, allowing tests to simulate retry sequences.
    """

    def __init__(
        self,
        responses: list[dict | Exception] | None = None,
    ) -> None:
        self._responses: list[dict | Exception] = list(responses or [])
        self.calls: list[dict] = []  # records all calls for assertion

    def classify(
        self,
        prompt: str,
        model: str,
        temperature: float = 0.1,
        timeout: float | None = None,
    ) -> dict:
        self.calls.append({
            "prompt": prompt,
            "model": model,
            "temperature": temperature,
            "timeout": timeout,
        })
        if not self._responses:
            raise RuntimeError("MockLLMBackend: no more responses configured")
        response = self._responses.pop(0)
        if isinstance(response, Exception):
            raise response
        return response

    def generate(
        self,
        prompt: str,
        model: str,
        temperature: float = 0.7,
        timeout: float | None = None,
    ) -> str:
        raise NotImplementedError("generate() not used by LLMClassifier")
```

**Key design features:**
- `responses` is a queue (list). Each `classify()` call pops the next item.
- If the item is an `Exception`, it is raised (simulates backend failure).
- If the item is a `dict`, it is returned (simulates successful JSON parse by backend).
- `self.calls` records all call arguments for assertion in tests.
- Implements both `classify()` and `generate()` to satisfy the `LLMBackend` protocol.

### 3.4 Helpers

```python
def _make_sheet_profile(**overrides: object) -> SheetProfile:
    """Build a SheetProfile with sensible tabular defaults."""
    defaults: dict = dict(
        name="Sheet1",
        row_count=100,
        col_count=5,
        merged_cell_count=0,
        merged_cell_ratio=0.0,
        header_row_detected=True,
        header_values=["A", "B", "C", "D", "E"],
        column_type_consistency=0.9,
        numeric_ratio=0.4,
        text_ratio=0.5,
        empty_ratio=0.1,
        sample_rows=[["1", "hello", "3.14", "", "world"]],
        has_formulas=False,
        is_hidden=False,
        parser_used=ParserUsed.OPENPYXL,
    )
    defaults.update(overrides)
    return SheetProfile(**defaults)


def _make_file_profile(
    sheets: list[SheetProfile] | None = None, **overrides: object
) -> FileProfile:
    """Build a FileProfile from a list of SheetProfiles."""
    if sheets is None:
        sheets = [_make_sheet_profile()]
    defaults: dict = dict(
        file_path="/tmp/test.xlsx",
        file_size_bytes=1024,
        sheet_count=len(sheets),
        sheet_names=[s.name for s in sheets],
        sheets=sheets,
        has_password_protected_sheets=False,
        has_chart_only_sheets=False,
        total_merged_cells=sum(s.merged_cell_count for s in sheets),
        total_rows=sum(s.row_count for s in sheets),
        content_hash="a" * 64,
    )
    defaults.update(overrides)
    return FileProfile(**defaults)


def _valid_response(
    type_: str = "tabular_data",
    confidence: float = 0.85,
    reasoning: str = "Consistent column types and header row detected.",
    sheet_types: dict[str, str] | None = None,
) -> dict:
    """Build a valid LLM response dict."""
    d: dict[str, object] = {
        "type": type_,
        "confidence": confidence,
        "reasoning": reasoning,
    }
    if sheet_types is not None:
        d["sheet_types"] = sheet_types
    return d
```

### 3.5 Fixtures

```python
@pytest.fixture()
def config() -> ExcelProcessorConfig:
    return ExcelProcessorConfig()


@pytest.fixture()
def profile() -> FileProfile:
    return _make_file_profile()
```

### 3.6 Complete Test Case List

#### Class: `TestValidResponseParsing`

| Test Name | What It Tests |
|---|---|
| `test_valid_tabular_response_returns_correct_result` | Valid `"tabular_data"` response -> `ClassificationResult` with `FileType.TABULAR_DATA`, correct confidence, reasoning |
| `test_valid_formatted_document_response` | Valid `"formatted_document"` response -> `FileType.FORMATTED_DOCUMENT` |
| `test_valid_hybrid_response_with_sheet_types` | Valid `"hybrid"` response with `sheet_types` -> `FileType.HYBRID`, `per_sheet_types` populated with `FileType` enums |
| `test_hybrid_response_per_sheet_types_are_filetype_enums` | `per_sheet_types` values are `FileType.TABULAR_DATA` / `FileType.FORMATTED_DOCUMENT`, not strings |
| `test_tier_used_reflects_llm_basic` | Calling with `ClassificationTier.LLM_BASIC` -> `tier_used == ClassificationTier.LLM_BASIC` |
| `test_tier_used_reflects_llm_reasoning` | Calling with `ClassificationTier.LLM_REASONING` -> `tier_used == ClassificationTier.LLM_REASONING` |
| `test_signals_is_none_for_llm_tiers` | `ClassificationResult.signals` is `None` for LLM tiers |

#### Class: `TestMalformedJsonRetry`

| Test Name | What It Tests |
|---|---|
| `test_json_decode_error_triggers_retry` | First call raises `json.JSONDecodeError`, second returns valid -> success, `W_LLM_RETRY` recorded |
| `test_generic_exception_triggers_retry` | First call raises generic `Exception`, second returns valid -> success |
| `test_two_json_failures_returns_fail_closed` | Both calls raise `json.JSONDecodeError` -> fail-closed result with `confidence=0.0` |
| `test_correction_hint_appended_to_prompt_on_retry` | After JSON error, second call's prompt contains "IMPORTANT" correction hint |

#### Class: `TestSchemaValidation`

| Test Name | What It Tests |
|---|---|
| `test_missing_type_field_triggers_schema_error` | Dict without `type` -> `E_LLM_SCHEMA_INVALID`, retry |
| `test_invalid_type_value_triggers_schema_error` | `type: "TABULAR_DATA"` (Python name, not value) -> `E_LLM_SCHEMA_INVALID` |
| `test_missing_reasoning_triggers_schema_error` | Dict without `reasoning` -> `E_LLM_SCHEMA_INVALID` |
| `test_empty_reasoning_triggers_schema_error` | `reasoning: ""` -> `E_LLM_SCHEMA_INVALID` (min_length=1) |
| `test_schema_error_retry_then_success` | First call returns invalid schema, second returns valid -> success |
| `test_two_schema_failures_returns_fail_closed` | Both calls return invalid schema -> fail-closed |

#### Class: `TestConfidenceBounds`

| Test Name | What It Tests |
|---|---|
| `test_confidence_above_1_is_clamped` | `confidence: 1.5` -> clamped to `1.0`, `E_LLM_CONFIDENCE_OOB` warning recorded, result NOT rejected |
| `test_confidence_below_0_is_clamped` | `confidence: -0.3` -> clamped to `0.0`, `E_LLM_CONFIDENCE_OOB` warning recorded |
| `test_confidence_exactly_0_is_valid` | `confidence: 0.0` -> accepted, no OOB warning |
| `test_confidence_exactly_1_is_valid` | `confidence: 1.0` -> accepted, no OOB warning |
| `test_confidence_oob_does_not_trigger_retry` | OOB confidence is clamped, NOT rejected -- only 1 LLM call made |

#### Class: `TestTimeoutHandling`

| Test Name | What It Tests |
|---|---|
| `test_timeout_triggers_retry` | First call raises `TimeoutError`, second returns valid -> success, `E_LLM_TIMEOUT` recorded |
| `test_two_timeouts_returns_fail_closed` | Both calls raise `TimeoutError` -> fail-closed |

#### Class: `TestFailClosed`

| Test Name | What It Tests |
|---|---|
| `test_fail_closed_result_has_zero_confidence` | After all retries fail -> `confidence == 0.0` |
| `test_fail_closed_result_has_correct_tier_used` | Fail-closed result has the tier that was attempted |
| `test_fail_closed_reasoning_mentions_failure` | Fail-closed reasoning contains "failed" or "fail-closed" |

#### Class: `TestTierModelSelection`

| Test Name | What It Tests |
|---|---|
| `test_tier2_uses_classification_model` | `ClassificationTier.LLM_BASIC` -> backend called with `model="qwen2.5:7b"` |
| `test_tier3_uses_reasoning_model` | `ClassificationTier.LLM_REASONING` -> backend called with `model="deepseek-r1:14b"` |
| `test_rule_based_tier_raises_value_error` | `ClassificationTier.RULE_BASED` -> `ValueError` raised |
| `test_custom_model_names_respected` | Custom config with different model names -> backend called with those names |
| `test_temperature_from_config` | `llm_temperature` from config passed to backend |
| `test_timeout_from_config` | `backend_timeout_seconds` from config passed to backend |

#### Class: `TestStructuralSummary`

| Test Name | What It Tests |
|---|---|
| `test_summary_contains_no_raw_values_by_default` | With `log_sample_data=False`, summary contains type labels (`int`, `str`, `float`, `empty`) but NOT the actual cell values (`"hello"`, `"3.14"`) |
| `test_summary_contains_values_when_log_sample_data_true` | With `log_sample_data=True`, summary contains actual cell values |
| `test_summary_contains_filename_not_full_path` | Summary contains `"test.xlsx"` but NOT `"/tmp/test.xlsx"` |
| `test_summary_contains_sheet_names` | Summary contains all sheet names from profile |
| `test_summary_contains_header_values` | Summary contains header values (always included, they are structural) |
| `test_summary_contains_merged_cell_info` | Summary contains merged cell count and ratio |
| `test_summary_mentions_hidden_sheets` | Hidden sheet -> summary mentions "Hidden sheet: yes" |
| `test_summary_mentions_formulas` | Sheet with formulas -> summary mentions "Contains formulas: yes" |
| `test_summary_mentions_password_protected` | File with password-protected sheets -> summary mentions it |
| `test_summary_respects_max_sample_rows` | With `max_sample_rows=1`, only 1 sample row appears in summary |
| `test_summary_redacts_when_log_sample_data_with_patterns` | With `log_sample_data=True` and `redact_patterns=["\\d{3}-\\d{4}"]`, matching values are replaced with `[REDACTED]` |

#### Class: `TestPromptContent`

| Test Name | What It Tests |
|---|---|
| `test_prompt_contains_tabular_data_enum_value` | Prompt contains `"tabular_data"` (not `"TABULAR_DATA"`) |
| `test_prompt_contains_formatted_document_enum_value` | Prompt contains `"formatted_document"` |
| `test_prompt_contains_hybrid_enum_value` | Prompt contains `"hybrid"` |
| `test_prompt_contains_structural_summary` | Prompt contains the structural summary text |
| `test_prompt_requests_json_only` | Prompt contains "Respond with JSON only" |

#### Class: `TestCellTypeInference`

| Test Name | What It Tests |
|---|---|
| `test_infer_cell_type_empty_string` | `""` -> `"empty"` |
| `test_infer_cell_type_whitespace` | `"  "` -> `"empty"` |
| `test_infer_cell_type_integer` | `"42"` -> `"int"` |
| `test_infer_cell_type_float` | `"3.14"` -> `"float"` |
| `test_infer_cell_type_string` | `"hello"` -> `"str"` |
| `test_infer_cell_type_negative_int` | `"-5"` -> `"int"` |
| `test_infer_cell_type_negative_float` | `"-3.14"` -> `"float"` |

#### Class: `TestLLMClassificationResponseModel`

| Test Name | What It Tests |
|---|---|
| `test_valid_model_creation` | Valid dict -> `LLMClassificationResponse` created successfully |
| `test_invalid_type_rejected` | `type: "invalid"` -> `ValidationError` |
| `test_empty_reasoning_rejected` | `reasoning: ""` -> `ValidationError` |
| `test_sheet_types_with_hybrid_value_rejected` | `sheet_types: {"S1": "hybrid"}` -> `ValidationError` (individual sheets cannot be hybrid) |
| `test_confidence_not_bounded_by_field` | `confidence: 5.0` -> model created (no Field bounds; checked manually) |

---

## 4. File: `src/ingestkit_excel/__init__.py` Update

### Current state (line 30):
```python
from ingestkit_excel.inspector import ExcelInspector
```

### Add after line 30:
```python
from ingestkit_excel.llm_classifier import LLMClassifier
```

### Current `__all__` (line 64-65):
```python
    # Inspector
    "ExcelInspector",
```

### Add after "ExcelInspector":
```python
    # LLM Classifier
    "LLMClassifier",
```

---

## 5. Verification Checklist

### ENUM_VALUE Pattern Compliance
- [ ] `LLMClassificationResponse.type` uses `Literal["tabular_data", "formatted_document", "hybrid"]` -- string VALUES, not Python names
- [ ] `LLMClassificationResponse.sheet_types` values use `Literal["tabular_data", "formatted_document"]` -- no `"hybrid"` for individual sheets
- [ ] `FileType(response.type)` used for enum conversion -- NOT `FileType[response.type]`
- [ ] `ErrorCode.E_LLM_TIMEOUT` used as enum member -- NOT string `"E_LLM_TIMEOUT"`
- [ ] Prompt text contains `"tabular_data"`, `"formatted_document"`, `"hybrid"` -- string values
- [ ] `ClassificationTier.LLM_BASIC` / `ClassificationTier.LLM_REASONING` used for `tier_used` -- NOT string values

### VERIFICATION_GAP Pattern Compliance
- [ ] `LLMBackend.classify()` returns `dict` -- skip `json.loads()` step, go straight to Pydantic validation
- [ ] Backend may raise on malformed JSON -- catch `json.JSONDecodeError` and generic `Exception`
- [ ] Backend may raise `TimeoutError` -- catch it explicitly
- [ ] `per_sheet_types` converted from `dict[str, str]` to `dict[str, FileType]` via `FileType(value)`
- [ ] Logging uses `logger = logging.getLogger("ingestkit_excel")`
- [ ] Prompts/responses only logged at DEBUG level when `config.log_llm_prompts` is `True`
- [ ] `redact_patterns` applied to any logged text when `log_sample_data=True`
- [ ] `ClassificationTier.RULE_BASED` passed to `classify()` raises `ValueError`

### Test Compliance
- [ ] All tests use `MockLLMBackend`, never a real LLM
- [ ] Helper functions reuse `_make_sheet_profile` / `_make_file_profile` pattern from `test_inspector.py`
- [ ] Test methods return `-> None`
- [ ] Test classes organized by concern
- [ ] Error codes verified by comparing `error.code` to enum member (e.g., `ErrorCode.E_LLM_MALFORMED_JSON`)

---

## 6. Key Design Decisions Summary

| Decision | Rationale |
|---|---|
| No `ge`/`le` on `LLMClassificationResponse.confidence` | Allows clamping instead of rejection (MAP section 7, Option A) |
| `classify()` rejects `RULE_BASED` tier with `ValueError` | LLMClassifier is not responsible for rule-based classification |
| Backend exceptions caught broadly (`json.JSONDecodeError`, `TimeoutError`, generic `Exception`) | Defensive coding per VERIFICATION_GAP pattern -- backends may wrap errors differently |
| Fail-closed returns `confidence=0.0` with arbitrary `file_type` | Router must check confidence, not file_type, to determine if result is usable |
| 2 total attempts (1 original + 1 retry) | Matches SPEC section 9.4: "Maximum 2 total attempts" |
| Correction hints appended to prompt, not replacing it | Preserves original context while adding correction guidance |
| `_redact()` applied to both sample data and logged prompts/responses | Single redaction point for PII safety |
| `signals=None` for all LLM tier results | LLM tiers do not produce signal breakdowns (those are Tier 1 only) |
| `sheet_types` key mismatch is a warning, not an error | LLM may produce slightly different sheet names; should not reject entire classification |
