{"parentUuid":null,"isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":"Research open-source projects on GitHub and the web that do template-driven form extraction from PDFs, scanned documents, and images. I need to understand what technologies, patterns, and architectures are being used in production-quality form extraction systems.\n\nFocus areas:\n1. **Template-based form extraction tools** — projects where you define a form template/layout and extract field values from matching documents\n2. **Document intelligence / form recognition** — open-source alternatives to AWS Textract, Azure Document Intelligence, Google Document AI\n3. **OCR overlay / region-based extraction** — tools that crop specific regions from documents and OCR them independently\n4. **Layout fingerprinting / document matching** — how projects match incoming documents to known templates\n5. **Form field extraction from PDFs** — PyMuPDF, pdfplumber, or other libraries for extracting fillable form data\n\nFor each significant project found, document:\n- Project name and GitHub URL\n- Stars / activity level\n- Core technology stack (Python libs, OCR engines, ML models)\n- Architecture pattern (template-driven? ML-based? hybrid?)\n- How they handle template definition (visual UI? JSON config? code?)\n- How they handle document matching (layout similarity? ML classification? manual assignment?)\n- How they handle multi-format support (PDF, image, Excel)\n- Strengths and weaknesses relevant to our use case (on-premises, no cloud, plugin architecture)\n- Any interesting patterns we should consider adopting\n\nSearch for:\n- \"form extraction template python\" on GitHub\n- \"document template matching OCR\" on GitHub\n- \"PDF form data extraction open source\"\n- \"layout-based document classification\"\n- \"structured data extraction from scanned forms\"\n- docTR, LayoutLM, Donut, PaddleOCR structured extraction\n- Camelot, Tabula, pdfplumber form capabilities\n- Any Python libraries specifically for template-based form field extraction\n\nAlso look at:\n- How Docling (IBM) handles form/document extraction\n- How PaddleOCR's PP-Structure handles form recognition\n- How LayoutLMv3 / LayoutParser handle document understanding\n- How marker-pdf handles PDF extraction\n- Any projects using visual template mapping for form extraction\n\nProvide a comprehensive research report with findings organized by project/tool."},"uuid":"daf406e3-1cbc-4697-96cd-2a00669a55b7","timestamp":"2026-02-12T16:56:01.530Z"}
{"parentUuid":"daf406e3-1cbc-4697-96cd-2a00669a55b7","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_014tCcZZuSntbicdzndYfVDK","type":"message","role":"assistant","content":[{"type":"text","text":"\n\nI'll conduct a comprehensive research survey across multiple search queries to cover all the focus areas you've outlined. Let me run these searches in parallel."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":2927,"cache_read_input_tokens":8854,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2927},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XWM87EpsM8QqiEMV46","type":"assistant","uuid":"01cba89d-5cda-4028-91eb-86961ca321c4","timestamp":"2026-02-12T16:56:04.647Z"}
{"parentUuid":"01cba89d-5cda-4028-91eb-86961ca321c4","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_014tCcZZuSntbicdzndYfVDK","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01CosiEo7LpAwfxd1ihxMnsZ","name":"WebSearch","input":{"query":"form extraction template python open source GitHub 2025 2026"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":2927,"cache_read_input_tokens":8854,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2927},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XWM87EpsM8QqiEMV46","type":"assistant","uuid":"85b3707b-b437-4c9b-a37c-3f650418336f","timestamp":"2026-02-12T16:56:05.205Z"}
{"parentUuid":"85b3707b-b437-4c9b-a37c-3f650418336f","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_014tCcZZuSntbicdzndYfVDK","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01MPBfFUDh85ypxHu5ps2yPC","name":"WebSearch","input":{"query":"document template matching OCR open source GitHub"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":2927,"cache_read_input_tokens":8854,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2927},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XWM87EpsM8QqiEMV46","type":"assistant","uuid":"f13a4cb5-1321-4ce2-bdc0-a534cb65b6c9","timestamp":"2026-02-12T16:56:05.569Z"}
{"parentUuid":"f13a4cb5-1321-4ce2-bdc0-a534cb65b6c9","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_014tCcZZuSntbicdzndYfVDK","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01K43jDJJUwMKviKgtSJqJNu","name":"WebSearch","input":{"query":"PDF form data extraction open source python library 2025"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":2927,"cache_read_input_tokens":8854,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2927},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XWM87EpsM8QqiEMV46","type":"assistant","uuid":"7d845233-9fe4-4cf2-bdba-1ab99e8cdd07","timestamp":"2026-02-12T16:56:05.996Z"}
{"parentUuid":"7d845233-9fe4-4cf2-bdba-1ab99e8cdd07","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_014tCcZZuSntbicdzndYfVDK","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01WrA5K8Tp6dqpHDyvUHMgtu","name":"WebSearch","input":{"query":"layout-based document classification open source python"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":2927,"cache_read_input_tokens":8854,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2927},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XWM87EpsM8QqiEMV46","type":"assistant","uuid":"ec294f1b-fd1e-4b73-b049-63eb3345b6a6","timestamp":"2026-02-12T16:56:06.373Z"}
{"parentUuid":"ec294f1b-fd1e-4b73-b049-63eb3345b6a6","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_014tCcZZuSntbicdzndYfVDK","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Et2Yhds7dwV86ETP9PooDf","name":"WebSearch","input":{"query":"structured data extraction scanned forms open source template-driven"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":2927,"cache_read_input_tokens":8854,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":2927},"output_tokens":2,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XWM87EpsM8QqiEMV46","type":"assistant","uuid":"dac07e4f-fe09-4578-96e9-2e6bdf8b762b","timestamp":"2026-02-12T16:56:06.713Z"}
{"parentUuid":"85b3707b-b437-4c9b-a37c-3f650418336f","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01CosiEo7LpAwfxd1ihxMnsZ","type":"tool_result","content":"Web search results for query: \"form extraction template python open source GitHub 2025 2026\"\n\nLinks: [{\"title\":\"GitHub - HarendraKumarSingh/form-extractor-ocr: Template based form extractor OCR. Train your own character and alphabet OCR.\",\"url\":\"https://github.com/HarendraKumarSingh/form-extractor-ocr\"},{\"title\":\"GitHub - sayak119/FORMOCR: Exact Sciences extract text from forms\",\"url\":\"https://github.com/sayak119/FORMOCR\"},{\"title\":\"GitHub - seriyps/python-form-parser: Simple utility/class for extract and fill HTML forms\",\"url\":\"https://github.com/seriyps/python-form-parser\"},{\"title\":\"GitHub - ucbepic/TWIX: TWIX is an open-source data extraction tool that reconstructs structured data from documents at scale, accurately and at low cost, by inferring the shared underlying visual template across documents\",\"url\":\"https://github.com/ucbepic/TWIX\"},{\"title\":\"forms · GitHub Topics · GitHub\",\"url\":\"https://github.com/topics/forms?l=python\"},{\"title\":\"form-builder · GitHub Topics · GitHub\",\"url\":\"https://github.com/topics/form-builder?l=python&o=desc&s=updated\"},{\"title\":\"GitHub - tulasiram58827/Information-Extraction-From-Documents: This repository contains an implementation of the \\\"Representation Learning for Information Extraction from Form-like Documents\\\" paper.\",\"url\":\"https://github.com/tulasiram58827/Information-Extraction-From-Documents\"},{\"title\":\"GitHub - Praneet9/Representation-Learning-for-Information-Extraction: Pytorch implementation of Paper by Google Research - Representation Learning for Information Extraction from Form-like Documents.\",\"url\":\"https://github.com/Praneet9/Representation-Learning-for-Information-Extraction\"},{\"title\":\"GitHub - timpaul/form-extractor-prototype: A prototype of a tool that generates web forms from document forms\",\"url\":\"https://github.com/timpaul/form-extractor-prototype\"},{\"title\":\"GitHub - microsoft/knowledge-extraction-recipes-forms: Knowledge Extraction For Forms Accelerators & Examples\",\"url\":\"https://github.com/microsoft/knowledge-extraction-recipes-forms\"}]\n\nBased on the search results, here are some notable open-source form extraction tools available on GitHub that use Python:\n\n## Notable Form Extraction Projects:\n\n1. **TWIX** - An open-source data extraction tool that reconstructs structured data from documents at scale, accurately and at low cost, by inferring the shared underlying visual template across documents. It's a Python package for extracting structured data from documents step by step, designed for production pipeline deployment, with optional user feedback to monitor and refine the extraction process.\n\n2. **form-extractor-ocr** - A template based form extractor OCR that extracts handwritten text from bank form scanned images using template matching, individual box extraction and OCR.\n\n3. **FORMOCR** - A form text extraction tool with both Python backend and a UI. It requires pip install of requirements and can launch a UI from the terminal.\n\n4. **form-extractor-prototype** - A tool that extracts the structure from a PDF or image of a form using Google Gemini Flash 2.5, replicates the form structure in JSON, and generates a multi-page web form in the GOV.UK style.\n\n5. **python-form-parser** - A simple utility/class for extracting and filling HTML forms.\n\n6. **Microsoft Knowledge Extraction Recipes** - Code accelerators available in Jupyter notebooks, APIs and Python scripts showcasing scenarios using diverse approaches for form knowledge extraction.\n\nThese projects offer various approaches to form extraction, from template-based OCR methods to AI-powered approaches using vision models.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"e1ed1bd8-a310-492b-925a-0fb504f94949","timestamp":"2026-02-12T16:56:12.687Z","sourceToolAssistantUUID":"85b3707b-b437-4c9b-a37c-3f650418336f"}
{"parentUuid":"f13a4cb5-1321-4ce2-bdc0-a534cb65b6c9","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01MPBfFUDh85ypxHu5ps2yPC","type":"tool_result","content":"Web search results for query: \"document template matching OCR open source GitHub\"\n\nLinks: [{\"title\":\"GitHub - Topdu/OpenOCR: OpenOCR: An Open-Source Toolkit for General-OCR Research and Applications, integrates a unified training and evaluation benchmark, commercial-grade OCR and Document Parsing systems, and faithful reproductions of the core implementations from a wide range of academic papers.\",\"url\":\"https://github.com/Topdu/OpenOCR\"},{\"title\":\"template-matching-ocr/ocr_template_match.py at master · NLGRF/template-matching-ocr\",\"url\":\"https://github.com/NLGRF/template-matching-ocr/blob/master/ocr_template_match.py\"},{\"title\":\"GitHub - SaurabhThube/ocr-Template-matching-: Template matching ocr using scanlines and templates. Accuracy more than 80%. Need to improve accuracy and small character recognition more precisely\",\"url\":\"https://github.com/SaurabhThube/ocr-Template-matching-\"},{\"title\":\"ocr · GitHub Topics · GitHub\",\"url\":\"https://github.com/topics/ocr\"},{\"title\":\"GitHub - kba/awesome-ocr: Links to awesome OCR projects\",\"url\":\"https://github.com/kba/awesome-ocr\"},{\"title\":\"GitHub - himanshumahajan138/FraudGuard: This system utilizes Optical Character Recognition (OCR) extracts text, while computer vision techniques map document layout. Then, SIFT (Scale-Invariant Feature Transform) cleverly matches documents to pre-defined templates, even with variations. This intelligent matching helps identify potential fraud for further investigation.\",\"url\":\"https://github.com/himanshumahajan138/FraudGuard\"},{\"title\":\"GitHub - LeoFCardoso/pdf2pdfocr: A free tool to OCR a PDF and add a text \\\"layer\\\" in the original file, making a searchable PDF. Use only open source tools. Please tip!\",\"url\":\"https://github.com/LeoFCardoso/pdf2pdfocr\"},{\"title\":\"GitHub - mindee/doctr: docTR (Document Text Recognition) - a seamless, high-performing & accessible library for OCR-related tasks powered by Deep Learning.\",\"url\":\"https://github.com/mindee/doctr\"},{\"title\":\"OCR/7 - template-matching/template-matching.ipynb at main · IgorMeloS/OCR\",\"url\":\"https://github.com/IgorMeloS/OCR/blob/main/7%20-%20template-matching/template-matching.ipynb\"},{\"title\":\"GitHub - NLGRF/template-matching-ocr\",\"url\":\"https://github.com/NLGRF/template-matching-ocr\"}]\n\nI found several relevant open-source projects for document template matching with OCR on GitHub:\n\n## Key Projects\n\n**1. OpenOCR** - An open-source toolkit that integrates unified training and evaluation benchmarks, commercial-grade OCR and Document Parsing systems. It focuses on General-OCR tasks including text detection and recognition, formula and table recognition, as well as document parsing and understanding.\n\n**2. Template Matching OCR Projects**\n- NLGRF/template-matching-ocr - uses correlation-based template matching with OpenCV's matchTemplate function to classify digits by comparing ROI regions against reference templates\n- SaurabhThube/ocr-Template-matching - uses scanlines and templates with accuracy over 80%\n\n**3. FraudGuard** - A system that uses OCR to extract text and computer vision techniques to map document layout, then uses SIFT (Scale-Invariant Feature Transform) to match documents to pre-defined templates.\n\n**4. docTR (Document Text Recognition)** - A seamless OCR library with functional routes for detection, recognition, OCR, and key information extraction.\n\n**5. pdf2pdfocr** - A tool to OCR a PDF and add a text layer in the original file using only open source tools.\n\nThese projects demonstrate various approaches to document template matching with OCR, from simple template correlation methods using OpenCV to more sophisticated deep learning-based document parsing systems.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"1bc515b4-d6e9-40be-9a56-2cddb95c2a5f","timestamp":"2026-02-12T16:56:13.399Z","sourceToolAssistantUUID":"f13a4cb5-1321-4ce2-bdc0-a534cb65b6c9"}
{"parentUuid":"7d845233-9fe4-4cf2-bdba-1ab99e8cdd07","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01K43jDJJUwMKviKgtSJqJNu","type":"tool_result","content":"Web search results for query: \"PDF form data extraction open source python library 2025\"\n\nLinks: [{\"title\":\"I Tested 7 Python PDF Extractors So You Don’t Have To (2025 Edition) | by Aman Kumar | Medium\",\"url\":\"https://onlyoneaman.medium.com/i-tested-7-python-pdf-extractors-so-you-dont-have-to-2025-edition-c88013922257\"},{\"title\":\"GitHub - opendatalab/MinerU: Transforms complex documents like PDFs into LLM-ready markdown/JSON for your Agentic workflows.\",\"url\":\"https://github.com/opendatalab/MinerU\"},{\"title\":\"GitHub - opendatalab/PDF-Extract-Kit: A Comprehensive Toolkit for High-Quality PDF Content Extraction\",\"url\":\"https://github.com/opendatalab/PDF-Extract-Kit\"},{\"title\":\"GitHub - pymupdf/PyMuPDF: PyMuPDF is a high performance Python library for data extraction, analysis, conversion & manipulation of PDF (and other) documents.\",\"url\":\"https://github.com/pymupdf/PyMuPDF\"},{\"title\":\"Data Extraction from Unstructured PDFs - Analytics Vidhya\",\"url\":\"https://www.analyticsvidhya.com/blog/2021/06/data-extraction-from-unstructured-pdfs/\"},{\"title\":\"Open Source Python Library to Create & Manage PDF Files\",\"url\":\"https://products.fileformat.com/pdf/python/pdfrw/\"},{\"title\":\"How to extract text from a PDF using PyMuPDF and Python\",\"url\":\"https://www.nutrient.io/blog/extract-text-from-pdf-pymupdf/\"},{\"title\":\"Extracting data from PDFs: A comprehensive guide to techniques and tools\",\"url\":\"https://www.nutrient.io/blog/pdf-data-extraction-developer-guide/\"},{\"title\":\"How to Process PDFs in Python: A Step-by-Step Guide | Unstructured\",\"url\":\"https://unstructured.io/blog/how-to-process-pdf-in-python\"},{\"title\":\"Python, Open-Source Libraries for Efficient PDF Management\",\"url\":\"https://dzone.com/articles/python-open-source-libraries-pdf-management\"}]\n\nI found comprehensive information about PDF form data extraction open source Python libraries in 2025. Here are the key findings:\n\n## Popular Open Source Python Libraries (2025)\n\n**For General PDF Data Extraction:**\n\nPyMuPDF is a high performance Python library for data extraction, analysis, conversion & manipulation of PDF (and other) documents. It works well for native text PDFs, but it requires additional development for OCR integration, table detection, form handling, and error recovery.\n\n**For Structured Data & Forms:**\n\nPDFplumber is designed for extracting structured data from PDFs, such as tables and forms. pdfplumber gives you low-level access to text boxes and lines, so you can handle tricky or nested tables with precision.\n\n**For Table Extraction:**\n\nCamelot automatically finds tables in two modes: Lattice (for tables with drawn cell borders) and Stream (for tables that rely on spacing), and exports to pandas DataFrames, CSV, JSON.\n\n**For Complex Document Processing:**\n\nMinerU is a tool that converts PDFs into machine-readable formats (e.g., markdown, JSON), allowing for easy extraction into any format.\n\n## 2025 Performance Benchmarks\n\nRecent tests showed textract (0.21s) with OCR capabilities, pypdfium2 (0.003s) with blazing speed and clean basic text, pypdf (0.024s) with reliable extraction, and pdfplumber (0.10s) that's good for tables.\n\n## Recommendation\n\nThe \"best\" extractor depends entirely on what you're building and how you'll use the extracted text. For form-specific extraction, PDFplumber appears to be the most targeted solution among the open source options.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"b41dc5ae-63f0-4332-9803-58e26775c56d","timestamp":"2026-02-12T16:56:15.214Z","sourceToolAssistantUUID":"7d845233-9fe4-4cf2-bdba-1ab99e8cdd07"}
{"parentUuid":"dac07e4f-fe09-4578-96e9-2e6bdf8b762b","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Et2Yhds7dwV86ETP9PooDf","type":"tool_result","content":"Web search results for query: \"structured data extraction scanned forms open source template-driven\"\n\nLinks: [{\"title\":\"Guide to Accurate AI Document Processing in 2025: Unstract\",\"url\":\"https://unstract.com/blog/ai-document-processing-with-unstract/\"},{\"title\":\"Extracting data from PDFs: A comprehensive guide to techniques and tools\",\"url\":\"https://www.nutrient.io/blog/pdf-data-extraction-developer-guide/\"},{\"title\":\"Open-Source Document Extraction: Unstract, DeepSeek & PostgreSQL\",\"url\":\"https://unstract.com/blog/open-source-document-data-extraction-with-unstract-deepseek/\"},{\"title\":\"GitHub - ucbepic/TWIX: TWIX is an open-source data extraction tool that reconstructs structured data from documents at scale, accurately and at low cost, by inferring the shared underlying visual template across documents\",\"url\":\"https://github.com/ucbepic/TWIX\"},{\"title\":\"LLMs for Structured Data Extraction from PDFs in 2026\",\"url\":\"https://unstract.com/blog/comparing-approaches-for-using-llms-for-structured-data-extraction-from-pdfs/\"},{\"title\":\"Data Extraction API For Documents - The Complete Guide (2026) | Parseur®\",\"url\":\"https://parseur.com/blog/data-extraction-api\"},{\"title\":\"Extract PDF Data Using AI – Free Template\",\"url\":\"https://parabola.io/tool/how-to-use-ai-to-automatically-extract-your-pdf-data\"},{\"title\":\"Open Source Framework for Data Extraction and Structuring\",\"url\":\"https://www.parsee.ai/\"},{\"title\":\"Structured Data Extraction In 3 Steps (Beta) | LlamaIndex\",\"url\":\"https://www.llamaindex.ai/blog/introducing-llamaextract-unlocking-structured-data-extraction-in-just-a-few-clicks\"},{\"title\":\"Form Data Extraction: From OCR to Deep Learning\",\"url\":\"https://nanonets.com/blog/form-data-extraction/\"}]\n\nBased on the search results, here's what I found regarding structured data extraction from scanned forms using open-source, template-driven approaches:\n\n## Key Open-Source Tools\n\n**TWIX** is a particularly relevant option for template-driven extraction. TWIX is a tool for automatically extracting structured data from templatized documents that are programmatically generated by populating fields in a visual template, offering a scalable solution with high accuracy and low cost. It provides a Python package for extracting structured data from documents step by step designed for production pipeline deployment, with an interactive UI playground for data extraction that allows users to edit the inferred template.\n\n**Unstract** is another popular open-source platform. Unstract is an open-source no-code LLM platform to launch APIs and ETL pipelines to structure unstructured documents. It processes PDFs, scanned images, forms, and even handwritten text to guarantee precise AI document extraction.\n\n**Parsee** offers template-based extraction. Parsee aims to be a simple, opinionated framework for easily extracting and structuring data from the most common sources of unstructured data, and you can start extracting data instantly by using LLMs, which can serve as \"universal\" models.\n\n## Other Approaches\n\nTesseract is an open-source OCR engine developed by HP that you can use to convert scanned documents such as paper invoices, receipts, and checks into searchable, editable digital files.\n\nThese tools vary in their approach—some use template inference, others use LLM-based prompting, and traditional OCR tools like Tesseract focus on text extraction from scanned images.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"bdeb8e79-bf60-40bd-b254-21604daa612b","timestamp":"2026-02-12T16:56:15.327Z","sourceToolAssistantUUID":"dac07e4f-fe09-4578-96e9-2e6bdf8b762b"}
{"parentUuid":"ec294f1b-fd1e-4b73-b049-63eb3345b6a6","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01WrA5K8Tp6dqpHDyvUHMgtu","type":"tool_result","content":"Web search results for query: \"layout-based document classification open source python\"\n\nLinks: [{\"title\":\"deepdoctection - An Open Source Framework for Document AI\",\"url\":\"https://konfuzio.com/en/deepdoctection/\"},{\"title\":\"document-layout-analysis · GitHub Topics · GitHub\",\"url\":\"https://github.com/topics/document-layout-analysis\"},{\"title\":\"GitHub - tstanislawek/awesome-document-understanding: A curated list of resources for Document Understanding (DU) topic\",\"url\":\"https://github.com/tstanislawek/awesome-document-understanding\"},{\"title\":\"Document Layout Analysis - Basic articles and bibliography\",\"url\":\"https://konfuzio.com/en/document-layout-analysis/\"},{\"title\":\"New open source AI model for document segmentation ...\",\"url\":\"https://www.aryn.ai/post/new-open-source-ai-model-for-document-segmentation-and-unstructured-etl\"},{\"title\":\"document-classification · GitHub Topics · GitHub\",\"url\":\"https://github.com/topics/document-classification\"},{\"title\":\"Melissa Dell | Layout Parser\",\"url\":\"https://dell-research-harvard.github.io/resources/layout-parser\"},{\"title\":\"Layout Parser\",\"url\":\"https://layout-parser.github.io/\"},{\"title\":\"Generative AI for Documents Classification\",\"url\":\"https://www.altimetrik.com/blog/generative-ai-for-document-classification\"},{\"title\":\"Open Source Data Labeling | Label Studio\",\"url\":\"https://labelstud.io/\"}]\n\nBased on the search results, here are the main open source Python tools and frameworks for layout-based document classification:\n\n## Key Open Source Python Tools\n\n**1. Layout Parser**\nLayout Parser is a Python library that provides a wide range of pre-trained deep learning models to recognize the layout of a document image, using state-of-the-art machine learning models to provide detailed and accurate analysis of document layout. Layout Parser is implemented with simple APIs and can perform off-the-shelf layout analysis with four lines of Python code. Layout Parser comes with full support for customized layout model training on your own dataset, enabling optimal prediction accuracy on your own dataset, with support for training from scratch or fine-tuning from an existing model.\n\n**2. Deepdoctection**\nDeepdoctection is a Python-based open source framework that supports you in extracting information from documents. Unlike common Natural Language Processing frameworks, deepdoctection offers the possibility to create pipelines where multimodal models can be used that read in image information in addition to text, including position information of words and text segments or the entire image. With deepdoctection you can use these models in combination with OCR for document classification or entity extraction.\n\n**3. Sycamore with Aryn Partitioner**\nA new open source, Apache v2.0 AI model for high-fidelity document segmentation has been released, with the DETR AI model that powers the partitioner being 100% open source with an Apache v2.0 license. The new Aryn Partitioner is included in the latest containerized release and Sycamore Python library, and currently operates on PDFs.\n\n**4. Label Studio**\nLabel Studio can classify documents into one or multiple categories using taxonomies of up to 10000 classes.\n\nThese tools support various approaches to document classification, from layout analysis to multimodal models that combine visual and textual information.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"c1a2f171-4d0f-4d0a-9626-9888632abfdc","timestamp":"2026-02-12T16:56:15.329Z","sourceToolAssistantUUID":"ec294f1b-fd1e-4b73-b049-63eb3345b6a6"}
{"parentUuid":"c1a2f171-4d0f-4d0a-9626-9888632abfdc","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_017H9Y3CDDuEqGS14nviRwt2","type":"message","role":"assistant","content":[{"type":"text","text":"Good, I have initial results. Let me now search for the specific projects and technologies you mentioned."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":909,"cache_creation_input_tokens":3996,"cache_read_input_tokens":11781,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":3996},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XXN3TATSGKwjxdqZ6E","type":"assistant","uuid":"a314c316-2355-4db6-924e-141ab4009580","timestamp":"2026-02-12T16:56:18.850Z"}
{"parentUuid":"a314c316-2355-4db6-924e-141ab4009580","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_017H9Y3CDDuEqGS14nviRwt2","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Q2i2M3hgzYwsmAyJ7JgHfX","name":"WebSearch","input":{"query":"docTR document text recognition GitHub stars architecture 2025"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":909,"cache_creation_input_tokens":3996,"cache_read_input_tokens":11781,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":3996},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XXN3TATSGKwjxdqZ6E","type":"assistant","uuid":"ce126220-ada8-4421-9f75-adc70966abd4","timestamp":"2026-02-12T16:56:18.898Z"}
{"parentUuid":"ce126220-ada8-4421-9f75-adc70966abd4","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_017H9Y3CDDuEqGS14nviRwt2","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01F6fJN5WaManGXjDxvvMCRi","name":"WebSearch","input":{"query":"PaddleOCR PP-Structure form recognition table extraction GitHub"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":909,"cache_creation_input_tokens":3996,"cache_read_input_tokens":11781,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":3996},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XXN3TATSGKwjxdqZ6E","type":"assistant","uuid":"d08b3b93-c8ef-445e-841e-a8dc67a03b0a","timestamp":"2026-02-12T16:56:19.602Z"}
{"parentUuid":"d08b3b93-c8ef-445e-841e-a8dc67a03b0a","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_017H9Y3CDDuEqGS14nviRwt2","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01NTmgSfiVJqhQxbXntwY5N8","name":"WebSearch","input":{"query":"LayoutLMv3 LayoutParser document understanding open source form extraction"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":909,"cache_creation_input_tokens":3996,"cache_read_input_tokens":11781,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":3996},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XXN3TATSGKwjxdqZ6E","type":"assistant","uuid":"9edf3026-db65-4d54-9dbf-68418a147640","timestamp":"2026-02-12T16:56:20.370Z"}
{"parentUuid":"9edf3026-db65-4d54-9dbf-68418a147640","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_017H9Y3CDDuEqGS14nviRwt2","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_011CYyaLzQNokGKB3FBbUgST","name":"WebSearch","input":{"query":"Docling IBM document extraction open source GitHub 2025"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":909,"cache_creation_input_tokens":3996,"cache_read_input_tokens":11781,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":3996},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XXN3TATSGKwjxdqZ6E","type":"assistant","uuid":"a1051f10-7710-4c1c-adf5-a4672caad03e","timestamp":"2026-02-12T16:56:21.137Z"}
{"parentUuid":"a1051f10-7710-4c1c-adf5-a4672caad03e","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_017H9Y3CDDuEqGS14nviRwt2","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_015G56bjiCM9E6KjUsNwF4Be","name":"WebSearch","input":{"query":"marker-pdf PDF extraction open source GitHub architecture"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":909,"cache_creation_input_tokens":3996,"cache_read_input_tokens":11781,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":3996},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XXN3TATSGKwjxdqZ6E","type":"assistant","uuid":"b3c10ce5-374a-4d84-a51b-bdc4363bb800","timestamp":"2026-02-12T16:56:21.751Z"}
{"parentUuid":"ce126220-ada8-4421-9f75-adc70966abd4","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Q2i2M3hgzYwsmAyJ7JgHfX","type":"tool_result","content":"Web search results for query: \"docTR document text recognition GitHub stars architecture 2025\"\n\nLinks: [{\"title\":\"GitHub - mindee/doctr: docTR (Document Text Recognition) - a seamless, high-performing & accessible library for OCR-related tasks powered by Deep Learning.\",\"url\":\"https://github.com/mindee/doctr\"},{\"title\":\"docTR: Document Text Recognition\",\"url\":\"https://mindee.github.io/doctr/\"},{\"title\":\"GitHub - felixdittrich92/OnnxTR: OnnxTR a docTR (Document Text Recognition) library Onnx pipeline wrapper - for seamless, high-performing & accessible OCR\",\"url\":\"https://github.com/felixdittrich92/OnnxTR\"},{\"title\":\"Exploring docTR — Document OCR with Deep Learning | by Rishika Dwaraghanath | Medium\",\"url\":\"https://medium.com/@rthiruve/exploring-doctr-document-ocr-with-deep-learning-539c5a3c6971\"},{\"title\":\"Choosing the right model - docTR documentation\",\"url\":\"https://mindee.github.io/doctr/latest/using_doctr/using_models.html\"},{\"title\":\"GitHub - computationalmaterial/doctr-invoice_extractor: docTR (Document Text Recognition) - a seamless, high-performing & accessible library for OCR-related tasks powered by Deep Learning.\",\"url\":\"https://github.com/computationalmaterial/doctr-invoice_extractor\"},{\"title\":\"GitHub - fkatada/py-doctr: docTR (Document Text Recognition) - a seamless, high-performing & accessible library for OCR-related tasks powered by Deep Learning.\",\"url\":\"https://github.com/fkatada/py-doctr\"},{\"title\":\"Releases · mindee/doctr\",\"url\":\"https://github.com/mindee/doctr/releases\"},{\"title\":\"optical-character-recognition · GitHub Topics · GitHub\",\"url\":\"https://github.com/topics/optical-character-recognition\"},{\"title\":\"GitHub - iitb-research-code/indic-doctr: docTR (Document Text Recognition) - a seamless, high-performing & accessible library for OCR-related tasks powered by Deep Learning.\",\"url\":\"https://github.com/iitb-research-code/indic-doctr\"}]\n\nBased on the search results, here's information about docTR (Document Text Recognition):\n\n## GitHub Stars\n\ndocTR has 5.7k GitHub stars as of December 5, 2025.\n\n## Architecture\n\ndocTR achieves end-to-end OCR using a two-stage approach: text detection (localizing words), then text recognition (identifying all characters in the word). You can select the architecture used for text detection and the one for text recognition from the list of available implementations.\n\n### Supported Architectures\n\nThe library includes several key architectures for both detection and recognition:\n\n**Text Detection:**\n- DBNet: Real-time Scene Text Detection with Differentiable Binarization\n- LinkNet and FAST (Faster Arbitrarily-Shaped Text Detector)\n\n**Text Recognition:**\n- CRNN: An End-to-End Trainable Neural Network for Image-based Sequence Recognition\n- SAR (Show, Attend and Read), MASTER (Multi-Aspect Non-local Network), ViTSTR (Vision Transformer), and VIPTR (Vision Permutable Extractor)\n\n### Design Approach\n\ndocTR follows a clean two-step pipeline architecture — detect first, then recognize — a design that makes sense for real-world OCR systems where documents can be anything from neatly scanned PDFs to random pictures with weird angles and lighting.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"201edb0e-7ad0-4d52-8d9e-4ed339e5e10e","timestamp":"2026-02-12T16:56:26.319Z","sourceToolAssistantUUID":"ce126220-ada8-4421-9f75-adc70966abd4"}
{"parentUuid":"d08b3b93-c8ef-445e-841e-a8dc67a03b0a","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01F6fJN5WaManGXjDxvvMCRi","type":"tool_result","content":"Web search results for query: \"PaddleOCR PP-Structure form recognition table extraction GitHub\"\n\nLinks: [{\"title\":\"PaddleOCR/ppstructure/table/README.md at main · PaddlePaddle/PaddleOCR\",\"url\":\"https://github.com/PaddlePaddle/PaddleOCR/blob/main/ppstructure/table/README.md\"},{\"title\":\"PaddleOCR/ppstructure/README.md at release/2.7 · PaddlePaddle/PaddleOCR\",\"url\":\"https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.7/ppstructure/README.md\"},{\"title\":\"Having trouble doing complex Table Recognition and Extraction from image to excel · PaddlePaddle/PaddleOCR · Discussion #15090\",\"url\":\"https://github.com/PaddlePaddle/PaddleOCR/discussions/15090\"},{\"title\":\"GitHub - PaddlePaddle/PaddleOCR: Turn any PDF or image document into structured data for your AI. A powerful, lightweight OCR toolkit that bridges the gap between images/PDFs and LLMs. Supports 100+ languages.\",\"url\":\"https://github.com/PaddlePaddle/PaddleOCR\"},{\"title\":\"PaddleOCR/ppstructure/table/README.md at release/2.7 · PaddlePaddle/PaddleOCR\",\"url\":\"https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.7/ppstructure/table/README.md\"},{\"title\":\"PP-Structure - PaddlePaddle/PaddleOCR\",\"url\":\"https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.6/ppstructure/README.md\"},{\"title\":\"PaddleOCR/ppstructure/layout/README.md at main · PaddlePaddle/PaddleOCR\",\"url\":\"https://github.com/PaddlePaddle/PaddleOCR/blob/main/ppstructure/layout/README.md\"},{\"title\":\"PaddleOCR_bopw/ppstructure/table/README.md at release/2.6 · anlab-ai/PaddleOCR_bopw\",\"url\":\"https://github.com/anlab-ai/PaddleOCR_bopw/blob/release/2.6/ppstructure/table/README.md\"},{\"title\":\"PaddleOCR/ppstructure/kie/README.md at main · PaddlePaddle/PaddleOCR\",\"url\":\"https://github.com/PaddlePaddle/PaddleOCR/blob/main/ppstructure/kie/README.md\"},{\"title\":\"PaddleOCR-headless/ppstructure/docs/quickstart_en.md at release/2.6 · catalyst-cooperative/PaddleOCR-headless\",\"url\":\"https://github.com/catalyst-cooperative/PaddleOCR-headless/blob/release/2.6/ppstructure/docs/quickstart_en.md\"}]\n\nBased on the search results, here's what I found about PaddleOCR PP-Structure form recognition and table extraction:\n\n## Overview\n\nPP-Structure is an intelligent document analysis system developed by the PaddleOCR team, which aims to help developers better complete tasks related to document understanding such as layout analysis and table recognition.\n\n## Key Capabilities\n\nPP-StructureV2 supports structured table recognition, and output the final result to Excel file; Support multimodal-based Key Information Extraction (KIE) tasks - Semantic Entity Recognition (SER) and Relation Extraction (RE); Support layout recovery, that is, restore the document in word or pdf format with the same layout as the original image.\n\n## Table Extraction Implementation\n\nThe system uses a pipeline approach with multiple components:\n\n1. **Text Detection**: PP-OCRv3 text detection model\n2. **Text Recognition**: PP-OCRv3 text recognition model  \n3. **Table Structure Recognition**: The PP-StructureV2 form recognition model (ch_ppstructure_mobile_v2.0_SLANet_infer)\n\nPP-StructureV2 supports independent use or flexible collocation of each module. For example, you can use layout analysis alone or table recognition alone.\n\n## GitHub Repository\n\nThe official repository is available at: **https://github.com/PaddlePaddle/PaddleOCR** with PP-Structure resources located in the `/ppstructure` directory, including dedicated documentation for table recognition at `/ppstructure/table/README.md`.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"731d3027-4b14-4176-9b6e-d72fd8c371a1","timestamp":"2026-02-12T16:56:28.233Z","sourceToolAssistantUUID":"d08b3b93-c8ef-445e-841e-a8dc67a03b0a"}
{"parentUuid":"9edf3026-db65-4d54-9dbf-68418a147640","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01NTmgSfiVJqhQxbXntwY5N8","type":"tool_result","content":"Web search results for query: \"LayoutLMv3 LayoutParser document understanding open source form extraction\"\n\nLinks: [{\"title\":\"microsoft/layoutlmv3-base · Hugging Face\",\"url\":\"https://huggingface.co/microsoft/layoutlmv3-base\"},{\"title\":\"LayoutLMv3\",\"url\":\"https://huggingface.co/docs/transformers/en/model_doc/layoutlmv3\"},{\"title\":\"Power of LayoutLM for Text Extraction | Nitor Infotech\",\"url\":\"https://www.nitorinfotech.com/blog/how-can-layoutlm-transform-text-extraction/\"},{\"title\":\"GitHub - Layout-Parser/layout-parser: A Unified Toolkit for Deep Learning Based Document Image Analysis\",\"url\":\"https://github.com/Layout-Parser/layout-parser\"},{\"title\":\"LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking\",\"url\":\"https://arxiv.org/pdf/2204.08387\"},{\"title\":\"LayoutLMv3 role in Document Layout Understanding - 2024\",\"url\":\"https://ubiai.tools/the-role-of-layoutlmv3-in-document-layout-understanding/\"},{\"title\":\"LayoutLMv3: from zero to hero — Part 1 | by Shiva Rama | Medium\",\"url\":\"https://medium.com/@shivarama/layoutlmv3-from-zero-to-hero-part-1-85d05818eec4\"},{\"title\":\"Layout Parser\",\"url\":\"https://layout-parser.github.io/\"},{\"title\":\"Architecting AI for Documents: A Deep Dive into LayoutLM - Informed\",\"url\":\"https://informediq.com/architecting-ai-for-documents-a-deep-dive-into-layoutlm/\"},{\"title\":\"The Role of LayoutLMv3 in Document Layout Understanding in 2024 | by Wiem Souai | UBIAI NLP | Medium\",\"url\":\"https://medium.com/ubiai-nlp/the-role-of-layoutlmv3-in-document-layout-understanding-in-2024-46d505105cfb\"}]\n\nBased on the search results, here's what I found about LayoutLMv3, LayoutParser, document understanding, open source, and form extraction:\n\n## LayoutLMv3\n\nLayoutLMv3 is a pre-trained multimodal Transformer for Document AI with unified text and image masking. LayoutLMv3 can be fine-tuned for both text-centric tasks, including form understanding, receipt understanding, and document visual question answering, and image-centric tasks such as document image classification and document layout analysis.\n\nLayoutLMv3 is a multimodal pre-trained model developed by Microsoft Research and built on foundations of the Transformer, with a BERT-like architecture and doesn't rely on a pre-trained CNN or Faster R-CNN backbone to extract visual features. LayoutLMv3 is an open-source model, available for free.\n\n## LayoutParser\n\nLayoutParser aims to provide a wide range of tools that aims to streamline Document Image Analysis (DIA) tasks. LayoutParser provides a rich repository of deep learning models for layout detection as well as a set of unified APIs for using them. LayoutParser is also a open platform that enables the sharing of layout detection models and DIA pipelines among the community.\n\n## Form Extraction\n\nExperimental results show that LayoutLMv3 achieves state-of-the-art performance not only in text-centric tasks, including form understanding, receipt understanding, and document visual question answering, but also in image-centric tasks such as document image classification and document layout analysis.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"50188ab6-9336-4c65-ad16-82e9d4c6e69f","timestamp":"2026-02-12T16:56:29.189Z","sourceToolAssistantUUID":"9edf3026-db65-4d54-9dbf-68418a147640"}
{"parentUuid":"a1051f10-7710-4c1c-adf5-a4672caad03e","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_011CYyaLzQNokGKB3FBbUgST","type":"tool_result","content":"Web search results for query: \"Docling IBM document extraction open source GitHub 2025\"\n\nLinks: [{\"title\":\"GitHub - docling-project/docling: Get your documents ready for gen AI\",\"url\":\"https://github.com/docling-project/docling\"},{\"title\":\"Documentation - Docling\",\"url\":\"https://docling-project.github.io/docling/\"},{\"title\":\"IBM is open-sourcing a new toolkit for document conversion - IBM Research\",\"url\":\"https://research.ibm.com/blog/docling-generative-AI\"},{\"title\":\"IBM Granite-Docling: End-to-end document understanding\",\"url\":\"https://www.ibm.com/new/announcements/granite-docling-end-to-end-document-conversion\"},{\"title\":\"ibm-granite/granite-docling-258M · Hugging Face\",\"url\":\"https://huggingface.co/ibm-granite/granite-docling-258M\"},{\"title\":\"GitHub - IBM/docling-graph: Transform unstructured documents into validated, rich and queryable knowledge graphs.\",\"url\":\"https://github.com/IBM/docling-graph\"},{\"title\":\"IBM Deep Search · GitHub\",\"url\":\"https://github.com/DS4SD/\"},{\"title\":\"IBM AI Releases Granite-Docling-258M: An Open-Source, Enterprise-Ready Document AI Model - MarkTechPost\",\"url\":\"https://www.marktechpost.com/2025/09/17/ibm-ai-releases-granite-docling-258m-an-open-source-enterprise-ready-document-ai-model/\"},{\"title\":\"Docling vs. Graphlit: When Open-Source Extraction Falls Short\",\"url\":\"https://www.graphlit.com/vs/docling\"},{\"title\":\"GitHub - docling-project/docling-parse: Simple package to extract text with coordinates from programmatic PDFs\",\"url\":\"https://github.com/docling-project/docling-parse\"}]\n\nBased on the search results, here's what I found about Docling, IBM's open-source document extraction project:\n\n## Overview\n\nDocling was started by the AI for knowledge team at IBM Research Zurich and is hosted as a project in the LF AI & Data Foundation. Docling is IBM's new open-source toolkit designed to more easily unearth information for generative AI applications.\n\n## Key Features\n\nDocling simplifies document processing, parsing diverse formats — including advanced PDF understanding — and providing seamless integrations with the gen AI ecosystem. The toolkit supports parsing of multiple document formats including PDF, DOCX, PPTX, XLSX, HTML, WAV, MP3, WebVTT, images (PNG, TIFF, JPEG, ...), LaTeX, and more.\n\n## GitHub Repository\n\nThe main Docling project is available on GitHub at [github.com/docling-project/docling](https://github.com/docling-project/docling). Since it was open-sourced in July, Docling has received more than 8,000 stars on GitHub.\n\n## 2025 Updates\n\nIBM has released Granite-Docling-258M, an open-source (Apache-2.0) vision-language model designed specifically for end-to-end document conversion. Granite-Docling is a product-ready evolution of the experimental SmolDocling-256M-preview model released by IBM Research in partnership with Hugging Face in March 2025.\n\nThere's also docling-graph, which transforms unstructured documents into validated, rich and queryable knowledge graphs.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"5a7cbce3-775a-4a7b-8562-397a7da67d6d","timestamp":"2026-02-12T16:56:29.445Z","sourceToolAssistantUUID":"a1051f10-7710-4c1c-adf5-a4672caad03e"}
{"parentUuid":"b3c10ce5-374a-4d84-a51b-bdc4363bb800","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_015G56bjiCM9E6KjUsNwF4Be","type":"tool_result","content":"Web search results for query: \"marker-pdf PDF extraction open source GitHub architecture\"\n\nLinks: [{\"title\":\"GitHub - adithya-s-k/marker-api: Easily deployable 🚀 API to convert PDF to markdown quickly with high accuracy.\",\"url\":\"https://github.com/adithya-s-k/marker-api\"},{\"title\":\"GitHub - cuuupid/cog-marker: Convert PDF to markdown quickly with high accuracy\",\"url\":\"https://github.com/cuuupid/cog-marker\"},{\"title\":\"marker-pdf · PyPI\",\"url\":\"https://pypi.org/project/marker-pdf/\"},{\"title\":\"GitHub - swswsws583/marker-solo: Convert PDF to markdown quickly with high accuracy\",\"url\":\"https://github.com/swswsws583/marker-solo\"},{\"title\":\"GitHub - L3-N0X/obsidian-marker: Make use of different AI models to convert your pdfs into markdown with perfect ocr, latex formulas, tables, images and more! Supports Mistral AI OCR (free) and self hosted variants!\",\"url\":\"https://github.com/L3-N0X/obsidian-marker\"},{\"title\":\"GitHub - datalab-to/marker: Convert PDF to markdown + JSON quickly with high accuracy\",\"url\":\"https://github.com/VikParuchuri/marker\"},{\"title\":\"GitHub - sddai/markerPDF: Convert PDF to markdown quickly with high accuracy\",\"url\":\"https://github.com/sddai/markerPDF\"},{\"title\":\"Deep Dive into Open Source PDF to Markdown Tools: Marker, MinerU, and Alternatives | Jimmy Song\",\"url\":\"https://jimmysong.io/blog/pdf-to-markdown-open-source-deep-dive/\"},{\"title\":\"GitHub - pakkiraja/marker-pdf-llm: Convert PDF to markdown quickly with high accuracy\",\"url\":\"https://github.com/pakkiraja/marker-pdf-llm\"},{\"title\":\"GitHub - kkeenee/VikParuchuri-marker: Convert PDF to markdown quickly with high accuracy\",\"url\":\"https://github.com/kkeenee/VikParuchuri-marker\"}]\n\nBased on the search results, here's what I found about marker-pdf:\n\n## Overview\n\nMarker is an open-source tool that converts PDF to markdown + JSON quickly with high accuracy. It's available on GitHub at VikParuchuri/marker, and you can install it using poetry.\n\n## Architecture\n\nMarker is designed to be easy to extend. The core units of marker are: Providers (which provide information from source files like PDFs), Builders (which generate the initial document blocks and fill in text), Processors (which process specific blocks, for example the table formatter), Renderers (which use blocks to render output), Schema (classes for all block types), and Converters.\n\n## Key Features\n\n- Significant performance in batch mode with a projected throughput of 25 pages/second on an H100, and 122 pages per second with parallel processing\n- Optional LLM integration that can merge tables across pages, handle inline math, format tables properly, and extract values from forms\n- Advanced table extraction using the TableConverter with performance measured against the FinTabNet dataset\n- Benchmarks favorably compared to cloud services like Llamaparse and Mathpix, as well as other open source tools\n\n## Performance Considerations\n\nMarker takes about 2GB of VRAM on average per task, so you can convert 24 documents in parallel on an A6000, though this may vary with newer GPU architectures.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"6670662a-ab29-44b3-9822-f53d632844e3","timestamp":"2026-02-12T16:56:30.888Z","sourceToolAssistantUUID":"b3c10ce5-374a-4d84-a51b-bdc4363bb800"}
{"parentUuid":"6670662a-ab29-44b3-9822-f53d632844e3","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_01Qj2YA5akPtiPsNGryhPFDA","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me search for more specific projects and details."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":5913,"cache_read_input_tokens":15777,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":5913},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XYWZhXr27RQEnz4N3d","type":"assistant","uuid":"9bad0c76-24b1-45a4-b82c-2f2ac5e45992","timestamp":"2026-02-12T16:56:33.526Z"}
{"parentUuid":"9bad0c76-24b1-45a4-b82c-2f2ac5e45992","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_01Qj2YA5akPtiPsNGryhPFDA","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01F7QSYbSh2rG2dccE3eWbRh","name":"WebSearch","input":{"query":"TWIX ucbepic template document extraction GitHub architecture how it works"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":5913,"cache_read_input_tokens":15777,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":5913},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XYWZhXr27RQEnz4N3d","type":"assistant","uuid":"fec471e6-6b35-4f23-9734-73d21d039ddb","timestamp":"2026-02-12T16:56:34.439Z"}
{"parentUuid":"fec471e6-6b35-4f23-9734-73d21d039ddb","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_01Qj2YA5akPtiPsNGryhPFDA","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_012ygMNAZ1Pq8zibWq5yPZya","name":"WebSearch","input":{"query":"deepdoctection document AI framework GitHub stars architecture"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":5913,"cache_read_input_tokens":15777,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":5913},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XYWZhXr27RQEnz4N3d","type":"assistant","uuid":"fa29e3f1-61b4-4676-a203-b9519e3174ec","timestamp":"2026-02-12T16:56:35.109Z"}
{"parentUuid":"fa29e3f1-61b4-4676-a203-b9519e3174ec","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_01Qj2YA5akPtiPsNGryhPFDA","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01XP1fZeKeiR2qdHzP43nJ41","name":"WebSearch","input":{"query":"Camelot tabula pdfplumber form field extraction comparison 2025"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":5913,"cache_read_input_tokens":15777,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":5913},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XYWZhXr27RQEnz4N3d","type":"assistant","uuid":"c6ea21c9-359c-4daa-9ad0-6709d87ffe19","timestamp":"2026-02-12T16:56:35.925Z"}
{"parentUuid":"c6ea21c9-359c-4daa-9ad0-6709d87ffe19","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_01Qj2YA5akPtiPsNGryhPFDA","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01GiPDeAfTAjeCDiDLhUYbnX","name":"WebSearch","input":{"query":"Donut OCR-free document understanding model form extraction GitHub"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":5913,"cache_read_input_tokens":15777,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":5913},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XYWZhXr27RQEnz4N3d","type":"assistant","uuid":"78e23807-0f49-4870-ada2-42536bd03c92","timestamp":"2026-02-12T16:56:36.699Z"}
{"parentUuid":"78e23807-0f49-4870-ada2-42536bd03c92","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_01Qj2YA5akPtiPsNGryhPFDA","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01VjNRtyPavGbufKp1QF8okx","name":"WebSearch","input":{"query":"Unstract open source document extraction LLM GitHub architecture"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":5913,"cache_read_input_tokens":15777,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":5913},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XYWZhXr27RQEnz4N3d","type":"assistant","uuid":"4eac5070-5139-4788-94c5-5057cf981f10","timestamp":"2026-02-12T16:56:37.211Z"}
{"parentUuid":"fec471e6-6b35-4f23-9734-73d21d039ddb","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01F7QSYbSh2rG2dccE3eWbRh","type":"tool_result","content":"Web search results for query: \"TWIX ucbepic template document extraction GitHub architecture how it works\"\n\nLinks: [{\"title\":\"GitHub - ucbepic/TWIX: TWIX is an open-source data extraction tool that reconstructs structured data from documents at scale, accurately and at low cost, by inferring the shared underlying visual template across documents\",\"url\":\"https://github.com/ucbepic/TWIX\"},{\"title\":\"EPIC Data Lab · GitHub\",\"url\":\"https://github.com/ucbepic\"},{\"title\":\"ucbepic repositories · GitHub\",\"url\":\"https://github.com/orgs/ucbepic/repositories\"},{\"title\":\"Liberating Structured Data from PDF Prisons – The Data People Research Blog\",\"url\":\"https://data-people-group.github.io/blogs/2025/04/30/twix/\"},{\"title\":\"GitHub - stephenmcconnachie/starred\",\"url\":\"https://github.com/stephenmcconnachie/starred\"},{\"title\":\"document-data-extraction · GitHub Topics · GitHub\",\"url\":\"https://github.com/topics/document-data-extraction\"},{\"title\":\"TWIX: Automatically Reconstructing Structured Data from Templatized Documents\",\"url\":\"https://arxiv.org/pdf/2501.06659\"},{\"title\":\"Quick Start Tutorial - docetl docs\",\"url\":\"https://ucbepic.github.io/docetl/tutorial/\"},{\"title\":\"GitHub - ucbepic/docetl: A system for agentic LLM-powered data processing and ETL\",\"url\":\"https://github.com/ucbepic/docetl\"},{\"title\":\"Getting Involved - docetl docs\",\"url\":\"https://ucbepic.github.io/docetl/community/\"}]\n\nBased on the search results, here's an overview of TWIX and its architecture:\n\n## What is TWIX?\n\nTWIX is a tool for automatically extracting structured data from templatized documents that are programmatically generated by populating fields in a visual template, offering a scalable solution with high accuracy and low cost.\n\n## How It Works\n\nTWIX's workflow: Given a set of templatized documents, TWIX extracts raw text, infers the template, and then extracts structured data. The process has four main steps:\n\n1. **Phrase Extraction**: Use Optical Character Recognition (OCR) tool to extract text phrases and bounding boxes.\n\n2. **Field Inference**: Cluster phrases by their visual location patterns to identify field candidates.\n\n3. **Template Inference**: Infer the document's structure by inferring row labels with visual constraints based on inferred fields.\n\n4. **Data Extraction**: Apply the inferred template to extract structured data efficiently and accurately at scale.\n\n## Key Features\n\n- A Python package for extracting structured data from documents step by step, designed for production pipeline deployment, with optional user feedback to monitor and refine the extraction process.\n\n- An interactive UI playground for data extraction that allows users to edit the inferred template, enabling more confident and accurate extraction.\n\n- To infer the underlying template, TWIX requires at least two records (or two pages) created with that template.\n\n## Performance\n\nTWIX outperforms 6 state-of-the-art data extraction tools by over 25% in precision and recall, and is 520× faster and 3786× cheaper than the most competitive baseline, GPT-4 Vision.\n\nThe project is available on GitHub at [ucbepic/TWIX](https://github.com/ucbepic/TWIX).\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"7f61db49-2c1e-456d-80c4-716cd22bffcb","timestamp":"2026-02-12T16:56:43.473Z","sourceToolAssistantUUID":"fec471e6-6b35-4f23-9734-73d21d039ddb"}
{"parentUuid":"fa29e3f1-61b4-4676-a203-b9519e3174ec","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_012ygMNAZ1Pq8zibWq5yPZya","type":"tool_result","content":"Web search results for query: \"deepdoctection document AI framework GitHub stars architecture\"\n\nLinks: [{\"title\":\"GitHub - deepdoctection/deepdoctection: A Repo For Document AI\",\"url\":\"https://github.com/deepdoctection/deepdoctection\"},{\"title\":\"GitHub - diegosiqueir4/deepdoctection: A Repo For Document AI\",\"url\":\"https://github.com/diegosiqueir4/deepdoctection\"},{\"title\":\"deepdoctection - An Open Source Framework for Document AI\",\"url\":\"https://konfuzio.com/en/deepdoctection/\"},{\"title\":\"GitHub - tstanislawek/awesome-document-understanding: A curated list of resources for Document Understanding (DU) topic\",\"url\":\"https://github.com/tstanislawek/awesome-document-understanding\"},{\"title\":\"deepdoctection · GitHub\",\"url\":\"https://github.com/deepdoctection\"},{\"title\":\"Releases · deepdoctection/deepdoctection\",\"url\":\"https://github.com/deepdoctection/deepdoctection/releases\"},{\"title\":\"deepdoctection/README.md at master · deepdoctection/deepdoctection\",\"url\":\"https://github.com/deepdoctection/deepdoctection/blob/master/README.md\"},{\"title\":\"deepdoctection download | SourceForge.net\",\"url\":\"https://sourceforge.net/projects/deepdoctection.mirror/\"},{\"title\":\"deepdoctection repositories · GitHub\",\"url\":\"https://github.com/orgs/deepdoctection/repositories\"},{\"title\":\"GitHub All-Stars #5: deepagents -Architecture of Deep Reasoning for Agentic AI | by Artur Skowroński | GitHub All-Stars | Medium\",\"url\":\"https://medium.com/github-all-stars/github-all-stars-5-deepagents-architecture-of-deep-reasoning-for-agentic-ai-b77261a49bde\"}]\n\nBased on the search results, here's what I found about deepdoctection:\n\n## Overview\n\ndeepdoctection is a Python library that orchestrates Scan and PDF document layout analysis, OCR and document and token classification. It does not implement models but enables you to build pipelines using highly acknowledged libraries for object detection, OCR and selected NLP tasks and provides an integrated framework for fine-tuning, evaluating and running models.\n\n## GitHub Stars\n\nThe project has 2.9k stars on GitHub, indicating it has garnered significant community interest.\n\n## Architecture and Features\n\nThe framework includes document layout analysis and table recognition in PyTorch with Detectron2 and Transformers, OCR with support of Tesseract, DocTr and AWS Textract, Document and token classification with the LayoutLM family, LiLT and many Bert-style models including features like sliding windows.\n\nIn contrast to common Natural Language Processing frameworks, deepdoctection offers the possibility to create pipelines where multimodal models can be used that read in image information in addition to text.\n\n## Key Capabilities\n\nUsers can build and run a pipeline for document extraction tasks, develop their own document extraction workflow, fine-tune pre-trained models and use them seamlessly for inference. The framework is distributed under the Apache 2.0 License.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"548afd2e-0a9a-495e-9a92-3196fd1d89d4","timestamp":"2026-02-12T16:56:43.477Z","sourceToolAssistantUUID":"fa29e3f1-61b4-4676-a203-b9519e3174ec"}
{"parentUuid":"c6ea21c9-359c-4daa-9ad0-6709d87ffe19","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01XP1fZeKeiR2qdHzP43nJ41","type":"tool_result","content":"Web search results for query: \"Camelot tabula pdfplumber form field extraction comparison 2025\"\n\nLinks: [{\"title\":\"Comparison with other PDF Table Extraction libraries and tools · camelot-dev/camelot Wiki · GitHub\",\"url\":\"https://github.com/camelot-dev/camelot/wiki/Comparison-with-other-PDF-Table-Extraction-libraries-and-tools\"},{\"title\":\"Best Python Libraries to Extract Tables From PDF in 2026\",\"url\":\"https://unstract.com/blog/extract-tables-from-pdf-python/\"},{\"title\":\"Comparison with other PDF Table Extraction libraries and tools · atlanhq/camelot Wiki · GitHub\",\"url\":\"https://github.com/atlanhq/camelot/wiki/Comparison-with-other-PDF-Table-Extraction-libraries-and-tools\"},{\"title\":\"Comparing 6 Frameworks for Rule-based PDF parsing\",\"url\":\"https://www.ai-bites.net/comparing-6-frameworks-for-rule-based-pdf-parsing/\"},{\"title\":\"A Comparative Study of PDF Parsing Tools Across Diverse Document Categories\",\"url\":\"https://arxiv.org/html/2410.09871v1\"},{\"title\":\"Our search for the best tabular-data extraction tool in 2024, and what we found - Features - Source: An OpenNews project\",\"url\":\"https://source.opennews.org/articles/our-search-best-tabular-data-extraction-tool-2024-/\"},{\"title\":\"Stop Copy-Pasting. Turn PDFs into Data in Seconds\",\"url\":\"https://wangari.substack.com/p/stop-copy-pasting-turn-pdfs-into\"},{\"title\":\"PDF - Python package Tabula, PdfPlumber and Camelot to extract Text and Tables - KNIME Forum (82131) – KNIME Community Hub\",\"url\":\"https://hub.knime.com/mlauber71/spaces/Forum/2024/PDF%20-%20Python%20package%20Tabula,%20PdfPlumber%20and%20Camelot%20to%20extract%20Text%20and%20Tables%20-%20KNIME%20Forum%20(82131)~x7mTolZW1uXhU38B/current-state\"},{\"title\":\"Extracting data from PDFs: A comprehensive guide to techniques and tools\",\"url\":\"https://www.nutrient.io/blog/pdf-data-extraction-developer-guide/\"},{\"title\":\"GitHub - atlanhq/camelot: Camelot: PDF Table Extraction for Humans\",\"url\":\"https://github.com/atlanhq/camelot\"}]\n\nBased on the web search results, here's what I found regarding Camelot, Tabula, and pdfplumber for PDF form field extraction comparison in 2025:\n\n## Key Findings\n\n**Table Extraction Performance:**\n\nRecent comparisons include Camelot, Tabula, Pdfplumber, and Pdftables, with pdfplumber performing exceptionally well with complex tables, while Camelot and Tabula are easier to use but may require adjustments for more complex tables.\n\nTabula is good for simple tables, Camelot can handle more complex tables but requires more fine-tuning, while pdfplumber can handle the most complex tables though it cannot detect tables automatically and does not always preserve table structure well.\n\n**2024-2025 Comparative Study:**\n\nIn a comparative study across diverse document categories, Camelot achieved the highest score in the Tender category (0.72), while Tabula outperformed others in the Manual, Scientific, and Patent categories. While rule-based tools like Camelot excel in specific document types, transformer-based models demonstrate superior versatility and consistency across all categories.\n\n**Form Field Extraction:**\n\nThe search results primarily focus on **table extraction** rather than form field extraction specifically. pdfplumber works really well on clean, machine-generated PDFs with strong underlying text layer accuracy, and does an exceptional job at extracting lines, intersections, cells, and tables from documents.\n\n**Note:** The search results did not contain specific information comparing these tools for form field extraction specifically, which is a different task from table extraction.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"5531a9be-ec7d-401a-8167-862c637ef632","timestamp":"2026-02-12T16:56:43.881Z","sourceToolAssistantUUID":"c6ea21c9-359c-4daa-9ad0-6709d87ffe19"}
{"parentUuid":"78e23807-0f49-4870-ada2-42536bd03c92","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01GiPDeAfTAjeCDiDLhUYbnX","type":"tool_result","content":"Web search results for query: \"Donut OCR-free document understanding model form extraction GitHub\"\n\nLinks: [{\"title\":\"GitHub - clovaai/donut: Official Implementation of OCR-free Document Understanding Transformer (Donut) and Synthetic Document Generator (SynthDoG), ECCV 2022\",\"url\":\"https://github.com/clovaai/donut\"},{\"title\":\"GitHub - javier-marti-isasi/OCR-free-Document-Understanding-with-Donut-Transformer: This project tackles a real-world challenge of automating client document processing, with a focus on enhancing document classification, error detection, data extraction, and validation.\",\"url\":\"https://github.com/javier-marti-isasi/OCR-free-Document-Understanding-with-Donut-Transformer\"},{\"title\":\"Donut : Document Understanding Transformer without OCR\",\"url\":\"https://sangdooyun.github.io/data/kim2021donut.pdf\"},{\"title\":\"OCR-free Document Understanding Transformer\",\"url\":\"https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136880493.pdf\"},{\"title\":\"[2111.15664] OCR-free Document Understanding Transformer\",\"url\":\"https://arxiv.org/abs/2111.15664\"},{\"title\":\"Donut\",\"url\":\"https://huggingface.co/docs/transformers/en/model_doc/donut\"},{\"title\":\"GitHub - santoshvutukuri/donut-no-OCR: Official Implementation of OCR-free Document Understanding Transformer (Donut) and Synthetic Document Generator (SynthDoG), ECCV 2022\",\"url\":\"https://github.com/santoshvutukuri/donut-no-OCR\"},{\"title\":\"GitHub - hftuner/clovaai-donut: collection of notebooks for finetuning donut model for various visual document understanding tasks, using huggingface Trainer.\",\"url\":\"https://github.com/hftuner/clovaai-donut\"},{\"title\":\"GitHub - ThanhPham1987/donut-ocr: Official Implementation of OCR-free Document Understanding Transformer (Donut) and Synthetic Document Generator (SynthDoG), ECCV 2022\",\"url\":\"https://github.com/ThanhPham1987/donut-ocr\"},{\"title\":\"GitHub - hiimbach/Amazing-Donut: Official Implementation of OCR-free Document Understanding Transformer (Donut) and Synthetic Document Generator (SynthDoG), ECCV 2022\",\"url\":\"https://github.com/hiimbach/Amazing-Donut\"}]\n\nI found comprehensive information about the Donut OCR-free document understanding model. Here's what the search revealed:\n\n## Overview\n\nDonut, Document understanding transformer, is a new method of document understanding that utilizes an OCR-free end-to-end Transformer model. Donut does not require off-the-shelf OCR engines/APIs, yet it shows state-of-the-art performances on various document understanding tasks.\n\n## Official GitHub Repository\n\nThe official implementation is available at **github.com/clovaai/donut**, which contains the official implementation of both Donut and SynthDoG (a Synthetic Document Generator) published at ECCV 2022.\n\n## Architecture & Capabilities\n\nDonut consists of an image Transformer encoder and an autoregressive text Transformer decoder to perform document understanding tasks such as document image classification, form understanding and visual question answering. More specifically, Donut features vision encoder (Swin) and a text decoder (BART). Swin converts document images into embeddings and BART processes them into meaningful text sequences.\n\n## Key Advantages\n\nUnlike traditional approaches that extract text using OCR before processing, Donut employs an end-to-end Transformer-based architecture to directly analyze document images. This eliminates OCR-related inefficiencies making it more accurate and adaptable to diverse languages and formats.\n\n## Form Extraction\n\nFor form extraction tasks, a JSON format is adopted due to its high representation capacity. A token sequence is one-to-one invertible to a JSON data.\n\nThe official repository includes training code, pre-trained models, and the SynthDoG synthetic document generator for creating training data across multiple languages.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"c3a4d8b0-b2a4-47e4-b0f7-4dda98aff83f","timestamp":"2026-02-12T16:56:44.496Z","sourceToolAssistantUUID":"78e23807-0f49-4870-ada2-42536bd03c92"}
{"parentUuid":"4eac5070-5139-4788-94c5-5057cf981f10","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01VjNRtyPavGbufKp1QF8okx","type":"tool_result","content":"Web search results for query: \"Unstract open source document extraction LLM GitHub architecture\"\n\nLinks: [{\"title\":\"GitHub - Zipstack/unstract: No-code LLM Platform to launch APIs and ETL Pipelines to structure unstructured documents\",\"url\":\"https://github.com/Zipstack/unstract\"},{\"title\":\"Unstract: LLM Powered ETL for Unstructured Data\",\"url\":\"https://unstract.com/\"},{\"title\":\"LLMs for Structured Data Extraction from PDFs in 2026\",\"url\":\"https://unstract.com/blog/comparing-approaches-for-using-llms-for-structured-data-extraction-from-pdfs/\"},{\"title\":\"Open-Source Document Extraction: Unstract, DeepSeek & PostgreSQL\",\"url\":\"https://unstract.com/blog/open-source-document-data-extraction-with-unstract-deepseek/\"},{\"title\":\"Intelligent Document Processing with No-Code LLM ...\",\"url\":\"https://adasci.org/intelligent-document-processing-with-no-code-llm-platform-unstract/\"},{\"title\":\"GitHub - KatherLab/LLMAIx: Document Information Extraction & Anonymization using local LLMs\",\"url\":\"https://github.com/KatherLab/LLMAIx\"},{\"title\":\"Unstract Open Source Edition | Unstract Documentation\",\"url\":\"https://docs.unstract.com/unstract/editions/open_source_edition/\"},{\"title\":\"GitHub - Unstructured-IO/unstructured: Convert documents to structured data effortlessly. Unstructured is open-source ETL solution for transforming complex documents into clean, structured formats for language models. Visit our website to learn more about our enterprise grade Platform product for production grade workflows, partitioning, enrichments, chunking and embedding.\",\"url\":\"https://github.com/Unstructured-IO/unstructured\"},{\"title\":\"GitHub - Open-Source-Legal/OpenContracts: Enterprise-grade and API-first LLM workspace for unstructured documents, including data extraction, redaction, rights management, prompt playground, and more!\",\"url\":\"https://github.com/Open-Source-Legal/OpenContracts\"},{\"title\":\"GitHub - google/langextract: A Python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization.\",\"url\":\"https://github.com/google/langextract\"}]\n\nBased on the search results, here's what I found about Unstract:\n\n## Overview\n\nUnstract is an open-source, no-code platform purpose-built for extracting data from unstructured documents using LLMs, with high accuracy. The source code is available on GitHub at https://github.com/Zipstack/unstract.\n\n## Architecture & Key Components\n\nUnstract is designed to extract structured data from unstructured documents such as PDFs, images, and scanned files, providing a modular and flexible framework allowing users to customize their document processing pipeline.\n\nThe platform features an **AI stack agnostic design**. Users can integrate any Large Language Model (choose from open-source models like DeepSeek R1, Mistral, or Llama), any Vector Database (supports PGVector, ChromaDB, Weaviate, and more), any Embedding Model (works with models from Ollama, Hugging Face, or custom-built ones), and any Text Extractor (can be configured with LLMWhisperer, Unstructured.io, and LlamaParse).\n\n## Key Features\n\nUnstract uses two Large Language Models to ensure trustworthy output—you either get the right response or no response at all. The platform reduces LLM token usage by up to 8x, dramatically cutting costs.\n\nAdditional capabilities include:\n- Prompt Studio, a purpose-built environment that supercharges schema definition efforts, allowing comparison of outputs from different LLMs side-by-side and launching extraction APIs with a single click.\n- The ability to turn any document into JSON with an API call and deploy Prompt Studio projects as REST API endpoints.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"ef690cbf-6674-4f48-ad1d-5b9920c2b657","timestamp":"2026-02-12T16:56:45.418Z","sourceToolAssistantUUID":"4eac5070-5139-4788-94c5-5057cf981f10"}
{"parentUuid":"ef690cbf-6674-4f48-ad1d-5b9920c2b657","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_01JQVLrHVxmYQdkLebdpEmUW","type":"message","role":"assistant","content":[{"type":"text","text":"Let me get more details on several more projects and specific technical aspects."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":4982,"cache_read_input_tokens":21690,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":4982},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XZakzQm4aSmGmpcpd7","type":"assistant","uuid":"af9273d1-254d-461a-be6d-c43985b8e5c1","timestamp":"2026-02-12T16:56:48.304Z"}
{"parentUuid":"af9273d1-254d-461a-be6d-c43985b8e5c1","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_01JQVLrHVxmYQdkLebdpEmUW","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01EfyUvNYVLJ9EQ5XvgirxAE","name":"WebSearch","input":{"query":"MinerU PDF extraction toolkit GitHub stars architecture features 2025"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":4982,"cache_read_input_tokens":21690,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":4982},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XZakzQm4aSmGmpcpd7","type":"assistant","uuid":"14c1d04c-518f-459e-8aad-06523abd4a52","timestamp":"2026-02-12T16:56:49.308Z"}
{"parentUuid":"14c1d04c-518f-459e-8aad-06523abd4a52","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_01JQVLrHVxmYQdkLebdpEmUW","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_017sXgxaQyKdgY4zk3eArqtX","name":"WebSearch","input":{"query":"PDF-Extract-Kit opendatalab GitHub document parsing"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":4982,"cache_read_input_tokens":21690,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":4982},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XZakzQm4aSmGmpcpd7","type":"assistant","uuid":"98a2a17b-d1ef-4025-b1bd-b36b77b3c12c","timestamp":"2026-02-12T16:56:49.918Z"}
{"parentUuid":"98a2a17b-d1ef-4025-b1bd-b36b77b3c12c","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_01JQVLrHVxmYQdkLebdpEmUW","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01NXKinVRbnft9vpoxSWmq5W","name":"WebSearch","input":{"query":"Unstructured-IO unstructured document ETL open source GitHub architecture"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":4982,"cache_read_input_tokens":21690,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":4982},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XZakzQm4aSmGmpcpd7","type":"assistant","uuid":"1624b1fe-62d3-4b25-8c1c-55e33889e52d","timestamp":"2026-02-12T16:56:50.469Z"}
{"parentUuid":"1624b1fe-62d3-4b25-8c1c-55e33889e52d","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_01JQVLrHVxmYQdkLebdpEmUW","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01GESGxvQGcbjyKguPEjp1No","name":"WebSearch","input":{"query":"visual template mapping form extraction region-based OCR python open source 2025"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":4982,"cache_read_input_tokens":21690,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":4982},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XZakzQm4aSmGmpcpd7","type":"assistant","uuid":"6ac6930a-422f-4a66-bbae-eabee25305a4","timestamp":"2026-02-12T16:56:51.446Z"}
{"parentUuid":"6ac6930a-422f-4a66-bbae-eabee25305a4","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_01JQVLrHVxmYQdkLebdpEmUW","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01MKi14fX1epmDDzviwFE1Co","name":"WebSearch","input":{"query":"PyMuPDF form field extraction AcroForm widget annotation python"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":4982,"cache_read_input_tokens":21690,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":4982},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XZakzQm4aSmGmpcpd7","type":"assistant","uuid":"3672fcf7-da8b-48fc-8be4-b3d1b78c5024","timestamp":"2026-02-12T16:56:51.960Z"}
{"parentUuid":"98a2a17b-d1ef-4025-b1bd-b36b77b3c12c","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_017sXgxaQyKdgY4zk3eArqtX","type":"tool_result","content":"Web search results for query: \"PDF-Extract-Kit opendatalab GitHub document parsing\"\n\nLinks: [{\"title\":\"GitHub - opendatalab/PDF-Extract-Kit: A Comprehensive Toolkit for High-Quality PDF Content Extraction\",\"url\":\"https://github.com/opendatalab/PDF-Extract-Kit\"},{\"title\":\"PDF-Extract-Kit/README.md at main · opendatalab/PDF-Extract-Kit\",\"url\":\"https://github.com/opendatalab/PDF-Extract-Kit/blob/main/README.md\"},{\"title\":\"GitHub - opendatalab/MinerU: Transforms complex documents like PDFs into LLM-ready markdown/JSON for your Agentic workflows.\",\"url\":\"https://github.com/opendatalab/MinerU\"},{\"title\":\"GitHub - opendatalab/PDF-Extract-Kit: A Comprehensive Toolkit for High-Quality PDF Content Extraction\",\"url\":\"https://github-com.translate.goog/opendatalab/PDF-Extract-Kit?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es&_x_tr_pto=tc\"},{\"title\":\"Releases · opendatalab/PDF-Extract-Kit\",\"url\":\"https://github.com/opendatalab/PDF-Extract-Kit/releases\"},{\"title\":\"https://github.com/opendatalab/PDF-Extract-Kit | Ecosyste.ms: Awesome\",\"url\":\"https://awesome.ecosyste.ms/projects/github.com/opendatalab/PDF-Extract-Kit\"},{\"title\":\"Pull requests · opendatalab/PDF-Extract-Kit\",\"url\":\"https://github.com/opendatalab/PDF-Extract-Kit/pulls\"},{\"title\":\"opendatalab/PDF-Extract-Kit · Discussions\",\"url\":\"https://github.com/opendatalab/PDF-Extract-Kit/discussions\"},{\"title\":\"PDF-Extract-Kit/requirements.txt at main · opendatalab/PDF-Extract-Kit\",\"url\":\"https://github.com/opendatalab/PDF-Extract-Kit/blob/main/requirements.txt\"},{\"title\":\"PDF-Extract-Kit/README_zh-CN.md at main · opendatalab/PDF-Extract-Kit\",\"url\":\"https://github.com/opendatalab/PDF-Extract-Kit/blob/main/README_zh-CN.md\"}]\n\nBased on the search results, here's what I found about PDF-Extract-Kit from opendatalab on GitHub:\n\n## Overview\n\nPDF-Extract-Kit is a powerful open-source toolkit designed to efficiently extract high-quality content from complex and diverse PDF documents.\n\n## Key Features\n\nThe toolkit incorporates state-of-the-art models for layout detection, formula detection, formula recognition, OCR, and other core document parsing tasks. The flexible modular design allows users to easily combine and construct various applications by modifying configuration files and minimal code, making application building as straightforward as stacking blocks.\n\n## Document Parsing Capabilities\n\nThe toolkit is fine-tuned with diverse document annotation data to deliver high-quality results across various complex document types. The fine-tuned models achieve accurate extraction results on diverse PDF documents such as papers, textbooks, research reports, and financial reports, and demonstrate high robustness to challenges like blurring and watermarks.\n\n## Release History\n\nThe toolkit was released on July 1, 2024, as a comprehensive toolkit for high-quality PDF content extraction, including Layout Detection, Formula Detection, Formula Recognition, and OCR. The official release of PDF-Extract-Kit 1.0 was released on October 10, 2024, rebuilt with modularity for more convenient and flexible model usage.\n\n## Related Project\n\nFor reconstructing extracted content into new documents such as PDF to Markdown, the project references another GitHub project: MinerU.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"578c9bad-f494-4e26-b2fa-63b1b13c1e8a","timestamp":"2026-02-12T16:56:57.653Z","sourceToolAssistantUUID":"98a2a17b-d1ef-4025-b1bd-b36b77b3c12c"}
{"parentUuid":"1624b1fe-62d3-4b25-8c1c-55e33889e52d","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01NXKinVRbnft9vpoxSWmq5W","type":"tool_result","content":"Web search results for query: \"Unstructured-IO unstructured document ETL open source GitHub architecture\"\n\nLinks: [{\"title\":\"GitHub - Unstructured-IO/unstructured: Convert documents to structured data effortlessly. Unstructured is open-source ETL solution for transforming complex documents into clean, structured formats for language models. Visit our website to learn more about our enterprise grade Platform product for production grade workflows, partitioning, enrichments, chunking and embedding.\",\"url\":\"https://github.com/Unstructured-IO/unstructured\"},{\"title\":\"Unstructured · GitHub\",\"url\":\"https://github.com/Unstructured-IO\"},{\"title\":\"Releases · Unstructured-IO/unstructured\",\"url\":\"https://github.com/unstructured-io/unstructured/releases\"},{\"title\":\"Unstructured-IO repositories · GitHub\",\"url\":\"https://github.com/orgs/Unstructured-IO/repositories\"},{\"title\":\"Overview - Unstructured\",\"url\":\"https://docs.unstructured.io/open-source/introduction/overview\"},{\"title\":\"Unstract: LLM Powered ETL for Unstructured Data\",\"url\":\"https://unstract.com/\"},{\"title\":\"Operationalize Unstructured Data for Enterprise GenAI | Unstructured\",\"url\":\"https://unstructured.io/\"},{\"title\":\"unstructured/example-docs at main · Unstructured-IO/unstructured\",\"url\":\"https://github.com/Unstructured-IO/unstructured/tree/main/example-docs\"},{\"title\":\"GitHub - Zipstack/unstract: No-code LLM Platform to launch APIs and ETL Pipelines to structure unstructured documents\",\"url\":\"https://github.com/Zipstack/unstract\"},{\"title\":\"unstructuredio (unstructured.io)\",\"url\":\"https://huggingface.co/unstructuredio\"}]\n\nBased on the search results, here's information about Unstructured-IO, their document ETL solution, and architecture:\n\n## Overview\n\nUnstructured is an open-source ETL solution for transforming complex documents into clean, structured formats for language models. The unstructured library provides open-source components for ingesting and pre-processing images and text documents, such as PDFs, HTML, Word docs, and many more.\n\n## Core Components and Architecture\n\nThe core library handles partitioning, cleaning, and chunking 25+ documents types for LLM applications and connecting to source and destination data sources. Unstructured's modular functions and connectors form a cohesive system that simplifies data ingestion and pre-processing, making it adaptable to different platforms and efficient in transforming unstructured data into structured outputs.\n\n## Key Features\n\nThe Unstructured open source library offers an open-source toolkit designed to simplify the ingestion and pre-processing of diverse data formats, including images and text-based documents such as PDFs, HTML files, Word documents, and more, with a focus on optimizing data workflows for Large Language Models (LLMs).\n\nThe solution includes modular components: Partitioning bricks that break raw documents down into standard, structured elements; cleaning bricks that remove unwanted text from documents, such as boilerplate and sentence fragments; and staging bricks that format data for downstream tasks, such as ML inference and data labeling.\n\n## GitHub Organization\n\nThe Unstructured-IO organization includes an open source API that wraps the unstructured Python library, a Python client library for their API, and a JavaScript client library for their API.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"b0c2f2ae-eb4f-4827-8be0-5bf3fd9a35e1","timestamp":"2026-02-12T16:56:59.783Z","sourceToolAssistantUUID":"1624b1fe-62d3-4b25-8c1c-55e33889e52d"}
{"parentUuid":"3672fcf7-da8b-48fc-8be4-b3d1b78c5024","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01MKi14fX1epmDDzviwFE1Co","type":"tool_result","content":"Web search results for query: \"PyMuPDF form field extraction AcroForm widget annotation python\"\n\nLinks: [{\"title\":\"Widget - PyMuPDF documentation\",\"url\":\"https://pymupdf.readthedocs.io/en/latest/widget.html\"},{\"title\":\"How to Show Form PDF Fields · pymupdf/PyMuPDF Wiki · GitHub\",\"url\":\"https://github.com/pymupdf/PyMuPDF/wiki/How-to-Show-Form-PDF-Fields\"},{\"title\":\"How to Add Form Fields · pymupdf/PyMuPDF Wiki · GitHub\",\"url\":\"https://github.com/pymupdf/PyMuPDF/wiki/How-to-Add-Form-Fields\"},{\"title\":\"Is there any way to print the pdf forms options using PyMyPDF · Issue #164 · pymupdf/PyMuPDF\",\"url\":\"https://github.com/pymupdf/PyMuPDF/issues/164\"},{\"title\":\"How to write something into a widget (form field) · pymupdf/PyMuPDF · Discussion #1836\",\"url\":\"https://github.com/pymupdf/PyMuPDF/discussions/1836\"},{\"title\":\"Page - PyMuPDF documentation\",\"url\":\"https://pymupdf.readthedocs.io/en/latest/page.html\"},{\"title\":\"Form Fields and Widget Handling | pymupdf/PyMuPDF | DeepWiki\",\"url\":\"https://deepwiki.com/pymupdf/PyMuPDF/3.3-form-fields-and-widget-handling\"},{\"title\":\"PyMuPDF/docs/widget.rst at main · pymupdf/PyMuPDF\",\"url\":\"https://github.com/pymupdf/PyMuPDF/blob/main/docs/widget.rst\"},{\"title\":\"Interactions with PDF Forms — pypdf 6.7.0 documentation\",\"url\":\"https://pypdf.readthedocs.io/en/stable/user/forms.html\"},{\"title\":\"Document - PyMuPDF documentation\",\"url\":\"https://pymupdf.readthedocs.io/en/latest/document.html\"}]\n\nI've found comprehensive information about PyMuPDF form field extraction. Here's what I found:\n\n## PyMuPDF Form Field Extraction Overview\n\nPyMuPDF represents PDF form fields with a class called \"Widget\", and fields are technically a special case of PDF annotations, which allow users with limited permissions to enter information in a PDF.\n\n### Detecting Form PDFs\n\nA document is a Form PDF if an object of type /AcroForm and at least one form field exist.\n\n### Accessing Form Fields\n\nSince MuPDF version 1.16.0, widgets (form fields) are accessed separately from other annotations. The page.first_annot property and page.annots() iterator exclude widgets while page.first_widget and page.widgets() provide widget-specific access.\n\n### Widget Properties\n\nWidget objects contain properties including field_name, field_value, field_type, rect (location), text_color, text_font, text_fontsize, and other formatting attributes.\n\n### Working with Widgets\n\nTo access complete widget information of a widget annotation on a page, consult its property annot.widget which is a Widget object, which includes attributes such as field name and field value. You can change a widget's field value and then use annot.updateWidget(widget) to write changes back to the field.\n\n### Supported Widget Types\n\nPyMuPDF's system for working with PDF form fields includes interactive elements like text fields, checkboxes, radio buttons, dropdown lists, and other form controls.\n\nThe PyMuPDF documentation provides detailed resources including a Widget class reference and example scripts for working with form fields in PDFs.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"34e1fc99-21e8-43d3-9454-40525ca954ed","timestamp":"2026-02-12T16:57:00.057Z","sourceToolAssistantUUID":"3672fcf7-da8b-48fc-8be4-b3d1b78c5024"}
{"parentUuid":"14c1d04c-518f-459e-8aad-06523abd4a52","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01EfyUvNYVLJ9EQ5XvgirxAE","type":"tool_result","content":"Web search results for query: \"MinerU PDF extraction toolkit GitHub stars architecture features 2025\"\n\nLinks: [{\"title\":\"GitHub - opendatalab/MinerU: Transforms complex documents like PDFs into LLM-ready markdown/JSON for your Agentic workflows.\",\"url\":\"https://github.com/opendatalab/MinerU\"},{\"title\":\"MinerU - MinerU\",\"url\":\"https://opendatalab.github.io/MinerU/\"},{\"title\":\"GitHub - opendatalab/PDF-Extract-Kit: A Comprehensive Toolkit for High-Quality PDF Content Extraction\",\"url\":\"https://github.com/opendatalab/PDF-Extract-Kit\"},{\"title\":\"GitHub - protagolabs/MinerU_Protago: A high-quality tool for convert PDF to Markdown and JSON.一站式开源高质量数据提取工具，将PDF转换成Markdown和JSON格式。\",\"url\":\"https://github.com/protagolabs/MinerU_Protago\"},{\"title\":\"GitHub - skyiron/MinerU-ai-rag: A high-quality tool for convert PDF to Markdown and JSON\",\"url\":\"https://github.com/skyiron/MinerU-ai-rag\"},{\"title\":\"GitHub - tsutomu-n/MinerU_PDF: A one-stop, open-source, high-quality data extraction tool, supports PDF/webpage/e-book extraction.一站式开源高质量数据提取工具，支持PDF/网页/多格式电子书提取。\",\"url\":\"https://github.com/tsutomu-n/MinerU_PDF\"},{\"title\":\"GitHub - StephenLiz/MinerU-autorun: A high-quality tool for convert PDF to Markdown and JSON.一站式开源高质量数据提取工具，将PDF转换成Markdown和JSON格式。\",\"url\":\"https://github.com/StephenLiz/MinerU-autorun\"},{\"title\":\"MinerU Parser: High-Precision PDF Extraction\",\"url\":\"https://www.emergentmind.com/topics/mineru-parser\"},{\"title\":\"GitHub - protagolabs/MinerU-2.10: A high-quality tool for convert PDF to Markdown and JSON.一站式开源高质量数据提取工具，将PDF转换成Markdown和JSON格式。\",\"url\":\"https://github.com/protagolabs/MinerU-2.10\"},{\"title\":\"MinerU Beginner's Guide - Ultimate Open Source PDF Data Extraction Tool • StableLearn | Make AI Your Superpower\",\"url\":\"https://stable-learn.com/en/mineru-tutorial/\"}]\n\nBased on the web search results, here's comprehensive information about the MinerU PDF extraction toolkit:\n\n## Overview\n\nMinerU is a tool that converts PDFs into machine-readable formats (e.g., markdown, JSON), allowing for easy extraction into any format. MinerU was born during the pre-training process of InternLM.\n\n## Key Features\n\nMinerU offers several important capabilities:\n\n- Remove headers, footers, footnotes, page numbers, etc., to ensure semantic coherence; output text in human-readable order, suitable for single-column, multi-column, and complex layouts; preserve the structure of the original document, including headings, paragraphs, lists, etc.; extract images, image descriptions, tables, table titles, and footnotes.\n\n- Directly extracts text from text PDFs, natively supports multi-language recognition in text PDF scenarios, greatly reduces parsing hallucinations, and supports text recognition in 109 languages for scanned PDF scenarios by specifying OCR language.\n\n## Architecture & Backends\n\nThe hybrid backend combines the advantages of pipeline and vlm backends, integrates capabilities of pipeline while adding extra extensibility on top of high accuracy.\n\nMinerU2.5 introduces a coarse-to-fine, two-stage parsing schema: Stage I performs global layout analysis on thumbnails using NaViT + Patch Merger + LLM components; Stage II executes targeted high-resolution recognition on native-resolution image crops, using context-specific prompts and the same VLM pipeline.\n\n## 2025 Updates\n\nRecent improvements include upgraded layout recognition model to the latest doclayout_yolo(2501) model, improving layout recognition accuracy, and upgraded formula parsing model to the latest unimernet(2501) model, improving formula recognition accuracy.\n\n## Deployment & Usage\n\nUsers can specify the backend as pipeline to run in a pure CPU environment. MinerU has built-in ready-to-use fastapi service and gradio webui.\n\nNote: The search results did not include specific information about current GitHub star counts, so I cannot provide that metric at this time.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"338c567b-e023-409d-b451-9169bd77bcd8","timestamp":"2026-02-12T16:57:00.503Z","sourceToolAssistantUUID":"14c1d04c-518f-459e-8aad-06523abd4a52"}
{"parentUuid":"6ac6930a-422f-4a66-bbae-eabee25305a4","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01GESGxvQGcbjyKguPEjp1No","type":"tool_result","content":"Web search results for query: \"visual template mapping form extraction region-based OCR python open source 2025\"\n\nLinks: [{\"title\":\"Chandra OCR: The Open Source Model Redefining Document Intelligence | by Aditya Inamdar | Dec, 2025 | Medium\",\"url\":\"https://medium.com/@inamdaraditya98/chandra-ocr-the-open-source-model-redefining-document-intelligence-23b8016a16fc\"},{\"title\":\"8 Top Open-Source OCR Models Compared: A Complete Guide\",\"url\":\"https://modal.com/blog/8-top-open-source-ocr-models-compared\"},{\"title\":\"7 Best Open-Source OCR Models 2025: Benchmarks & Cost Comparison | E2E Networks\",\"url\":\"https://www.e2enetworks.com/blog/complete-guide-open-source-ocr-models-2025\"},{\"title\":\"5 Open Source Tools You Can Use to Train and Deploy an OCR Project | Towards Data Science\",\"url\":\"https://towardsdatascience.com/5-open-source-tools-you-can-use-to-train-and-deploy-an-ocr-project-8f204dec862b/\"},{\"title\":\"Supercharge your OCR Pipelines with Open Models\",\"url\":\"https://huggingface.co/blog/ocr-open-models\"},{\"title\":\"Python OCR Libraries: The Top Choices for Text Extraction in 2025 | by Dr. Shouke Wei | Medium\",\"url\":\"https://medium.com/@shouke.wei/python-ocr-libraries-the-top-choices-for-text-extraction-in-2025-7a069c2120ee\"},{\"title\":\"Comparing the Best Open Source OCR Tools in 2025\",\"url\":\"https://unstract.com/blog/best-opensource-ocr-tools-in-2025/\"},{\"title\":\"OCR a document, form, or invoice with Tesseract, OpenCV, and Python - PyImageSearch\",\"url\":\"https://pyimagesearch.com/2020/09/07/ocr-a-document-form-or-invoice-with-tesseract-opencv-and-python/\"},{\"title\":\"Top 7 Open Source OCR Models - KDnuggets\",\"url\":\"https://www.kdnuggets.com/top-7-open-source-ocr-models\"},{\"title\":\"10 Awesome OCR Models for 2025 - KDnuggets\",\"url\":\"https://www.kdnuggets.com/10-awesome-ocr-models-for-2025\"}]\n\nI found comprehensive information about visual template mapping, form extraction, region-based OCR, and open-source Python tools for 2025. Here's what the search revealed:\n\n## Recent Open-Source OCR Developments in 2025\n\nOctober 2025 saw a wave of open-source OCR model releases, with six major models dropping in a single month, making this an excellent time to explore these technologies.\n\n## Key Open-Source Python Tools for Form and Region-Based Extraction\n\n**PaddleOCR and PaddleOCR-VL**: PaddleOCR, developed by the PaddlePaddle team, is one of the most advanced OCR toolkits, particularly effective for complex, structured documents where both text and layout extraction is required, especially for invoices. PaddleOCR-VL from Baidu's PaddlePaddle team uses a hybrid two-stage pipeline that separates layout analysis from content recognition, with stage 1 detecting document elements and predicting reading order, and stage 2 processing cropped regions.\n\n**Template-Based Approach**: A practical approach involves defining the locations of fields in input images, aligning a scanned document with its template, extracting ROIs at field locations, and applying OCR to each ROI while associating text with form fields.\n\n**EasyOCR**: EasyOCR is one of the most widely used open-source OCR libraries with support for 80+ languages, built on PyTorch, and works well for quick text extraction tasks on scanned documents.\n\n## Advanced LLM-Based Models\n\nLLM-based models treat OCR as part of a broader visual-language problem, fusing text extraction with layout reasoning and question answering to output structured JSON or interpret diagrams.\n\nNotable models include:\n- Qwen2.5-VL (Alibaba's multimodal vision-language model) has proven top-tier performance on benchmarks with bounding boxes and point detection, works well for complex documents mixing text with diagrams and charts, and is valuable for mapping values to table cells or extracting regions of interest.\n- Chandra OCR is a powerful, open source alternative that democratizes high quality document intelligence.\n\nThese tools offer flexible Python integration for your form extraction and region-based OCR pipeline needs.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"c27a9ca5-46f7-4ba1-a644-07aba659871d","timestamp":"2026-02-12T16:57:03.178Z","sourceToolAssistantUUID":"6ac6930a-422f-4a66-bbae-eabee25305a4"}
{"parentUuid":"c27a9ca5-46f7-4ba1-a644-07aba659871d","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_01EEiCuQk3UJHsakfGSEuTKb","type":"message","role":"assistant","content":[{"type":"text","text":"Let me get a few more important details and GitHub stats."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":918,"cache_creation_input_tokens":4376,"cache_read_input_tokens":26672,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":4376},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4Xatw9o6m1PDFSrftDt","type":"assistant","uuid":"5e97397a-c887-4676-bd0b-b135a390a2ce","timestamp":"2026-02-12T16:57:06.192Z"}
{"parentUuid":"5e97397a-c887-4676-bd0b-b135a390a2ce","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_01EEiCuQk3UJHsakfGSEuTKb","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_017ih5KwvpLWn43J1oPASq81","name":"WebSearch","input":{"query":"Parsee AI open source framework data extraction structured GitHub"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":918,"cache_creation_input_tokens":4376,"cache_read_input_tokens":26672,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":4376},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4Xatw9o6m1PDFSrftDt","type":"assistant","uuid":"5ef1cd0a-7912-4b78-a3e6-70c709a3bbd7","timestamp":"2026-02-12T16:57:06.909Z"}
{"parentUuid":"5ef1cd0a-7912-4b78-a3e6-70c709a3bbd7","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_01EEiCuQk3UJHsakfGSEuTKb","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Y1hhiEaZVmQz3CABHbjTxv","name":"WebSearch","input":{"query":"form-extractor-ocr template based bank form OCR GitHub HarendraKumarSingh"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":918,"cache_creation_input_tokens":4376,"cache_read_input_tokens":26672,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":4376},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4Xatw9o6m1PDFSrftDt","type":"assistant","uuid":"cb884c85-cfc6-4aff-a4e8-d93231ed9b83","timestamp":"2026-02-12T16:57:08.037Z"}
{"parentUuid":"cb884c85-cfc6-4aff-a4e8-d93231ed9b83","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_01EEiCuQk3UJHsakfGSEuTKb","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_0158c8AXeF1PuHoBdjbqjsh9","name":"WebSearch","input":{"query":"Qwen2.5-VL document form extraction structured data 2025"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":918,"cache_creation_input_tokens":4376,"cache_read_input_tokens":26672,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":4376},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4Xatw9o6m1PDFSrftDt","type":"assistant","uuid":"c13a6966-562b-4986-b4f5-651854dba56d","timestamp":"2026-02-12T16:57:08.957Z"}
{"parentUuid":"c13a6966-562b-4986-b4f5-651854dba56d","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_01EEiCuQk3UJHsakfGSEuTKb","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_018NWkkU4YveyEvqGbsw8q7v","name":"WebSearch","input":{"query":"Surya OCR text detection recognition layout analysis GitHub VikParuchuri"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":918,"cache_creation_input_tokens":4376,"cache_read_input_tokens":26672,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":4376},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4Xatw9o6m1PDFSrftDt","type":"assistant","uuid":"13478f7f-d732-4073-b8d9-bc7e5ddf4708","timestamp":"2026-02-12T16:57:09.562Z"}
{"parentUuid":"13478f7f-d732-4073-b8d9-bc7e5ddf4708","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_01EEiCuQk3UJHsakfGSEuTKb","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_019G5AwTC2ALq1Vv1hUXsSoz","name":"WebSearch","input":{"query":"open source document intelligence on-premises self-hosted form extraction alternative textract 2025"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":918,"cache_creation_input_tokens":4376,"cache_read_input_tokens":26672,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":4376},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4Xatw9o6m1PDFSrftDt","type":"assistant","uuid":"22effc88-4f26-4b7d-9cb5-2817726d663a","timestamp":"2026-02-12T16:57:10.287Z"}
{"parentUuid":"5ef1cd0a-7912-4b78-a3e6-70c709a3bbd7","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_017ih5KwvpLWn43J1oPASq81","type":"tool_result","content":"Web search results for query: \"Parsee AI open source framework data extraction structured GitHub\"\n\nLinks: [{\"title\":\"Open Source Framework for Data Extraction and Structuring\",\"url\":\"https://www.parsee.ai/\"},{\"title\":\"GitHub - parsee-ai/parsee-core: Retrieval of fully structured data made easy. Use LLMs or custom models. Specialized on PDFs and HTML files. Extensive support of tabular data extraction and multimodal queries.\",\"url\":\"https://github.com/parsee-ai/parsee-core\"},{\"title\":\"Parsee.ai · GitHub\",\"url\":\"https://github.com/parsee-ai\"},{\"title\":\"GitHub - parsee-ai/parsee-datasets: Datasets, case studies and benchmarks for extracting structured information from PDFs, HTML files or images, created by the Parsee.ai team. Datasets also on Hugging Face: https://huggingface.co/parsee-ai\",\"url\":\"https://github.com/parsee-ai/parsee-datasets\"},{\"title\":\"Parsee.ai – a framework to easily extract complex structured data with LLMs | Hacker News\",\"url\":\"https://news.ycombinator.com/item?id=39885052\"},{\"title\":\"GitHub - katanaml/sparrow: Structured data extraction and instruction calling with ML, LLM and Vision LLM\",\"url\":\"https://github.com/katanaml/sparrow\"},{\"title\":\"5 Best Document Parsers in 2026 (Tested on Financial PDFs)\",\"url\":\"https://www.f22labs.com/blogs/5-best-document-parsers-in-2025-tested/\"},{\"title\":\"GitHub - DocumindHQ/documind: Open-source platform for extracting structured data from documents using AI.\",\"url\":\"https://github.com/DocumindHQ/documind\"},{\"title\":\"GitHub - genieincodebottle/parsemypdf: Collection of PDF parsing libraries like AI based docling, claude, openai, gemini, meta's llama-vision, unstructured-io, and pdfminer, pymupdf, pdfplumber etc for efficient snapshot, text, table, and metadata extraction.\",\"url\":\"https://github.com/genieincodebottle/parsemypdf\"},{\"title\":\"GitHub - Filimoa/open-parse: Improved file parsing for LLM’s\",\"url\":\"https://github.com/Filimoa/open-parse\"}]\n\nBased on the search results, here's what I found about Parsee AI:\n\n## Overview\n\nParsee AI is a high-level open source data extraction and structuring framework created by SimFin, with the goal of making the structuring of data from the most common sources of unstructured data (mainly PDFs, HTML files and images) as easy as possible.\n\n## Key Features\n\nParsee uses LLMs and custom AI models to transform unstructured PDFs, HTMLs, and images into fully structured data. More specifically:\n\n- It is specialized on PDFs and HTML files with extensive support of tabular data extraction and multimodal queries.\n- The framework allows you to define type-safe extraction templates that guarantee data will be parsed exactly into the format you require.\n\n## Capabilities\n\nYou can start extracting data instantly by using LLMs, which can serve as \"universal\" models. Additionally, once you have some data extracted, you can create datasets and train custom, task-specific AI-models that might outperform LLMs in accuracy and cost-efficiency.\n\n## GitHub Repository\n\nParsee can be used entirely from your local Python environment, and there is also a hosted version of Parsee available at app.parsee.ai where you can edit and share your extraction templates and run jobs in the cloud.\n\nThe main GitHub repository is available at https://github.com/parsee-ai/parsee-core, along with companion repositories for PDF reading and datasets.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"71cf5ba1-32a5-418c-99e1-f5c90cac7d94","timestamp":"2026-02-12T16:57:14.946Z","sourceToolAssistantUUID":"5ef1cd0a-7912-4b78-a3e6-70c709a3bbd7"}
{"parentUuid":"cb884c85-cfc6-4aff-a4e8-d93231ed9b83","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Y1hhiEaZVmQz3CABHbjTxv","type":"tool_result","content":"Web search results for query: \"form-extractor-ocr template based bank form OCR GitHub HarendraKumarSingh\"\n\nLinks: [{\"title\":\"GitHub - HarendraKumarSingh/form-extractor-ocr: Template based form extractor OCR. Train your own character and alphabet OCR.\",\"url\":\"https://github.com/HarendraKumarSingh/form-extractor-ocr\"},{\"title\":\"form-extractor-ocr/README.md at master · HarendraKumarSingh/form-extractor-ocr\",\"url\":\"https://github.com/HarendraKumarSingh/form-extractor-ocr/blob/master/README.md\"},{\"title\":\"form-extractor-ocr/main/preprocessing.py at master · HarendraKumarSingh/form-extractor-ocr\",\"url\":\"https://github.com/HarendraKumarSingh/form-extractor-ocr/blob/master/main/preprocessing.py\"},{\"title\":\"GitHub - jsfenfen/whatwordwhere: Tooling to extract data from scanned paper forms OCR-ed by Tesseract using the HOCR standard.\",\"url\":\"https://github.com/jsfenfen/whatwordwhere\"},{\"title\":\"GitHub - microsoft/knowledge-extraction-recipes-forms: Knowledge Extraction For Forms Accelerators & Examples\",\"url\":\"https://github.com/microsoft/knowledge-extraction-recipes-forms\"},{\"title\":\"Form Data Extraction: From OCR to Deep Learning\",\"url\":\"https://nanonets.com/blog/form-data-extraction/\"},{\"title\":\"GitHub - Pirimid/financial-documents-ocr-deep-learning\",\"url\":\"https://github.com/Pirimid/financial-documents-ocr-deep-learning\"},{\"title\":\"OCR Form Processing [Complete Guide]\",\"url\":\"https://www.docsumo.com/blog/ocr-form-processing\"},{\"title\":\"GitHub - jaykabra/Bank-Statement-OCR: Optical Character Recognition for Bank Statements using python and tabula.\",\"url\":\"https://github.com/jaykabra/Bank-Statement-OCR\"},{\"title\":\"Bank statement US extraction model - Document Intelligence - Foundry Tools | Microsoft Learn\",\"url\":\"https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/prebuilt/bank-statement?view=doc-intel-4.0.0\"}]\n\nBased on the search results, I found information about the project you're looking for:\n\n## form-extractor-ocr by HarendraKumarSingh\n\nThe form-extractor-ocr is a template-based form extractor OCR that extracts handwritten text from bank form scanned images using template matching, individual box extraction and OCR. It allows you to train your own character and alphabet OCR with pytesseract.\n\n**Repository Location:** The project is hosted on GitHub at https://github.com/HarendraKumarSingh/form-extractor-ocr\n\n**Key Features:**\n- Extracts handwritten text from bank form scanned images (or any form scanned copy) using template matching, individual box extraction and OCR\n- Prediction output uses red tagging for confidence scores less than 80% and blue tagging for confidence scores 80% or higher\n- The current model was trained on 10 images each of handwritten characters (a-z, A-Z) and numbers (0-9), with accuracy improvable using more training data\n\nThis project is particularly useful for automating bank form processing and other structured form data extraction tasks using OCR technology.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"4eba8bcc-0d99-4876-8dc9-4e14a86f5680","timestamp":"2026-02-12T16:57:15.204Z","sourceToolAssistantUUID":"cb884c85-cfc6-4aff-a4e8-d93231ed9b83"}
{"parentUuid":"13478f7f-d732-4073-b8d9-bc7e5ddf4708","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_018NWkkU4YveyEvqGbsw8q7v","type":"tool_result","content":"Web search results for query: \"Surya OCR text detection recognition layout analysis GitHub VikParuchuri\"\n\nLinks: [{\"title\":\"GitHub - datalab-to/surya: OCR, layout analysis, reading order, table recognition in 90+ languages\",\"url\":\"https://github.com/datalab-to/surya\"},{\"title\":\"GitHub - cognitiveailab/surya-sci: OCR, layout analysis, reading order, line detection in 90+ languages\",\"url\":\"https://github.com/cognitiveailab/surya-sci\"},{\"title\":\"surya/pyproject.toml at master · datalab-to/surya\",\"url\":\"https://github.com/VikParuchuri/surya/blob/master/pyproject.toml\"},{\"title\":\"GitHub - CibinQuadance/surya-OCR: OCR and line detection in 90+ languages\",\"url\":\"https://github.com/CibinQuadance/surya-OCR\"},{\"title\":\"GitHub - VikParuchuri/surya: OCR, layout analysis, reading order, table recognition in 90+ languages\",\"url\":\"https://github.com/VikParuchuri/surya?tab=readme-ov-file\"},{\"title\":\"Releases · datalab-to/surya\",\"url\":\"https://github.com/VikParuchuri/surya/releases\"},{\"title\":\"GitHub - suryatmodulus/surya-ocr: Accurate line-level text detection and recognition (OCR) in any language\",\"url\":\"https://github.com/suryatmodulus/surya-ocr\"},{\"title\":\"GitHub - kunthet/surya-text-line-detection-ocr: Accurate line-level text detection and recognition (OCR) in any language\",\"url\":\"https://github.com/kunthet/surya-text-line-detection-ocr\"},{\"title\":\"GitHub - VikParuchuri/surya: OCR, layout analysis, reading o...\",\"url\":\"https://sublime.app/card/github-vikparuchuri-surya-ocr-layout-analysis-reading-order-line-detection-in-90-languages\"},{\"title\":\"surya/ocr_app.py at master · VikParuchuri/surya\",\"url\":\"https://github.com/VikParuchuri/surya/blob/master/ocr_app.py\"}]\n\nBased on the web search results, here's what I found about Surya:\n\n## Overview\n\nSurya is a document OCR toolkit that does: OCR in 90+ languages, Line-level text detection in any language, Reading order detection, and Layout analysis (table, image, header, etc.)\n\n## Key Features\n\nText detection, layout analysis, and reading order will work with any language. The project covers OCR, layout, reading order, and table recognition in 90+ languages.\n\n## Technical Details\n\nText detection was trained on 4x A6000s for 3 days. It used a diverse set of images as training data. It was trained from scratch using a modified efficientvit architecture for semantic segmentation.\n\nText recognition was trained on 4x A6000s for 2 weeks. It was trained using a modified donut model (GQA, MoE layer, UTF-16 decoding, layer config changes).\n\n## GitHub Repository\n\nYou can develop surya by cloning https://github.com/VikParuchuri/surya.git, and the project is authored by Vikas Paruchuri and the Datalab Team.\n\nThe project is publicly available on GitHub and provides tools for document analysis tasks with strong multilingual support.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"4afccc78-b8bd-45cb-809f-76c03555f53b","timestamp":"2026-02-12T16:57:16.243Z","sourceToolAssistantUUID":"13478f7f-d732-4073-b8d9-bc7e5ddf4708"}
{"parentUuid":"c13a6966-562b-4986-b4f5-651854dba56d","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_0158c8AXeF1PuHoBdjbqjsh9","type":"tool_result","content":"Web search results for query: \"Qwen2.5-VL document form extraction structured data 2025\"\n\nLinks: [{\"title\":\"March 5, 2025 Qwen2.5-VL Technical Report Qwen Team, Alibaba Group\",\"url\":\"https://arxiv.org/pdf/2502.13923\"},{\"title\":\"February 20, 2025 Qwen2.5-VL Technical Report Qwen Team, Alibaba Group\",\"url\":\"https://www.52nlp.cn/wp-content/uploads/2025/02/Qwen2.5-VL%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A.pdf\"},{\"title\":\"[2502.13923] Qwen2.5-VL Technical Report\",\"url\":\"https://arxiv.org/abs/2502.13923\"},{\"title\":\"Paper page - Qwen2.5-VL Technical Report\",\"url\":\"https://huggingface.co/papers/2502.13923\"},{\"title\":\"GitHub - chrisalehman/ai-document-extraction: Demonstrates the utility of applying a QLoRA fine-tuned version of the Qwen-2.5-VL-7B-Instruct vision-language model for extracting text from W2 images.\",\"url\":\"https://github.com/chrisalehman/ai-document-extraction\"},{\"title\":\"Fine-tuning Qwen2.5-VL for Document Information Extraction\",\"url\":\"https://ubiai.tools/fine-tuning-qwen-for-reliable-information-extraction-from-documents/\"},{\"title\":\"Qwen-VL 2.5: The Future of OCR with Open-Source LLMs\",\"url\":\"https://www.arsturn.com/blog/qwen-vl-2-5-the-open-source-llm-thats-revolutionizing-ocr\"},{\"title\":\"Qwen/Qwen2.5-VL-72B-Instruct · Hugging Face\",\"url\":\"https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct\"},{\"title\":\"Qwen2.5-VL Technical Report\",\"url\":\"https://www.researchgate.net/publication/389167570_Qwen25-VL_Technical_Report\"},{\"title\":\"Qwen/Qwen2.5-VL-7B-Instruct · Hugging Face\",\"url\":\"https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct\"}]\n\nBased on the search results, here's what I found about Qwen2.5-VL and document form extraction:\n\n## Qwen2.5-VL Overview\n\nQwen2.5-VL is the latest flagship model of the Qwen vision-language series, which demonstrates significant advancements in both foundational capabilities and innovative functionalities. Qwen2.5-VL is available in three sizes, addressing diverse use cases from edge AI to high-performance computing.\n\n## Document and Form Extraction Capabilities\n\nIt provides robust structured data extraction from invoices, forms, and tables, as well as detailed analysis of charts, diagrams, and layouts. Qwen2.5-VL can accurately localize objects in an image by generating bounding boxes or points, and it can provide stable JSON outputs for coordinates and attributes.\n\n## Performance and Model Variants\n\nThe flagship Qwen2.5-VL-72B model matches state-of-the-art models like GPT-4o and Claude 3.5 Sonnet, particularly excelling in document and diagram understanding. Released in January 2025, Qwen-2.5-VL-7B-Instruct is a 7-billion-parameter VLM optimized for document parsing, offering robust performance on dense, structured text while remaining lightweight enough for fine-tuning on modest hardware.\n\n## Practical Applications\n\nThe model harnesses Qwen-2.5-VL-7B-Instruct to extract structured data from W2 forms, directly addressing the needs of document-heavy workflows. Additionally, fine-tuned models exhibited substantial improvements, with Qwen2.5-VL-3B-Instruct achieving relative accuracy gains exceeding 100%.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"294cf34b-2fbe-4f65-a8fe-0302b52acad4","timestamp":"2026-02-12T16:57:17.209Z","sourceToolAssistantUUID":"c13a6966-562b-4986-b4f5-651854dba56d"}
{"parentUuid":"22effc88-4f26-4b7d-9cb5-2817726d663a","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_019G5AwTC2ALq1Vv1hUXsSoz","type":"tool_result","content":"Web search results for query: \"open source document intelligence on-premises self-hosted form extraction alternative textract 2025\"\n\nLinks: [{\"title\":\"7 Best Open-Source OCR Models 2025: Benchmarks & Cost Comparison | E2E Networks\",\"url\":\"https://www.e2enetworks.com/blog/complete-guide-open-source-ocr-models-2025\"},{\"title\":\"deepdoctection - An Open Source Framework for Document AI\",\"url\":\"https://konfuzio.com/en/deepdoctection/\"},{\"title\":\"Chandra OCR: The Open Source Model Redefining Document Intelligence | by Aditya Inamdar | Dec, 2025 | Medium\",\"url\":\"https://medium.com/@inamdaraditya98/chandra-ocr-the-open-source-model-redefining-document-intelligence-23b8016a16fc\"},{\"title\":\"Comparing the Top 6 OCR (Optical Character Recognition) Models/Systems in 2025 - MarkTechPost\",\"url\":\"https://www.marktechpost.com/2025/11/02/comparing-the-top-6-ocr-optical-character-recognition-models-systems-in-2025/\"},{\"title\":\"Comparing the Top 6 OCR Models in 2025: A Comprehensive Analysis • Dev|Journal\",\"url\":\"https://earezki.com/ai-news/2025-11-02-comparing-the-top-6-ocr-optical-character-recognition-models-systems-in-2025/\"},{\"title\":\"Top 6 OCR Models in 2025: Google vs. AWS vs. Azure vs. ABBYY\",\"url\":\"https://aicosoft.com/ai-tools-applications/the-best-ocr-tools-of-2025-a-plain-english-guide-to-choosing-the-right-one\"},{\"title\":\"Open-Source Document Extraction: Unstract, DeepSeek & PostgreSQL\",\"url\":\"https://unstract.com/blog/open-source-document-data-extraction-with-unstract-deepseek/\"},{\"title\":\"DeepSeek-OCR vs Leading OCR Systems (2025): Expert Comparison Guide\",\"url\":\"https://skywork.ai/blog/ai-agent/deepseek-ocr-vs-google-azure-aws-abbyy-paddleocr-tesseract-comparison/\"},{\"title\":\"[Feature Request] Alternative OCR engines (Azure AI Vision, Google Cloud Vision etc.) · paperless-ngx/paperless-ngx · Discussion #5128\",\"url\":\"https://github.com/paperless-ngx/paperless-ngx/discussions/5128\"},{\"title\":\"Top 6 OCR Models of 2025: Accuracy, Language & Int... — AI News Digest\",\"url\":\"https://www.aidrift.tech/news/top-6-ocr-models-of-2025-accuracy-language-integra\"}]\n\nBased on the search results, here's a comprehensive overview of open-source, on-premises, self-hosted document intelligence and form extraction alternatives to AWS Textract as of 2025:\n\n## Top Open-Source Alternatives\n\n### **PaddleOCR 3.0**\nPaddleOCR 3.0 provides Apache licensed PP OCRv5, PP StructureV3, and PP ChatOCRv4 for self hosted document parsing. It is an Apache licensed open source toolkit that aims to bridge images and PDFs to LLM ready structured data. It ships with PP OCRv5 for multilingual recognition, PP StructureV3 for document parsing and table reconstruction, and PP ChatOCRv4 for key information extraction. It supports 100 plus languages, runs on CPU and GPU, and has mobile and edge variants.\n\n### **Chandra OCR**\nChandra OCR, an open source model from Datalab released in late 2025, is touted as a new state of the art in open source optical character recognition, and has quickly gained traction for tackling messy, real world problems. It excels at parsing complex tables, often the \"bane of every document parsing tool,\" and can accurately capture details in forms, including whether a checkbox is marked.\n\n### **DeepSeek OCR**\nDeepSeek OCR is an open model positioned as a VLM-based OCR with \"contexts optical compression\" to cut vision token counts while maintaining recognition quality, with the paper describing near‑lossless 10× compression and high throughput on A100 GPUs.\n\n### **deepdoctection**\ndeepdoctection is a Python-based open source framework that supports you in extracting information from documents. In contrast to common Natural Language Processing frameworks, deepdoctection offers the possibility to create pipelines where multimodal models can be used, and it has been shown that especially for documents from the daily business world (forms, reports, presentations), AI models that take visual information into account work better for extraction.\n\n### **Unstract**\nUnstract open source platform can be installed on your local machine through Docker, configured with Ollama (LLM engine), Unstructured.io/LlamaParse (OCR), and connected to an open-source vector database, such as PostgreSQL, using PGVector, establishing an efficient document data extraction pipeline, eliminating the need for cloud services.\n\n## Cost Comparison\n\nAzure Document Intelligence, AWS Textract, and Google Document AI charge $1.50 per 1,000 pages for basic text extraction, and add structured data extraction (tables, forms, key-value pairs) costs jump to $10-50 per 1,000 pages. In contrast, LightOn OCR processes a million pages for $141, making it 10.6× cheaper than standard cloud pricing.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"95d45b93-369b-4ad6-aee5-dfec03a78356","timestamp":"2026-02-12T16:57:24.663Z","sourceToolAssistantUUID":"22effc88-4f26-4b7d-9cb5-2817726d663a"}
{"parentUuid":"95d45b93-369b-4ad6-aee5-dfec03a78356","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_016CspiRfVeu3wJDe217DRRg","type":"message","role":"assistant","content":[{"type":"text","text":"Let me now get the GitHub stats and a few more details on key projects."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":6364,"cache_read_input_tokens":31048,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":6364},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XcUoeHRe68aMPUopnS","type":"assistant","uuid":"a0215ebd-48b6-49c1-9462-98df84c953f2","timestamp":"2026-02-12T16:57:27.999Z"}
{"parentUuid":"a0215ebd-48b6-49c1-9462-98df84c953f2","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_016CspiRfVeu3wJDe217DRRg","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_014dZyr1eyfUqMtjHaZy52k1","name":"WebSearch","input":{"query":"Sparrow katanaml structured data extraction vision LLM GitHub"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":6364,"cache_read_input_tokens":31048,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":6364},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XcUoeHRe68aMPUopnS","type":"assistant","uuid":"284913f2-b106-4d6d-b78c-7bb33b873988","timestamp":"2026-02-12T16:57:28.717Z"}
{"parentUuid":"284913f2-b106-4d6d-b78c-7bb33b873988","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_016CspiRfVeu3wJDe217DRRg","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01N5FvkJxPjwAmFv2PnEACwW","name":"WebSearch","input":{"query":"Documind open source structured data extraction documents GitHub"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":6364,"cache_read_input_tokens":31048,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":6364},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XcUoeHRe68aMPUopnS","type":"assistant","uuid":"69e85c43-9740-4e17-befc-6588ec3bb6ed","timestamp":"2026-02-12T16:57:29.236Z"}
{"parentUuid":"69e85c43-9740-4e17-befc-6588ec3bb6ed","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_016CspiRfVeu3wJDe217DRRg","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01MegU7iByDFCZrsPZg3bUW2","name":"WebSearch","input":{"query":"open-parse improved file parsing LLM GitHub Filimoa"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":6364,"cache_read_input_tokens":31048,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":6364},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XcUoeHRe68aMPUopnS","type":"assistant","uuid":"997fc474-7c08-4264-baf1-8c21a5cd6cc8","timestamp":"2026-02-12T16:57:30.049Z"}
{"parentUuid":"997fc474-7c08-4264-baf1-8c21a5cd6cc8","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_016CspiRfVeu3wJDe217DRRg","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_011e7qEMffMoc4GzoR9Usg5V","name":"WebSearch","input":{"query":"pdfplumber form extraction capabilities regions bounding box python"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":6364,"cache_read_input_tokens":31048,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":6364},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XcUoeHRe68aMPUopnS","type":"assistant","uuid":"f8ca0881-08c8-4d20-9bda-3c798a05dd07","timestamp":"2026-02-12T16:57:30.672Z"}
{"parentUuid":"284913f2-b106-4d6d-b78c-7bb33b873988","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_014dZyr1eyfUqMtjHaZy52k1","type":"tool_result","content":"Web search results for query: \"Sparrow katanaml structured data extraction vision LLM GitHub\"\n\nLinks: [{\"title\":\"GitHub - katanaml/sparrow: Structured data extraction and instruction calling with ML, LLM and Vision LLM\",\"url\":\"https://github.com/katanaml/sparrow\"},{\"title\":\"Releases · katanaml/sparrow\",\"url\":\"https://github.com/katanaml/sparrow/releases\"},{\"title\":\"sparrow/README.MD at main · katanaml/sparrow\",\"url\":\"https://github.com/katanaml/sparrow/blob/main/README.MD\"},{\"title\":\"sparrow/CHANGELOG.md at main · katanaml/sparrow\",\"url\":\"https://github.com/katanaml/sparrow/blob/main/CHANGELOG.md\"},{\"title\":\"sparrow/sparrow-data at main · katanaml/sparrow\",\"url\":\"https://github.com/katanaml/sparrow/tree/main/sparrow-data\"},{\"title\":\"sparrow/sparrow-ml/llm at main · katanaml/sparrow\",\"url\":\"https://github.com/katanaml/sparrow/tree/main/sparrow-ml/llm\"},{\"title\":\"sparrow/environment_setup.md at main · katanaml/sparrow\",\"url\":\"https://github.com/katanaml/sparrow/blob/main/environment_setup.md\"},{\"title\":\"Katana ML · GitHub\",\"url\":\"https://github.com/katanaml\"},{\"title\":\"sparrow/sparrow-ml/llm/config.properties at main · katanaml/sparrow\",\"url\":\"https://github.com/katanaml/sparrow/blob/main/sparrow-ml/llm/config.properties\"},{\"title\":\"sparrow/sparrow-ui at main · katanaml/sparrow\",\"url\":\"https://github.com/katanaml/sparrow/tree/main/sparrow-ui\"}]\n\nI found the project! Here's what the search revealed:\n\nSparrow is a project for structured data extraction and instruction calling with ML, LLM and Vision LLM. It's available on GitHub at katanaml/sparrow.\n\n**Key Features:**\n\nThe project handles universal document processing including invoices, receipts, forms, bank statements, and tables, with a pluggable architecture that supports multiple backends like MLX (Apple Silicon), Ollama, vLLM, Docker, and Hugging Face Cloud GPU.\n\n**Core Components:**\n\nSparrow Parse is used for structured data extraction from documents, Sparrow Instructor for text analysis, summarization and Q&A, and Sparrow Agents for complex multi-step document processing workflows.\n\n**Capabilities:**\n\nThe system provides JSON schema-based extraction with automatic validation, RESTful APIs for easy integration, and supports local Vision LLMs like Mistral and QwenVL. Recent updates include enhanced document processing with bounding box annotations and visual coordinate extraction for precise field location tracking.\n\nYou can try Sparrow online at sparrow.katanaml.io, which runs on Mac Mini M4 Pro.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"0549345b-65ec-4cbd-98c2-a546d43f3919","timestamp":"2026-02-12T16:57:36.014Z","sourceToolAssistantUUID":"284913f2-b106-4d6d-b78c-7bb33b873988"}
{"parentUuid":"69e85c43-9740-4e17-befc-6588ec3bb6ed","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01N5FvkJxPjwAmFv2PnEACwW","type":"tool_result","content":"Web search results for query: \"Documind open source structured data extraction documents GitHub\"\n\nLinks: [{\"title\":\"GitHub - DocumindHQ/documind: Open-source platform for extracting structured data from documents using AI.\",\"url\":\"https://github.com/DocumindHQ/documind\"},{\"title\":\"Releases · DocumindHQ/documind\",\"url\":\"https://github.com/DocumindHQ/documind/releases\"},{\"title\":\"documind/package.json at main · DocumindHQ/documind\",\"url\":\"https://github.com/DocumindHQ/documind/blob/main/package.json\"},{\"title\":\"documind/README.md at main · DocumindHQ/documind\",\"url\":\"https://github.com/DocumindHQ/documind/blob/main/README.md\"},{\"title\":\"Documind · GitHub\",\"url\":\"https://github.com/DocumindHQ\"},{\"title\":\"Show HN: Documind – Open-source AI tool to turn documents into structured data | Hacker News\",\"url\":\"https://news.ycombinator.com/item?id=42171311\"},{\"title\":\"Documind: Extract structured data from documents with AI – OpenAlternative\",\"url\":\"https://openalternative.co/documind\"},{\"title\":\"Documind – Extract structured data from any document\",\"url\":\"https://www.documind.xyz/?_bhlid=24fa4e30bd0a33cbe78df7b51f60451b3ef9828e\"},{\"title\":\"GitHub - DocumindHQ/docs\",\"url\":\"https://github.com/DocumindHQ/docs\"},{\"title\":\"GitHub - avirsaha/DocuMind: DocuMind - Python project for document analysis. Analyze, summarize, and determine sentiment in text and word documents. Supports .txt and .docx formats. Unlock the power of document analysis!\",\"url\":\"https://github.com/avirsaha/DocuMind\"}]\n\nBased on the search results, here's what I found about Documind:\n\n## Overview\n\nDocumind is an open-source platform for extracting structured data from documents using AI. It is an advanced document processing tool that leverages AI to extract structured data from PDFs, and is built to handle PDF conversions, extract relevant information, and format results as specified by customizable schemas.\n\n## Key Features\n\nThe platform converts PDFs to images for detailed AI processing, uses OpenAI's API to extract and structure information, and allows users to specify extraction schemas for various document formats. \n\nDocumind comes with built-in templates for extracting data from popular document types like invoices, bank statements, and more, which make it easier to get started without defining your own schema.\n\n## GitHub Repository\n\nThe main repository is located at **https://github.com/DocumindHQ/documind** and is maintained by the DocumindHQ organization. The current version is 1.1.5, and you can install it using npm install documind.\n\n## Additional Resources\n\n- Official website: https://www.documind.xyz\n- Documentation repository: https://github.com/DocumindHQ/docs\n- The repo was built on top of Zerox - https://github.com/getomni-ai/zerox.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"230c0b23-3134-44c0-8a40-6dce77dcdb46","timestamp":"2026-02-12T16:57:36.363Z","sourceToolAssistantUUID":"69e85c43-9740-4e17-befc-6588ec3bb6ed"}
{"parentUuid":"997fc474-7c08-4264-baf1-8c21a5cd6cc8","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01MegU7iByDFCZrsPZg3bUW2","type":"tool_result","content":"Web search results for query: \"open-parse improved file parsing LLM GitHub Filimoa\"\n\nLinks: [{\"title\":\"GitHub - Filimoa/open-parse: Improved file parsing for LLM’s\",\"url\":\"https://github.com/Filimoa/open-parse\"},{\"title\":\"GitHub - ExiaHan/Filimoa-open-parse: Improved file parsing for LLM’s\",\"url\":\"https://github.com/ExiaHan/Filimoa-open-parse\"},{\"title\":\"Releases · Filimoa/open-parse\",\"url\":\"https://github.com/Filimoa/open-parse/releases\"},{\"title\":\"Open Parse\",\"url\":\"https://filimoa.github.io/open-parse/\"},{\"title\":\"open-parse/README.md at main · Filimoa/open-parse\",\"url\":\"https://github.com/Filimoa/open-parse/blob/main/README.md\"},{\"title\":\"open-parse/docs/processing/customization.md at main · Filimoa/open-parse\",\"url\":\"https://github.com/Filimoa/open-parse/blob/main/docs/processing/customization.md\"},{\"title\":\"open-parse/ARCHITECTURE.md at main · Filimoa/open-parse\",\"url\":\"https://github.com/Filimoa/open-parse/blob/main/ARCHITECTURE.md\"},{\"title\":\"open-parse/src/cookbooks/semantic_processing.ipynb at main · Filimoa/open-parse\",\"url\":\"https://github.com/Filimoa/open-parse/blob/main/src/cookbooks/semantic_processing.ipynb\"},{\"title\":\"open-parse/src/cookbooks/quickstart.ipynb at main · Filimoa/open-parse\",\"url\":\"https://github.com/Filimoa/open-parse/blob/main/src/cookbooks/quickstart.ipynb\"},{\"title\":\"open-parse/src/cookbooks at main · Filimoa/open-parse\",\"url\":\"https://github.com/Filimoa/open-parse/tree/main/src/cookbooks\"}]\n\nBased on the search results, here's what I found about open-parse:\n\nOpen-Parse is improved file parsing for LLMs created by Filimoa on GitHub. The project addresses limitations of traditional document parsing approaches.\n\n**Key Features:**\n\n- Open-Parse visually analyzes documents for superior LLM input, going beyond naive text splitting\n- Basic markdown support for parsing headings, bold and italics\n- High-precision table support that extracts tables into clean Markdown formats with accuracy that surpasses traditional tools\n- Extensible design that allows easily implementing custom post-processing steps\n\n**How It Works:**\n\nThe system parses text into markdown by looking at the font size and style character by character, combines this into spans representing strings of characters with the same styling, and groups spans into lines and lines into elements.\n\n**Advanced Capabilities:**\n\nOpen-Parse includes a semantic processing pipeline that uses embeddings to cluster similar nodes together, which is powerful though the project notes needing more robust clustering methods.\n\nThe project repository is available at: https://github.com/Filimoa/open-parse\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"79452571-c171-4af4-b8f4-3b3b16b25ca2","timestamp":"2026-02-12T16:57:38.967Z","sourceToolAssistantUUID":"997fc474-7c08-4264-baf1-8c21a5cd6cc8"}
{"parentUuid":"f8ca0881-08c8-4d20-9bda-3c798a05dd07","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_011e7qEMffMoc4GzoR9Usg5V","type":"tool_result","content":"Web search results for query: \"pdfplumber form extraction capabilities regions bounding box python\"\n\nLinks: [{\"title\":\"GitHub - jsvine/pdfplumber: Plumb a PDF for detailed information about each char, rectangle, line, et cetera — and easily extract text and tables.\",\"url\":\"https://github.com/jsvine/pdfplumber\"},{\"title\":\"Unlocking PDF Data the Smart Way: A Practical Guide to pdfplumber | by Sangeethasaravanan | Medium\",\"url\":\"https://sangeethasaravanan.medium.com/unlocking-pdf-data-the-smart-way-a-practical-guide-to-pdfplumber-3c80b5d9d491\"},{\"title\":\"pdfplumber-aemc · PyPI\",\"url\":\"https://pypi.org/project/pdfplumber-aemc/0.5.28/\"},{\"title\":\"pdfplumber · PyPI\",\"url\":\"https://pypi.org/project/pdfplumber/\"},{\"title\":\"Extracting PDF Data With Pdfplumber - Lines, Rectangles, And Crop — Hive\",\"url\":\"https://hive.blog/python/@geekgirl/extracting-pdf-data-with-pdfplumber-lines-rectangles-and-crop\"},{\"title\":\"How to extract structured fields using pdfplumber - woteq Softwares\",\"url\":\"https://woteq.com/how-to-extract-structured-fields-using-pdfplumber/\"},{\"title\":\"Can PDFPlumber Extract Tables from PDFs? - PDFPlumber\",\"url\":\"https://www.pdfplumber.com/can-pdfplumber-extract-tables-from-pdfs/\"},{\"title\":\"Can PDFPlumber Extract Images from PDFs? - PDFPlumber\",\"url\":\"https://www.pdfplumber.com/can-pdfplumber-extract-images-from-pdfs/\"},{\"title\":\"PDFPlumber: The Ultimate Python Library for Precision PDF Table and Text Extraction with Visual Debugging - BrightCoding\",\"url\":\"https://www.blog.brightcoding.dev/2025/09/29/pdfplumber-the-ultimate-python-library-for-precision-pdf-table-and-text-extraction-with-visual-debugging/\"},{\"title\":\"PDFPlumber – Extract & Process PDF Data Easily\",\"url\":\"https://www.pdfplumber.com/\"}]\n\nBased on the search results, here's what I found about pdfplumber's form extraction capabilities, regions, and bounding box features:\n\n## Form Extraction Capabilities\n\npdfplumber doesn't have an interface for working with form data, but you can access it using pdfplumber's wrappers around pdfminer. You can retrieve form field names and values and store them in a dictionary, though you may have to modify the script to handle cases like nested fields.\n\n## Region-Based Extraction with Bounding Boxes\n\nYou can crop a portion of a page and extract text or tables only from that region. The library provides several methods for working with defined regions:\n\n- **Cropping:** Page.crop(bounding_box) returns a version of the page cropped to the bounding box. Cropped pages retain objects that fall at least partly within the bounding box, and if an object falls only partly within the box, its dimensions are sliced to fit the bounding box.\n\n- **Bounding Box Format:** Bounding boxes are defined with coordinates as (x0, top, x1, bottom), measured in points from the bottom-left corner, and you can use within_bbox() to extract text.\n\n## PDF Objects Supported\n\nEach of these properties (objects, chars, lines, rects, curves, images) is a list containing one dictionary for each such object embedded on the page. PDFPlumber leverages bounding boxes—rectangular regions that define the position of each character or element on the page. By analyzing these boxes and the vertical/horizontal alignment of text, the library determines cell boundaries and organizes content into structured rows and columns.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"80d902cd-9cfc-4ec8-a242-3e48e33cf0d6","timestamp":"2026-02-12T16:57:40.623Z","sourceToolAssistantUUID":"f8ca0881-08c8-4d20-9bda-3c798a05dd07"}
{"parentUuid":"80d902cd-9cfc-4ec8-a242-3e48e33cf0d6","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_01X9VoUJYoGCPPxR6s31s1eW","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me get GitHub star counts for the remaining key projects and a couple more tools."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":3548,"cache_read_input_tokens":37412,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":3548},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XdeteA7KX2pUsu99TK","type":"assistant","uuid":"c63bb001-8cde-49d9-b3dc-5954841eb267","timestamp":"2026-02-12T16:57:43.858Z"}
{"parentUuid":"c63bb001-8cde-49d9-b3dc-5954841eb267","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_01X9VoUJYoGCPPxR6s31s1eW","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01G1iZ8cgVr14iuwxwWWdWJT","name":"Bash","input":{"command":"gh api repos/ucbepic/TWIX --jq '.stargazers_count, .updated_at, .language' 2>/dev/null","description":"Get TWIX GitHub stats"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":3548,"cache_read_input_tokens":37412,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":3548},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XdeteA7KX2pUsu99TK","type":"assistant","uuid":"592a5283-404d-404f-989a-eb990fc4ad38","timestamp":"2026-02-12T16:57:45.087Z"}
{"parentUuid":"592a5283-404d-404f-989a-eb990fc4ad38","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_01X9VoUJYoGCPPxR6s31s1eW","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01V1fSY2y74xytj5zQUZ5zVX","name":"Bash","input":{"command":"gh api repos/PaddlePaddle/PaddleOCR --jq '.stargazers_count, .updated_at, .language' 2>/dev/null","description":"Get PaddleOCR GitHub stats"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":3548,"cache_read_input_tokens":37412,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":3548},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XdeteA7KX2pUsu99TK","type":"assistant","uuid":"63063032-bd45-47f0-ae67-25e8c40c469c","timestamp":"2026-02-12T16:57:45.808Z"}
{"parentUuid":"63063032-bd45-47f0-ae67-25e8c40c469c","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_01X9VoUJYoGCPPxR6s31s1eW","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01C3boX3xmBqrcGZLS2Dsuio","name":"Bash","input":{"command":"gh api repos/mindee/doctr --jq '.stargazers_count, .updated_at, .language' 2>/dev/null","description":"Get docTR GitHub stats"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":3548,"cache_read_input_tokens":37412,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":3548},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XdeteA7KX2pUsu99TK","type":"assistant","uuid":"d5939dbe-ca3b-48c9-a53a-c14a65c6d5da","timestamp":"2026-02-12T16:57:46.638Z"}
{"parentUuid":"d5939dbe-ca3b-48c9-a53a-c14a65c6d5da","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_01X9VoUJYoGCPPxR6s31s1eW","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01NHtQzqGxtzTAwPn6a5guZb","name":"Bash","input":{"command":"gh api repos/docling-project/docling --jq '.stargazers_count, .updated_at, .language' 2>/dev/null","description":"Get Docling GitHub stats"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":3548,"cache_read_input_tokens":37412,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":3548},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XdeteA7KX2pUsu99TK","type":"assistant","uuid":"b2dad2a0-fa5d-4f69-b619-efdcb54ae5d4","timestamp":"2026-02-12T16:57:47.661Z"}
{"parentUuid":"b2dad2a0-fa5d-4f69-b619-efdcb54ae5d4","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_01X9VoUJYoGCPPxR6s31s1eW","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_015TDspNJzvJh5qYVXRmfJUR","name":"Bash","input":{"command":"gh api repos/datalab-to/marker --jq '.stargazers_count, .updated_at, .language' 2>/dev/null","description":"Get marker GitHub stats"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":3548,"cache_read_input_tokens":37412,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":3548},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XdeteA7KX2pUsu99TK","type":"assistant","uuid":"5d989516-0ef1-4b8f-86ae-66f06a151bc7","timestamp":"2026-02-12T16:57:48.473Z"}
{"parentUuid":"5d989516-0ef1-4b8f-86ae-66f06a151bc7","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"progress","data":{"type":"bash_progress","output":"","fullOutput":"","elapsedTimeSeconds":2,"totalLines":0},"toolUseID":"bash-progress-0","parentToolUseID":"toolu_01G1iZ8cgVr14iuwxwWWdWJT","uuid":"db7ae1ab-0881-4246-af96-c89cb83917dc","timestamp":"2026-02-12T16:57:49.052Z"}
{"parentUuid":"592a5283-404d-404f-989a-eb990fc4ad38","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01G1iZ8cgVr14iuwxwWWdWJT","type":"tool_result","content":"211\n2026-02-04T11:17:25Z\nPython","is_error":false}]},"uuid":"73f89b76-93fc-41fb-b293-a55bcffad3e2","timestamp":"2026-02-12T16:57:49.992Z","sourceToolAssistantUUID":"592a5283-404d-404f-989a-eb990fc4ad38"}
{"parentUuid":"63063032-bd45-47f0-ae67-25e8c40c469c","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01V1fSY2y74xytj5zQUZ5zVX","type":"tool_result","content":"70698\n2026-02-12T16:20:36Z\nPython","is_error":false}]},"uuid":"86c1767d-54a9-4caf-95b6-610ccc14bbd2","timestamp":"2026-02-12T16:57:52.751Z","sourceToolAssistantUUID":"63063032-bd45-47f0-ae67-25e8c40c469c"}
{"parentUuid":"d5939dbe-ca3b-48c9-a53a-c14a65c6d5da","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01C3boX3xmBqrcGZLS2Dsuio","type":"tool_result","content":"5858\n2026-02-11T16:51:51Z\nPython","is_error":false}]},"uuid":"22df6fff-9f76-40c1-877a-2b7b9ca98093","timestamp":"2026-02-12T16:57:54.438Z","sourceToolAssistantUUID":"d5939dbe-ca3b-48c9-a53a-c14a65c6d5da"}
{"parentUuid":"b2dad2a0-fa5d-4f69-b619-efdcb54ae5d4","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01NHtQzqGxtzTAwPn6a5guZb","type":"tool_result","content":"52840\n2026-02-12T16:57:35Z\nPython","is_error":false}]},"uuid":"48b1e1ff-9e59-4d79-a071-0b19e38dd07a","timestamp":"2026-02-12T16:57:56.172Z","sourceToolAssistantUUID":"b2dad2a0-fa5d-4f69-b619-efdcb54ae5d4"}
{"parentUuid":"5d989516-0ef1-4b8f-86ae-66f06a151bc7","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_015TDspNJzvJh5qYVXRmfJUR","type":"tool_result","content":"31620\n2026-02-12T16:37:46Z\nPython","is_error":false}]},"uuid":"2e655d03-0a8e-4d9a-9e35-f84f9def2a95","timestamp":"2026-02-12T16:57:58.088Z","sourceToolAssistantUUID":"5d989516-0ef1-4b8f-86ae-66f06a151bc7"}
{"parentUuid":"2e655d03-0a8e-4d9a-9e35-f84f9def2a95","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_011ikJf8DDiZNGpo4ztSX2t9","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_015mB52TzahvoYtWHz5vRdd3","name":"Bash","input":{"command":"gh api repos/opendatalab/MinerU --jq '.stargazers_count, .updated_at, .language' 2>/dev/null","description":"Get MinerU GitHub stats"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":718,"cache_read_input_tokens":40960,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":718},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XewSaYSn2e4qiMPUBJ","type":"assistant","uuid":"e176a69d-561c-4105-b018-6d2339020744","timestamp":"2026-02-12T16:58:01.001Z"}
{"parentUuid":"e176a69d-561c-4105-b018-6d2339020744","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_011ikJf8DDiZNGpo4ztSX2t9","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01QGSVom4YSBWRC98p77Tcu4","name":"Bash","input":{"command":"gh api repos/deepdoctection/deepdoctection --jq '.stargazers_count, .updated_at, .language' 2>/dev/null","description":"Get deepdoctection GitHub stats"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":718,"cache_read_input_tokens":40960,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":718},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XewSaYSn2e4qiMPUBJ","type":"assistant","uuid":"eb6f0c8d-bbf4-435f-882c-76153e2801e8","timestamp":"2026-02-12T16:58:01.825Z"}
{"parentUuid":"e176a69d-561c-4105-b018-6d2339020744","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_015mB52TzahvoYtWHz5vRdd3","type":"tool_result","content":"54291\n2026-02-12T16:49:32Z\nPython","is_error":false}]},"uuid":"4a811039-d845-41ef-9ae5-e91154168970","timestamp":"2026-02-12T16:58:02.625Z","sourceToolAssistantUUID":"e176a69d-561c-4105-b018-6d2339020744"}
{"parentUuid":"4a811039-d845-41ef-9ae5-e91154168970","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_011ikJf8DDiZNGpo4ztSX2t9","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01XiKWhbbMgFDNno4CrVGR2k","name":"Bash","input":{"command":"gh api repos/Zipstack/unstract --jq '.stargazers_count, .updated_at, .language' 2>/dev/null","description":"Get Unstract GitHub stats"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":718,"cache_read_input_tokens":40960,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":718},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XewSaYSn2e4qiMPUBJ","type":"assistant","uuid":"419c5164-cfa2-4c42-996d-773aa27f836e","timestamp":"2026-02-12T16:58:02.910Z"}
{"parentUuid":"419c5164-cfa2-4c42-996d-773aa27f836e","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_011ikJf8DDiZNGpo4ztSX2t9","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_013BPUoku14oR3SSeEbaZYNG","name":"Bash","input":{"command":"gh api repos/Unstructured-IO/unstructured --jq '.stargazers_count, .updated_at, .language' 2>/dev/null","description":"Get Unstructured GitHub stats"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":718,"cache_read_input_tokens":40960,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":718},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XewSaYSn2e4qiMPUBJ","type":"assistant","uuid":"500dd66e-d244-4727-a776-994662f6c3ab","timestamp":"2026-02-12T16:58:03.560Z"}
{"parentUuid":"500dd66e-d244-4727-a776-994662f6c3ab","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_011ikJf8DDiZNGpo4ztSX2t9","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Yb83YNk8yw5VP3gf5URZnk","name":"Bash","input":{"command":"gh api repos/katanaml/sparrow --jq '.stargazers_count, .updated_at, .language' 2>/dev/null","description":"Get Sparrow GitHub stats"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":718,"cache_read_input_tokens":40960,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":718},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XewSaYSn2e4qiMPUBJ","type":"assistant","uuid":"64312e25-d08e-4013-8e9d-3f97260710ec","timestamp":"2026-02-12T16:58:04.307Z"}
{"parentUuid":"eb6f0c8d-bbf4-435f-882c-76153e2801e8","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01QGSVom4YSBWRC98p77Tcu4","type":"tool_result","content":"3137\n2026-02-11T14:51:56Z\nPython","is_error":false}]},"uuid":"00a46c64-74c6-4076-92dc-6c95d5bea0a0","timestamp":"2026-02-12T16:58:04.639Z","sourceToolAssistantUUID":"eb6f0c8d-bbf4-435f-882c-76153e2801e8"}
{"parentUuid":"00a46c64-74c6-4076-92dc-6c95d5bea0a0","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_011ikJf8DDiZNGpo4ztSX2t9","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Q8umTmMzwCXfmHsXeLfxrx","name":"Bash","input":{"command":"gh api repos/datalab-to/surya --jq '.stargazers_count, .updated_at, .language' 2>/dev/null","description":"Get Surya GitHub stats"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":718,"cache_read_input_tokens":40960,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":718},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XewSaYSn2e4qiMPUBJ","type":"assistant","uuid":"82b1cd8d-e881-4724-8172-ab7879a43fba","timestamp":"2026-02-12T16:58:05.084Z"}
{"parentUuid":"82b1cd8d-e881-4724-8172-ab7879a43fba","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_011ikJf8DDiZNGpo4ztSX2t9","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01B2wKonuJYp3ybMivbgS5Wd","name":"Bash","input":{"command":"gh api repos/clovaai/donut --jq '.stargazers_count, .updated_at, .language' 2>/dev/null","description":"Get Donut GitHub stats"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":718,"cache_read_input_tokens":40960,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":718},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XewSaYSn2e4qiMPUBJ","type":"assistant","uuid":"f7f4c873-fc2c-4ee7-a342-8374987c18b7","timestamp":"2026-02-12T16:58:05.809Z"}
{"parentUuid":"419c5164-cfa2-4c42-996d-773aa27f836e","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01XiKWhbbMgFDNno4CrVGR2k","type":"tool_result","content":"6120\n2026-02-12T11:21:31Z\nPython","is_error":false}]},"uuid":"bd71dc5e-c38c-438f-bf3b-a50887ef36f1","timestamp":"2026-02-12T16:58:06.235Z","sourceToolAssistantUUID":"419c5164-cfa2-4c42-996d-773aa27f836e"}
{"parentUuid":"bd71dc5e-c38c-438f-bf3b-a50887ef36f1","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_011ikJf8DDiZNGpo4ztSX2t9","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01NCvoa2ghaiSiToD9sHXpRT","name":"Bash","input":{"command":"gh api repos/Layout-Parser/layout-parser --jq '.stargazers_count, .updated_at, .language' 2>/dev/null","description":"Get LayoutParser GitHub stats"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":718,"cache_read_input_tokens":40960,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":718},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XewSaYSn2e4qiMPUBJ","type":"assistant","uuid":"0b4b1a0f-f6f8-4a62-90da-219aceac1cb9","timestamp":"2026-02-12T16:58:06.440Z"}
{"parentUuid":"0b4b1a0f-f6f8-4a62-90da-219aceac1cb9","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_011ikJf8DDiZNGpo4ztSX2t9","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_016i1XZX2AozYhv69a8fi8Eh","name":"Bash","input":{"command":"gh api repos/jsvine/pdfplumber --jq '.stargazers_count, .updated_at, .language' 2>/dev/null","description":"Get pdfplumber GitHub stats"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":718,"cache_read_input_tokens":40960,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":718},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XewSaYSn2e4qiMPUBJ","type":"assistant","uuid":"872b064d-8bca-4c56-b934-422393a4a728","timestamp":"2026-02-12T16:58:07.219Z"}
{"parentUuid":"872b064d-8bca-4c56-b934-422393a4a728","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_011ikJf8DDiZNGpo4ztSX2t9","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Enm76yRagJKYNt9UgtPc4N","name":"Bash","input":{"command":"gh api repos/camelot-dev/camelot --jq '.stargazers_count, .updated_at, .language' 2>/dev/null","description":"Get Camelot GitHub stats"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":718,"cache_read_input_tokens":40960,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":718},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XewSaYSn2e4qiMPUBJ","type":"assistant","uuid":"9907ee4e-bd21-4ed4-b7ca-f16b67ffc32b","timestamp":"2026-02-12T16:58:07.973Z"}
{"parentUuid":"500dd66e-d244-4727-a776-994662f6c3ab","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_013BPUoku14oR3SSeEbaZYNG","type":"tool_result","content":"13962\n2026-02-12T11:23:48Z\nHTML","is_error":false}]},"uuid":"75275a28-e0ff-403f-b136-cc55c46c5b2b","timestamp":"2026-02-12T16:58:08.421Z","sourceToolAssistantUUID":"500dd66e-d244-4727-a776-994662f6c3ab"}
{"parentUuid":"64312e25-d08e-4013-8e9d-3f97260710ec","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Yb83YNk8yw5VP3gf5URZnk","type":"tool_result","content":"5121\n2026-02-12T14:21:49Z\nPython","is_error":false}]},"uuid":"7e33a3e6-1dee-4e09-8932-231f4b80d6a1","timestamp":"2026-02-12T16:58:10.511Z","sourceToolAssistantUUID":"64312e25-d08e-4013-8e9d-3f97260710ec"}
{"parentUuid":"82b1cd8d-e881-4724-8172-ab7879a43fba","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Q8umTmMzwCXfmHsXeLfxrx","type":"tool_result","content":"19267\n2026-02-12T16:38:17Z\nPython","is_error":false}]},"uuid":"c54a3109-02e6-4724-ad17-366c08efef7f","timestamp":"2026-02-12T16:58:12.295Z","sourceToolAssistantUUID":"82b1cd8d-e881-4724-8172-ab7879a43fba"}
{"parentUuid":"f7f4c873-fc2c-4ee7-a342-8374987c18b7","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01B2wKonuJYp3ybMivbgS5Wd","type":"tool_result","content":"6786\n2026-02-12T14:33:15Z\nPython","is_error":false}]},"uuid":"377c2cc8-e89b-4ab1-a1cd-93e3c23b76cf","timestamp":"2026-02-12T16:58:14.385Z","sourceToolAssistantUUID":"f7f4c873-fc2c-4ee7-a342-8374987c18b7"}
{"parentUuid":"0b4b1a0f-f6f8-4a62-90da-219aceac1cb9","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01NCvoa2ghaiSiToD9sHXpRT","type":"tool_result","content":"5650\n2026-02-11T08:15:43Z\nPython","is_error":false}]},"uuid":"efca77f8-7a39-4e18-b024-296eba703e62","timestamp":"2026-02-12T16:58:15.898Z","sourceToolAssistantUUID":"0b4b1a0f-f6f8-4a62-90da-219aceac1cb9"}
{"parentUuid":"872b064d-8bca-4c56-b934-422393a4a728","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_016i1XZX2AozYhv69a8fi8Eh","type":"tool_result","content":"9702\n2026-02-12T16:18:20Z\nPython","is_error":false}]},"uuid":"59880cd7-75f0-46db-b661-8ddae6bb9108","timestamp":"2026-02-12T16:58:17.574Z","sourceToolAssistantUUID":"872b064d-8bca-4c56-b934-422393a4a728"}
{"parentUuid":"9907ee4e-bd21-4ed4-b7ca-f16b67ffc32b","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Enm76yRagJKYNt9UgtPc4N","type":"tool_result","content":"3595\n2026-02-12T11:38:14Z\nPython","is_error":false}]},"uuid":"6197c7b1-9778-48b7-92fe-f2c3dc0c3fa2","timestamp":"2026-02-12T16:58:19.110Z","sourceToolAssistantUUID":"9907ee4e-bd21-4ed4-b7ca-f16b67ffc32b"}
{"parentUuid":"6197c7b1-9778-48b7-92fe-f2c3dc0c3fa2","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_017GTRZ5bCbLuUg7Fv5XNysZ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_014eeeDQBNhHNes9EezM81gw","name":"Bash","input":{"command":"gh api repos/pymupdf/PyMuPDF --jq '.stargazers_count, .updated_at, .language' 2>/dev/null","description":"Get PyMuPDF GitHub stats"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1371,"cache_read_input_tokens":41678,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1371},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XgVKG51R8VYSZ1Bu83","type":"assistant","uuid":"76f59fd7-0b0f-47e6-bff7-8cfa52c4a79f","timestamp":"2026-02-12T16:58:21.961Z"}
{"parentUuid":"76f59fd7-0b0f-47e6-bff7-8cfa52c4a79f","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_017GTRZ5bCbLuUg7Fv5XNysZ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_013MN1oE8WiYFZqF4FgFNNKB","name":"Bash","input":{"command":"gh api repos/parsee-ai/parsee-core --jq '.stargazers_count, .updated_at, .language' 2>/dev/null","description":"Get Parsee GitHub stats"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1371,"cache_read_input_tokens":41678,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1371},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XgVKG51R8VYSZ1Bu83","type":"assistant","uuid":"ea5f53ec-7e6d-4900-b90b-8e30ddc2d400","timestamp":"2026-02-12T16:58:22.971Z"}
{"parentUuid":"76f59fd7-0b0f-47e6-bff7-8cfa52c4a79f","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_014eeeDQBNhHNes9EezM81gw","type":"tool_result","content":"9039\n2026-02-12T16:39:50Z\nPython","is_error":false}]},"uuid":"baadf944-9e05-4456-b8dc-b7d43789d6b3","timestamp":"2026-02-12T16:58:23.440Z","sourceToolAssistantUUID":"76f59fd7-0b0f-47e6-bff7-8cfa52c4a79f"}
{"parentUuid":"baadf944-9e05-4456-b8dc-b7d43789d6b3","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_017GTRZ5bCbLuUg7Fv5XNysZ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01KifbWmEBD2hiD77zAiNpMY","name":"Bash","input":{"command":"gh api repos/Filimoa/open-parse --jq '.stargazers_count, .updated_at, .language' 2>/dev/null","description":"Get open-parse GitHub stats"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1371,"cache_read_input_tokens":41678,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1371},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XgVKG51R8VYSZ1Bu83","type":"assistant","uuid":"cb9a6c98-318b-408b-aecd-78fe729bdfd7","timestamp":"2026-02-12T16:58:23.631Z"}
{"parentUuid":"cb9a6c98-318b-408b-aecd-78fe729bdfd7","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_017GTRZ5bCbLuUg7Fv5XNysZ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01X928tfcT2XJM1Jz6bX779a","name":"Bash","input":{"command":"gh api repos/DocumindHQ/documind --jq '.stargazers_count, .updated_at, .language' 2>/dev/null","description":"Get Documind GitHub stats"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1371,"cache_read_input_tokens":41678,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1371},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XgVKG51R8VYSZ1Bu83","type":"assistant","uuid":"c7d01863-27c8-4abb-8fb7-16a9e93d68a6","timestamp":"2026-02-12T16:58:24.201Z"}
{"parentUuid":"ea5f53ec-7e6d-4900-b90b-8e30ddc2d400","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_013MN1oE8WiYFZqF4FgFNNKB","type":"tool_result","content":"83\n2026-01-24T19:34:32Z\nPython","is_error":false}]},"uuid":"86ded2a0-fbd1-4389-b179-5a92b2de1b30","timestamp":"2026-02-12T16:58:25.020Z","sourceToolAssistantUUID":"ea5f53ec-7e6d-4900-b90b-8e30ddc2d400"}
{"parentUuid":"86ded2a0-fbd1-4389-b179-5a92b2de1b30","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_017GTRZ5bCbLuUg7Fv5XNysZ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_019PcEjzS6WJF587KbcDGtL2","name":"Bash","input":{"command":"gh api repos/opendatalab/PDF-Extract-Kit --jq '.stargazers_count, .updated_at, .language' 2>/dev/null","description":"Get PDF-Extract-Kit GitHub stats"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1371,"cache_read_input_tokens":41678,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1371},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XgVKG51R8VYSZ1Bu83","type":"assistant","uuid":"fa9a61a5-3d55-40ff-b204-8c1e74616369","timestamp":"2026-02-12T16:58:25.196Z"}
{"parentUuid":"cb9a6c98-318b-408b-aecd-78fe729bdfd7","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01KifbWmEBD2hiD77zAiNpMY","type":"tool_result","content":"3152\n2026-02-12T08:55:09Z\nPython","is_error":false}]},"uuid":"006ba119-a6a8-47d1-a9e5-bf88c7414fdf","timestamp":"2026-02-12T16:58:26.491Z","sourceToolAssistantUUID":"cb9a6c98-318b-408b-aecd-78fe729bdfd7"}
{"parentUuid":"006ba119-a6a8-47d1-a9e5-bf88c7414fdf","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_017GTRZ5bCbLuUg7Fv5XNysZ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01J3qNrhxSW5yDpC5QG9wbRP","name":"Bash","input":{"command":"gh api repos/microsoft/layoutlmv3 --jq '.stargazers_count, .updated_at, .language' 2>/dev/null || gh api repos/microsoft/unilm --jq '.stargazers_count, .updated_at, .language' 2>/dev/null","description":"Get LayoutLMv3/UniLM GitHub stats"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1371,"cache_read_input_tokens":41678,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1371},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XgVKG51R8VYSZ1Bu83","type":"assistant","uuid":"fb67f1a8-9d47-4aa1-b598-a4000ec0d3ac","timestamp":"2026-02-12T16:58:26.640Z"}
{"parentUuid":"fb67f1a8-9d47-4aa1-b598-a4000ec0d3ac","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_017GTRZ5bCbLuUg7Fv5XNysZ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_013kWYRVtS2Qk9ZdnJqG7isx","name":"Bash","input":{"command":"gh api repos/HarendraKumarSingh/form-extractor-ocr --jq '.stargazers_count, .updated_at, .language' 2>/dev/null","description":"Get form-extractor-ocr GitHub stats"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1371,"cache_read_input_tokens":41678,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1371},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XgVKG51R8VYSZ1Bu83","type":"assistant","uuid":"1f72e5e6-9de3-4329-b9c0-03f4e19e171f","timestamp":"2026-02-12T16:58:27.736Z"}
{"parentUuid":"c7d01863-27c8-4abb-8fb7-16a9e93d68a6","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01X928tfcT2XJM1Jz6bX779a","type":"tool_result","content":"1465\n2026-02-09T17:08:51Z\nJavaScript","is_error":false}]},"uuid":"82c8d099-db72-4775-9d33-58c64c8dd497","timestamp":"2026-02-12T16:58:28.640Z","sourceToolAssistantUUID":"c7d01863-27c8-4abb-8fb7-16a9e93d68a6"}
{"parentUuid":"82c8d099-db72-4775-9d33-58c64c8dd497","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_017GTRZ5bCbLuUg7Fv5XNysZ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Q5XqR4PLT16DWPP96sqGDf","name":"Bash","input":{"command":"gh api repos/microsoft/knowledge-extraction-recipes-forms --jq '.stargazers_count, .updated_at, .language' 2>/dev/null","description":"Get MS knowledge-extraction-recipes GitHub stats"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1371,"cache_read_input_tokens":41678,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1371},"output_tokens":25,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4XgVKG51R8VYSZ1Bu83","type":"assistant","uuid":"a7bf1a30-a237-44d2-b0f0-5cd792e2e737","timestamp":"2026-02-12T16:58:28.947Z"}
{"parentUuid":"fa9a61a5-3d55-40ff-b204-8c1e74616369","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_019PcEjzS6WJF587KbcDGtL2","type":"tool_result","content":"9350\n2026-02-12T16:26:57Z\nPython","is_error":false}]},"uuid":"0ef7ac89-3dff-434e-86f1-e16e24d0a7e4","timestamp":"2026-02-12T16:58:30.405Z","sourceToolAssistantUUID":"fa9a61a5-3d55-40ff-b204-8c1e74616369"}
{"parentUuid":"fb67f1a8-9d47-4aa1-b598-a4000ec0d3ac","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01J3qNrhxSW5yDpC5QG9wbRP","type":"tool_result","content":"{\"message\":\"Not Found\",\"documentation_url\":\"https://docs.github.com/rest/repos/repos#get-a-repository\",\"status\":\"404\"}22020\n2026-02-12T11:26:02Z\nPython","is_error":false}]},"uuid":"03b85b00-7bec-493f-bffe-a6efd2d5c6e4","timestamp":"2026-02-12T16:58:32.708Z","sourceToolAssistantUUID":"fb67f1a8-9d47-4aa1-b598-a4000ec0d3ac"}
{"parentUuid":"1f72e5e6-9de3-4329-b9c0-03f4e19e171f","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_013kWYRVtS2Qk9ZdnJqG7isx","type":"tool_result","content":"18\n2025-04-06T17:16:45Z\nPython","is_error":false}]},"uuid":"776eda3b-e30b-4f2e-b3ff-d9bec34771ad","timestamp":"2026-02-12T16:58:34.154Z","sourceToolAssistantUUID":"1f72e5e6-9de3-4329-b9c0-03f4e19e171f"}
{"parentUuid":"a7bf1a30-a237-44d2-b0f0-5cd792e2e737","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Q5XqR4PLT16DWPP96sqGDf","type":"tool_result","content":"222\n2025-12-09T16:23:13Z\nJupyter Notebook","is_error":false}]},"uuid":"d86dbc50-93b4-45ac-8686-a8cae6c9bc75","timestamp":"2026-02-12T16:58:35.902Z","sourceToolAssistantUUID":"a7bf1a30-a237-44d2-b0f0-5cd792e2e737"}
{"parentUuid":"d86dbc50-93b4-45ac-8686-a8cae6c9bc75","isSidechain":true,"userType":"external","cwd":"/home/jjob/projects/ingestkit","sessionId":"612a5750-6827-4a12-92ae-ddbe27ee030a","version":"2.1.39","gitBranch":"main","agentId":"af08524","slug":"deep-sniffing-treehouse","message":{"model":"claude-opus-4-6","id":"msg_017nxBq15Hgu9UHF6hB9KQ2P","type":"message","role":"assistant","content":[{"type":"text","text":"Now I have all the data needed for a comprehensive report. Here is the complete research document.\n\n---\n\n# Comprehensive Research Report: Template-Driven Form Extraction from Documents\n\n## Executive Summary\n\nThis report surveys the open-source landscape for template-driven form extraction from PDFs, scanned documents, and images. The field has matured significantly through 2024-2025, with a clear shift from pure rule-based/OCR approaches toward hybrid architectures that combine traditional PDF parsing with vision-language models (VLMs). For an on-premises, plugin-based system like ingestkit, the most relevant projects are those that support self-hosted inference, backend-agnostic design, and template-driven extraction without cloud dependencies.\n\nThe projects below are organized into tiers by relevance to your use case.\n\n---\n\n## TIER 1: Directly Relevant Template-Driven Form Extraction\n\n### 1. TWIX (UC Berkeley EPIC Lab)\n\n- **GitHub:** [https://github.com/ucbepic/TWIX](https://github.com/ucbepic/TWIX)\n- **Stars:** ~211 | **Language:** Python | **Last updated:** Feb 2026\n- **License:** Not specified in search (research project)\n\n**Core Technology Stack:**\n- OCR for phrase extraction (bounding boxes)\n- Clustering algorithms for field inference\n- Visual constraint-based template inference\n- No deep learning required for core pipeline\n\n**Architecture Pattern:** Pure template-driven. TWIX automatically infers the visual template from a set of documents generated by the same template, then extracts structured data at scale.\n\n**How it works (4-step pipeline):**\n1. **Phrase Extraction** -- OCR extracts text phrases with bounding boxes\n2. **Field Inference** -- Clusters phrases by visual location patterns to identify field candidates\n3. **Template Inference** -- Infers document structure by identifying row labels with visual constraints\n4. **Data Extraction** -- Applies inferred template to extract structured data\n\n**Template Definition:** Automatic inference from at least 2 sample documents. Also provides an interactive UI playground where users can edit the inferred template.\n\n**Document Matching:** Visual location clustering -- phrases appearing at similar coordinates across documents are identified as belonging to the same template field.\n\n**Strengths for our use case:**\n- Exactly the template-driven pattern we are investigating\n- No LLM or cloud dependency for core extraction\n- Claims 520x faster and 3786x cheaper than GPT-4 Vision\n- Over 25% improvement in precision/recall vs. 6 SOTA baselines\n- Python package designed for production pipeline deployment\n- Requires only 2 sample documents to infer a template\n\n**Weaknesses:**\n- Relatively small project (211 stars), limited community\n- Designed for \"templatized documents\" (programmatically generated forms) -- may struggle with hand-designed or inconsistent layouts\n- Limited multi-format support (focused on document images/PDFs)\n- Research project, not battle-tested in enterprise settings\n\n**Key pattern to adopt:** The 4-step template inference pipeline (OCR -> cluster -> infer template -> extract) is an elegant approach that could be adapted for Excel-based form extraction where the \"template\" is the visual layout of the spreadsheet.\n\n---\n\n### 2. Sparrow (Katana ML)\n\n- **GitHub:** [https://github.com/katanaml/sparrow](https://github.com/katanaml/sparrow)\n- **Stars:** ~5,121 | **Language:** Python | **Last updated:** Feb 2026\n- **License:** GPL-3.0\n\n**Core Technology Stack:**\n- Vision LLMs (Mistral, QwenVL, etc.)\n- MLX (Apple Silicon), Ollama, vLLM backends\n- JSON schema-based extraction with validation\n- RESTful APIs\n\n**Architecture Pattern:** Hybrid (schema-driven + VLM). Users define a JSON extraction schema, and the system uses Vision LLMs to extract fields matching that schema.\n\n**Template Definition:** JSON schema that defines the fields to extract. No visual template editor -- schema is code/config.\n\n**Document Matching:** Not automatic; the user specifies which schema to apply.\n\n**Multi-format:** PDF, images, invoices, receipts, forms, bank statements, tables.\n\n**Strengths for our use case:**\n- Pluggable architecture with multiple backend support (Ollama for on-premises)\n- JSON schema-based extraction aligns well with Pydantic validation patterns\n- Supports local Vision LLMs -- no cloud dependency\n- Bounding box annotations for precise field location tracking\n- Active development, decent community (5k+ stars)\n\n**Weaknesses:**\n- GPL-3.0 license is restrictive for commercial embedding\n- Relies on VLMs, so requires GPU for reasonable performance\n- No automatic template inference -- user must define schemas manually\n- No built-in document classification/matching\n\n**Key pattern to adopt:** JSON schema-driven extraction with automatic validation, and the pluggable backend architecture (Ollama/vLLM/MLX) for on-premises LLM inference.\n\n---\n\n### 3. Parsee (SimFin)\n\n- **GitHub:** [https://github.com/parsee-ai/parsee-core](https://github.com/parsee-ai/parsee-core)\n- **Stars:** ~83 | **Language:** Python | **Last updated:** Jan 2026\n\n**Core Technology Stack:**\n- LLMs as \"universal\" extractors\n- Custom task-specific AI models (trainable)\n- PDF parsing, HTML parsing, image support\n- Type-safe extraction templates\n\n**Architecture Pattern:** Template-driven + LLM hybrid. Define type-safe extraction templates, use LLMs for initial extraction, then optionally train custom models for accuracy/cost optimization.\n\n**Template Definition:** Code-defined type-safe extraction templates that guarantee output format. Can be edited/shared via hosted UI.\n\n**Document Matching:** Manual assignment of extraction templates to documents.\n\n**Strengths for our use case:**\n- Type-safe extraction templates align perfectly with Pydantic model validation\n- Progressive refinement path: start with LLM, graduate to custom models\n- Runs entirely locally from Python environment\n- Specialized in tabular data extraction from PDFs\n\n**Weaknesses:**\n- Very small project (83 stars), limited community support\n- Early stage, limited documentation\n- Less mature than alternatives\n\n**Key pattern to adopt:** The progressive refinement strategy (LLM -> custom model) and type-safe template definitions.\n\n---\n\n### 4. form-extractor-ocr (HarendraKumarSingh)\n\n- **GitHub:** [https://github.com/HarendraKumarSingh/form-extractor-ocr](https://github.com/HarendraKumarSingh/form-extractor-ocr)\n- **Stars:** ~18 | **Language:** Python | **Last updated:** Apr 2025\n\n**Core Technology Stack:**\n- OpenCV for template matching and image alignment\n- Tesseract (pytesseract) for OCR\n- Custom character/alphabet OCR training\n\n**Architecture Pattern:** Classic template-driven. Define regions on a template image, align incoming documents to the template via image registration, crop regions, OCR each region independently.\n\n**How it works:**\n1. Define template with field regions (bounding boxes)\n2. Align scanned document to template using image registration\n3. Extract individual boxes/regions\n4. Run OCR on each region\n5. Output field values with confidence scores\n\n**Strengths for our use case:**\n- Pure template-driven approach -- exactly the region-based extraction pattern\n- No LLM or cloud dependency\n- Confidence scoring per field (red <80%, blue >=80%)\n- Trainable character recognition\n\n**Weaknesses:**\n- Tiny project, essentially a demo/prototype\n- Only handles scanned bank forms\n- No PDF support (image-only)\n- No automatic template inference\n\n**Key pattern to adopt:** The region-based extraction pattern (template definition -> document alignment -> region crop -> per-region OCR) is the canonical approach for template-driven form extraction.\n\n---\n\n## TIER 2: Document Intelligence Platforms (Open-Source Textract/DocAI Alternatives)\n\n### 5. PaddleOCR + PP-Structure\n\n- **GitHub:** [https://github.com/PaddlePaddle/PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR)\n- **Stars:** ~70,698 | **Language:** Python | **Last updated:** Feb 2026 (very active)\n- **License:** Apache-2.0\n\n**Core Technology Stack:**\n- PaddlePaddle deep learning framework\n- PP-OCRv5 for text detection/recognition (100+ languages)\n- PP-StructureV3 for layout analysis, table recognition, key information extraction\n- PP-ChatOCRv4 for LLM-driven KIE\n- YOLO-based layout detection\n- SLANet for table structure recognition\n\n**Architecture Pattern:** Deep learning pipeline. Modular stages: layout analysis -> text detection -> text recognition -> table structure recognition -> key information extraction.\n\n**PP-Structure specifically provides:**\n- **Layout Analysis:** Identifies regions (text, table, figure, title, etc.)\n- **Table Recognition:** Extracts table structure and content to HTML/Excel\n- **Key Information Extraction (KIE):** Semantic Entity Recognition (SER) and Relation Extraction (RE) -- identifies form fields and their values\n- **Layout Recovery:** Reconstructs document layout in Word/PDF\n\n**Template Definition:** No visual template editor. KIE uses LayoutLM-family models fine-tuned on labeled data. Pre-trained models available for common document types.\n\n**Document Matching:** Layout analysis classifies regions; KIE models are trained per document type.\n\n**Multi-format:** PDF, images. No native Excel support.\n\n**Strengths for our use case:**\n- Apache-2.0 license -- fully permissive\n- Massive community, production-hardened\n- CPU and GPU support, mobile/edge variants\n- KIE module (SER + RE) is specifically designed for form field extraction\n- Can run entirely on-premises\n- 100+ language support\n\n**Weaknesses:**\n- PaddlePaddle framework dependency (not PyTorch/TensorFlow ecosystem)\n- Requires training/fine-tuning for custom form types\n- Heavy installation footprint\n- Documentation is sometimes incomplete in English\n\n**Key pattern to adopt:** The SER (Semantic Entity Recognition) + RE (Relation Extraction) pattern for form KIE is extremely relevant -- it identifies \"what is a field label\" and \"what is a field value\" and then links them together.\n\n---\n\n### 6. Docling (IBM / LF AI & Data Foundation)\n\n- **GitHub:** [https://github.com/docling-project/docling](https://github.com/docling-project/docling)\n- **Stars:** ~52,840 | **Language:** Python | **Last updated:** Feb 2026 (very active)\n- **License:** MIT\n\n**Core Technology Stack:**\n- Custom PDF parser (`docling-parse`)\n- Granite-Docling-258M vision-language model (Apache-2.0)\n- Layout analysis models\n- Table structure recognition\n- OCR integration\n\n**Architecture Pattern:** Pipeline with VLM acceleration. Parses documents through a multi-stage pipeline with optional VLM enhancement.\n\n**Multi-format support:** PDF, DOCX, PPTX, XLSX, HTML, images (PNG, TIFF, JPEG), LaTeX, audio (WAV, MP3), and more. **Notable: native XLSX support.**\n\n**Strengths for our use case:**\n- MIT license\n- Huge community (52k+ stars), actively maintained by IBM\n- Native XLSX support -- directly relevant to ingestkit\n- Granite-Docling-258M is a small (258M params), Apache-2.0 VLM that can run on modest hardware\n- Part of LF AI & Data Foundation (enterprise governance)\n- Export to Markdown/JSON for LLM consumption\n- docling-graph extension for knowledge graph construction\n\n**Weaknesses:**\n- Not specifically template-driven -- more of a general document conversion tool\n- No visual template definition\n- Form field extraction is not a primary focus\n- Still maturing (relatively new as a standalone project)\n\n**Key pattern to adopt:** The multi-format pipeline architecture (especially XLSX support), the small-model VLM approach (Granite-Docling-258M), and the structured output format.\n\n---\n\n### 7. deepdoctection\n\n- **GitHub:** [https://github.com/deepdoctection/deepdoctection](https://github.com/deepdoctection/deepdoctection)\n- **Stars:** ~3,137 | **Language:** Python | **Last updated:** Feb 2026\n- **License:** Apache-2.0\n\n**Core Technology Stack:**\n- Detectron2 / Transformers for layout analysis\n- Tesseract, docTR, or AWS Textract for OCR (pluggable)\n- LayoutLM family, LiLT, BERT-style models for token classification\n- Supports sliding window for long documents\n\n**Architecture Pattern:** Orchestration framework. Does not implement models itself -- enables building pipelines from existing models. A \"meta-framework\" for Document AI.\n\n**Template Definition:** No template system. Uses ML models for layout analysis and token classification.\n\n**Strengths for our use case:**\n- Apache-2.0 license\n- Backend-agnostic design (pluggable OCR, pluggable layout models)\n- Pipeline composition pattern similar to ingestkit's architecture\n- Supports LayoutLM fine-tuning for custom document types\n- Works well for \"daily business documents\" (forms, reports)\n\n**Weaknesses:**\n- Smaller community than alternatives\n- Requires ML expertise for customization\n- No template-driven extraction\n- Heavy dependency tree (Detectron2, Transformers)\n\n**Key pattern to adopt:** The pipeline orchestration pattern where the framework composes pluggable detection/recognition/classification components is architecturally very similar to ingestkit's protocol-based design.\n\n---\n\n### 8. Unstract (Zipstack)\n\n- **GitHub:** [https://github.com/Zipstack/unstract](https://github.com/Zipstack/unstract)\n- **Stars:** ~6,120 | **Language:** Python | **Last updated:** Feb 2026\n- **License:** AGPL-3.0\n\n**Core Technology Stack:**\n- LLM-powered extraction (supports DeepSeek, Mistral, Llama via Ollama)\n- Pluggable vector databases (PGVector, ChromaDB, Weaviate)\n- Pluggable embedding models (Ollama, Hugging Face)\n- Pluggable text extractors (LLMWhisperer, Unstructured.io, LlamaParse)\n- Docker-based deployment\n\n**Architecture Pattern:** LLM-powered ETL platform. No-code platform where users define extraction schemas via \"Prompt Studio\" and deploy as API endpoints.\n\n**Template Definition:** \"Prompt Studio\" -- a visual environment for defining extraction schemas. Compare outputs from different LLMs side-by-side.\n\n**Document Matching:** Not automatic; user assigns templates to document types.\n\n**Strengths for our use case:**\n- Fully self-hostable via Docker\n- AI stack agnostic (any LLM, any vector DB, any embedder)\n- Dual-LLM verification for trustworthy output\n- Reduces LLM token usage by up to 8x\n- \"Prompt Studio\" for visual schema definition\n\n**Weaknesses:**\n- AGPL-3.0 license is very restrictive\n- Heavy platform (not a library -- a full application)\n- Requires LLM for all extraction (no rule-based fallback)\n- Over-engineered for embedded use in a plugin system\n\n**Key pattern to adopt:** The dual-LLM verification pattern (two LLMs cross-check each other) and the \"Prompt Studio\" concept for interactive schema definition.\n\n---\n\n### 9. Unstructured (Unstructured-IO)\n\n- **GitHub:** [https://github.com/Unstructured-IO/unstructured](https://github.com/Unstructured-IO/unstructured)\n- **Stars:** ~13,962 | **Language:** Python/HTML | **Last updated:** Feb 2026\n- **License:** Apache-2.0\n\n**Core Technology Stack:**\n- Partitioning bricks (break documents into structured elements)\n- Cleaning bricks (remove boilerplate, sentence fragments)\n- Staging bricks (format for downstream tasks)\n- Supports 25+ document types\n- Integration with LLM ecosystems (LangChain, LlamaIndex)\n\n**Architecture Pattern:** ETL pipeline with modular \"bricks.\" Each brick performs a specific transformation step. Documents flow through a pipeline of partitioning -> cleaning -> staging.\n\n**Template Definition:** No template system. Rule-based partitioning with ML-enhanced element detection.\n\n**Multi-format:** PDF, HTML, Word, PowerPoint, Excel, images, email, and many more.\n\n**Strengths for our use case:**\n- Apache-2.0 license\n- Excellent multi-format support including Excel\n- Modular \"brick\" architecture -- very composable\n- Large community, active development\n- Good LLM ecosystem integrations\n\n**Weaknesses:**\n- Enterprise Platform product is separate (not open source)\n- Not template-driven -- general-purpose document ETL\n- No form field extraction specifically\n- Open source version has fewer capabilities than Platform\n\n**Key pattern to adopt:** The \"brick\" composition pattern (partitioning bricks -> cleaning bricks -> staging bricks) for pipeline construction.\n\n---\n\n## TIER 3: OCR Engines and Document Understanding Models\n\n### 10. docTR (Mindee)\n\n- **GitHub:** [https://github.com/mindee/doctr](https://github.com/mindee/doctr)\n- **Stars:** ~5,858 | **Language:** Python | **Last updated:** Feb 2026\n- **License:** Apache-2.0\n\n**Core Technology Stack:**\n- Two-stage pipeline: text detection + text recognition\n- PyTorch and TensorFlow backends (pluggable)\n- Detection: DBNet, LinkNet, FAST\n- Recognition: CRNN, SAR, MASTER, ViTSTR\n\n**Architecture Pattern:** Pure OCR pipeline. Detect text regions, then recognize characters within each region.\n\n**Strengths for our use case:**\n- Apache-2.0 license\n- High-quality OCR with modern deep learning models\n- Dual-backend support (PyTorch/TensorFlow)\n- Good building block for a template-driven system (provides the OCR layer)\n- French company (Mindee) -- enterprise backing\n\n**Weaknesses:**\n- Pure OCR -- no template system, no form understanding\n- No layout analysis beyond text detection\n- Requires integration work to build form extraction on top\n\n**Key pattern to adopt:** Could serve as the OCR engine in a template-driven pipeline. The two-stage detect-then-recognize pattern is clean and composable.\n\n---\n\n### 11. Surya (Datalab / Vik Paruchuri)\n\n- **GitHub:** [https://github.com/datalab-to/surya](https://github.com/datalab-to/surya)\n- **Stars:** ~19,267 | **Language:** Python | **Last updated:** Feb 2026\n- **License:** GPL-3.0 (commercial license available)\n\n**Core Technology Stack:**\n- Custom EfficientViT-based text detection model\n- Modified Donut model for text recognition (GQA, MoE layers, UTF-16)\n- Layout analysis (table, image, header detection)\n- Reading order detection\n- 90+ language support\n\n**Architecture Pattern:** End-to-end document OCR toolkit with layout understanding.\n\n**Strengths for our use case:**\n- Very high quality OCR and layout analysis\n- Table recognition included\n- 90+ languages\n- Reading order detection (important for form parsing)\n- Active development by same author as Marker\n\n**Weaknesses:**\n- GPL-3.0 license (commercial license required for embedding)\n- No template system\n- GPU recommended for good performance\n\n**Key pattern to adopt:** The reading order detection is valuable for form extraction -- understanding which label goes with which value depends on reading order.\n\n---\n\n### 12. Donut (CLOVA AI / Naver)\n\n- **GitHub:** [https://github.com/clovaai/donut](https://github.com/clovaai/donut)\n- **Stars:** ~6,786 | **Language:** Python | **Last updated:** Feb 2026\n- **License:** MIT\n\n**Core Technology Stack:**\n- Swin Transformer (image encoder)\n- BART (text decoder)\n- OCR-free end-to-end architecture\n- SynthDoG synthetic document generator\n\n**Architecture Pattern:** End-to-end VLM. No OCR step -- the model directly reads document images and outputs structured text/JSON.\n\n**Template Definition:** Output format is defined as JSON schema. The model is fine-tuned to output JSON matching the schema for a given document type.\n\n**Strengths for our use case:**\n- MIT license\n- Eliminates OCR dependency entirely\n- JSON output for form extraction\n- SynthDoG can generate synthetic training data\n- State-of-the-art on form understanding benchmarks\n\n**Weaknesses:**\n- Requires fine-tuning per document type (not zero-shot for forms)\n- GPU required for inference\n- Older model (2022) -- newer VLMs (Qwen2.5-VL, Florence-2) may outperform\n- Not template-driven in the traditional sense\n\n**Key pattern to adopt:** The OCR-free approach and JSON schema output format. SynthDoG for generating training data for custom form types.\n\n---\n\n### 13. LayoutLMv3 (Microsoft)\n\n- **GitHub:** Part of [https://github.com/microsoft/unilm](https://github.com/microsoft/unilm) (~22,020 stars)\n- **Available on Hugging Face:** [https://huggingface.co/microsoft/layoutlmv3-base](https://huggingface.co/microsoft/layoutlmv3-base)\n- **License:** CC-BY-NC-SA-4.0 (non-commercial only for pre-trained weights)\n\n**Core Technology Stack:**\n- Multimodal Transformer (text + layout + image)\n- BERT-like architecture\n- No CNN/Faster-RCNN backbone required (unlike LayoutLMv1/v2)\n- Token classification for entity extraction\n\n**Architecture Pattern:** Pre-trained multimodal model. Fine-tune on labeled form data for: form understanding, receipt understanding, document VQA, document classification, layout analysis.\n\n**Strengths for our use case:**\n- State-of-the-art on form understanding benchmarks (FUNSD, CORD)\n- Multimodal -- uses text, layout position, and image simultaneously\n- Available via Hugging Face Transformers library\n- Can be fine-tuned for custom form types\n\n**Weaknesses:**\n- CC-BY-NC-SA-4.0 license -- **non-commercial use only** for pre-trained weights\n- Requires labeled training data for fine-tuning\n- Not a template system -- ML model-based\n- Requires GPU for training and inference\n\n**Key pattern to adopt:** The multimodal approach (text + position + image) for form field understanding. The SER (Semantic Entity Recognition) pattern where each token is classified as header/question/answer.\n\n---\n\n### 14. LayoutParser\n\n- **GitHub:** [https://github.com/Layout-Parser/layout-parser](https://github.com/Layout-Parser/layout-parser)\n- **Stars:** ~5,650 | **Language:** Python | **Last updated:** Feb 2026\n- **License:** Apache-2.0\n\n**Core Technology Stack:**\n- Detectron2 for layout detection\n- Pre-trained models for various document types\n- GCV (Google Cloud Vision) OCR integration (optional)\n- Community model sharing platform\n\n**Architecture Pattern:** Layout detection toolkit. Provides pre-trained models that identify document regions (text, table, figure, list, title), then downstream tools extract content from each region.\n\n**Strengths for our use case:**\n- Apache-2.0 license\n- Clean API (4 lines of code for basic layout analysis)\n- Pre-trained models for many document types\n- Supports custom model training\n- Community model sharing\n\n**Weaknesses:**\n- Focused on layout detection only -- no OCR or form extraction built in\n- Detectron2 dependency (heavy)\n- Less active development recently\n- No template system\n\n**Key pattern to adopt:** Region-based document decomposition as a pre-processing step before form extraction.\n\n---\n\n## TIER 4: PDF Parsing Libraries (Building Blocks)\n\n### 15. pdfplumber\n\n- **GitHub:** [https://github.com/jsvine/pdfplumber](https://github.com/jsvine/pdfplumber)\n- **Stars:** ~9,702 | **Language:** Python | **Last updated:** Feb 2026\n- **License:** MIT\n\n**Core Technology Stack:**\n- Built on pdfminer.six\n- Character-level text extraction with coordinates\n- Line/rectangle/curve detection\n- Table extraction (Lattice and Stream modes)\n- Visual debugging\n\n**Region-Based Extraction:**\n- `Page.crop(bounding_box)` -- crop a page to a specific region\n- `Page.within_bbox(bbox)` -- extract text within a bounding box\n- All objects include position coordinates\n- Can identify lines, rectangles, and intersections for table detection\n\n**Form Field Extraction:**\n- No direct AcroForm interface, but can access form data via pdfminer wrappers\n- Primarily designed for visual/structural extraction, not fillable form fields\n\n**Strengths for our use case:**\n- MIT license\n- Excellent for region-based extraction from PDFs\n- Precise character-level coordinate data\n- Visual debugging tools\n- Good table extraction\n\n**Weaknesses:**\n- Text PDFs only (no OCR for scanned documents)\n- No form field (AcroForm/widget) native support\n- No template system\n- No layout classification\n\n**Key pattern to adopt:** The region-based cropping approach (`crop` + `within_bbox`) is directly usable for template-driven extraction from text-layer PDFs.\n\n---\n\n### 16. PyMuPDF (fitz)\n\n- **GitHub:** [https://github.com/pymupdf/PyMuPDF](https://github.com/pymupdf/PyMuPDF)\n- **Stars:** ~9,039 | **Language:** Python | **Last updated:** Feb 2026\n- **License:** AGPL-3.0 (commercial license available)\n\n**Core Technology Stack:**\n- MuPDF C library bindings\n- High-performance PDF/XPS/EPUB rendering\n- Widget class for form fields\n- Text extraction with coordinates\n- Image extraction\n\n**Form Field Extraction (AcroForm/Widgets):**\n- `page.widgets()` iterator -- access all form fields on a page\n- Widget properties: `field_name`, `field_value`, `field_type`, `rect` (bounding box)\n- Supports: text fields, checkboxes, radio buttons, dropdowns, list boxes\n- Can read AND write form field values\n- Detects whether a PDF is a form PDF via `/AcroForm` object\n\n**Strengths for our use case:**\n- Native, high-performance form field extraction\n- Can both read and fill form fields\n- Widget bounding boxes enable spatial reasoning\n- Extremely fast (C-based)\n- Comprehensive PDF feature support\n\n**Weaknesses:**\n- AGPL-3.0 license (restrictive; commercial license costs money)\n- Only works with fillable PDF forms (AcroForm), not scanned forms\n- No OCR or layout analysis built in\n- No template system\n\n**Key pattern to adopt:** For fillable PDF forms, PyMuPDF's Widget API is the gold standard. The `field_name` + `field_value` + `rect` combination gives you structured form data with spatial coordinates.\n\n---\n\n### 17. Camelot\n\n- **GitHub:** [https://github.com/camelot-dev/camelot](https://github.com/camelot-dev/camelot)\n- **Stars:** ~3,595 | **Language:** Python | **Last updated:** Feb 2026\n- **License:** MIT\n\n**Core Technology Stack:**\n- Two modes: Lattice (cell borders) and Stream (spacing-based)\n- Ghostscript/PyPDF2 backends\n- Export to pandas DataFrame, CSV, JSON, HTML, Excel\n\n**Strengths:** Excellent for tables with drawn cell borders (Lattice mode). MIT license. Exports to pandas.\n\n**Weaknesses:** Table-focused, not form-focused. Text PDFs only. Stream mode less reliable.\n\n---\n\n## TIER 5: Full Document Conversion Pipelines\n\n### 18. Marker (Datalab / Vik Paruchuri)\n\n- **GitHub:** [https://github.com/datalab-to/marker](https://github.com/datalab-to/marker)\n- **Stars:** ~31,620 | **Language:** Python | **Last updated:** Feb 2026\n- **License:** GPL-3.0 (commercial license available)\n\n**Architecture:** Provider -> Builder -> Processor -> Renderer pipeline. Extensible via plugins.\n\n**Key components:** Providers (source file access), Builders (initial document blocks + text), Processors (block-level processing, e.g., table formatting), Renderers (output generation), Schema (block type classes), Converters.\n\n**Form-related:** Optional LLM integration can \"extract values from forms\" as one of its capabilities. Table extraction is a strong suit.\n\n**Strengths:** Very high quality PDF->Markdown conversion. Extensible plugin architecture. Active development.\n\n**Weaknesses:** GPL-3.0 license. Primarily a conversion tool, not a form extraction tool. GPU recommended (2GB VRAM per task).\n\n**Key pattern to adopt:** The Provider -> Builder -> Processor -> Renderer pipeline architecture is clean and extensible.\n\n---\n\n### 19. MinerU (OpenDataLab)\n\n- **GitHub:** [https://github.com/opendatalab/MinerU](https://github.com/opendatalab/MinerU)\n- **Stars:** ~54,291 | **Language:** Python | **Last updated:** Feb 2026\n- **License:** AGPL-3.0\n\n**Architecture:** Two-stage coarse-to-fine parsing: Stage I (global layout analysis on thumbnails via NaViT + Patch Merger + LLM), Stage II (targeted high-resolution recognition on cropped regions with context-specific prompts).\n\n**Strengths:** Massive community. Handles complex layouts. Multi-column support. Can run on CPU (pipeline backend).\n\n**Weaknesses:** AGPL-3.0. Not template-driven. Primarily a document conversion tool.\n\n**Key pattern to adopt:** The coarse-to-fine two-stage approach (thumbnail layout analysis -> high-resolution region extraction) is relevant for form processing.\n\n---\n\n### 20. PDF-Extract-Kit (OpenDataLab)\n\n- **GitHub:** [https://github.com/opendatalab/PDF-Extract-Kit](https://github.com/opendatalab/PDF-Extract-Kit)\n- **Stars:** ~9,350 | **Language:** Python | **Last updated:** Feb 2026\n- **License:** Apache-2.0\n\n**Architecture:** Modular toolkit with configurable components for layout detection, formula detection, formula recognition, OCR. Uses DocLayout-YOLO for layout analysis.\n\n**Strengths:** Apache-2.0 license. Modular design (\"stacking blocks\"). Robust to blurring/watermarks.\n\n**Weaknesses:** PDF-focused. Not template-driven.\n\n---\n\n### 21. Documind\n\n- **GitHub:** [https://github.com/DocumindHQ/documind](https://github.com/DocumindHQ/documind)\n- **Stars:** ~1,465 | **Language:** JavaScript/TypeScript | **Last updated:** Feb 2026\n\n**Architecture:** Converts PDFs to images, uses OpenAI API for extraction with customizable schemas. Built-in templates for invoices, bank statements.\n\n**Strengths:** Schema-based extraction. Built-in document type templates.\n\n**Weaknesses:** JavaScript (not Python). Requires OpenAI API (cloud dependency). Built on Zerox.\n\n---\n\n### 22. Open-Parse (Filimoa)\n\n- **GitHub:** [https://github.com/Filimoa/open-parse](https://github.com/Filimoa/open-parse)\n- **Stars:** ~3,152 | **Language:** Python | **Last updated:** Feb 2026\n- **License:** MIT\n\n**Architecture:** Visual analysis pipeline. Parses text into markdown by examining font size/style character by character, combines into spans, groups into lines, groups into elements. Includes semantic processing pipeline using embeddings.\n\n**Strengths:** MIT license. Visual analysis approach. Semantic clustering for grouping related content. Extensible post-processing.\n\n**Weaknesses:** Focused on LLM-ready parsing, not form extraction specifically.\n\n---\n\n## TIER 6: Vision-Language Models (Emerging Approach)\n\n### 23. Qwen2.5-VL (Alibaba)\n\n- **Hugging Face:** [https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)\n- **Sizes:** 3B, 7B, 72B parameter variants\n- **License:** Apache-2.0\n\n**Capabilities:** Structured data extraction from invoices, forms, tables. Bounding box and point detection. Stable JSON output with coordinates.\n\n**Strengths for our use case:**\n- Apache-2.0 license (even for weights)\n- Can run locally via Ollama (7B fits on consumer hardware)\n- Bounding box output enables spatial reasoning about form fields\n- JSON structured output mode\n- Fine-tunable with QLoRA on modest hardware (demonstrated >100% accuracy gains)\n- Top-tier document understanding benchmarks\n\n**Weaknesses:** Requires GPU. Not template-driven (prompt-driven). Inference cost per document.\n\n**Key pattern to adopt:** Using a local VLM (Qwen2.5-VL-7B via Ollama) as the \"reasoning engine\" in a template-driven pipeline. The model can identify form fields, read values, and output structured JSON -- all on-premises.\n\n---\n\n## Comparative Summary Table\n\n| Project | Stars | License | Template-Driven? | On-Premises? | Form Focus? | Multi-Format? |\n|---------|-------|---------|-------------------|-------------|-------------|---------------|\n| **TWIX** | 211 | Research | Yes (auto-inferred) | Yes | Yes | PDF/Image |\n| **Sparrow** | 5.1k | GPL-3.0 | JSON schema | Yes (Ollama) | Yes | PDF/Image |\n| **Parsee** | 83 | - | Type-safe templates | Yes | Partial | PDF/HTML/Image |\n| **PaddleOCR** | 70.7k | Apache-2.0 | No (ML models) | Yes | Yes (KIE) | PDF/Image |\n| **Docling** | 52.8k | MIT | No | Yes | No | PDF/XLSX/DOCX/+ |\n| **deepdoctection** | 3.1k | Apache-2.0 | No (ML pipeline) | Yes | Partial | PDF/Image |\n| **Unstract** | 6.1k | AGPL-3.0 | Prompt Studio | Yes (Docker) | Yes | PDF/Image |\n| **Unstructured** | 14k | Apache-2.0 | No | Yes | No | 25+ formats |\n| **docTR** | 5.9k | Apache-2.0 | No (OCR only) | Yes | No | PDF/Image |\n| **Surya** | 19.3k | GPL-3.0 | No (OCR+Layout) | Yes | No | PDF/Image |\n| **Donut** | 6.8k | MIT | JSON schema (fine-tuned) | Yes | Yes | Image |\n| **LayoutLMv3** | 22k* | CC-BY-NC-SA | No (fine-tuned) | Yes | Yes | Image |\n| **pdfplumber** | 9.7k | MIT | No (but crop API) | Yes | Partial | PDF only |\n| **PyMuPDF** | 9.0k | AGPL-3.0 | No (Widget API) | Yes | Yes (fillable) | PDF |\n| **Marker** | 31.6k | GPL-3.0 | No | Yes | Partial | PDF |\n| **MinerU** | 54.3k | AGPL-3.0 | No | Yes | No | PDF |\n\n*LayoutLMv3 stars are for the parent unilm repository.\n\n---\n\n## Key Architectural Patterns Identified\n\n### Pattern 1: Template Inference Pipeline (TWIX)\n```\nDocuments -> OCR -> Phrase Clustering -> Template Inference -> Data Extraction\n```\nBest for: Batch processing of documents from the same template. No training required. Requires 2+ sample documents.\n\n### Pattern 2: Schema-Driven VLM Extraction (Sparrow, Documind)\n```\nDocument Image -> Vision LLM -> JSON Schema Validation -> Structured Output\n```\nBest for: Flexible extraction where document types vary. Requires GPU. Good accuracy with modern VLMs.\n\n### Pattern 3: Region-Based Template Extraction (form-extractor-ocr, pdfplumber crop)\n```\nTemplate Definition (regions) -> Document Alignment -> Region Crop -> Per-Region OCR -> Field Values\n```\nBest for: Fixed-layout forms where field positions are known. Highly deterministic. No ML required.\n\n### Pattern 4: SER + RE (PaddleOCR KIE, LayoutLMv3)\n```\nOCR -> Token Classification (SER: header/question/answer) -> Relation Extraction (RE: link Q->A) -> Structured Output\n```\nBest for: Forms where field labels and values need to be linked. Requires training data. Most accurate for complex forms.\n\n### Pattern 5: Coarse-to-Fine (MinerU, PaddleOCR PP-Structure)\n```\nThumbnail -> Layout Analysis -> Region Crop -> High-Res Recognition -> Structured Output\n```\nBest for: Complex documents with mixed content. Two-stage approach balances speed and accuracy.\n\n### Pattern 6: Pipeline Orchestration (deepdoctection, Marker, Unstructured)\n```\nPluggable stages: [Detection] -> [Recognition] -> [Classification] -> [Extraction] -> [Output]\n```\nBest for: Building customizable pipelines from composable components. Matches ingestkit's protocol-based architecture.\n\n---\n\n## Recommendations for ingestkit\n\nGiven ingestkit's constraints (on-premises, no cloud, plugin architecture, Protocol-based backends, Pydantic models), the most relevant patterns and projects are:\n\n1. **TWIX's template inference approach** is the most directly relevant for template-driven extraction. The 4-step pipeline (OCR -> cluster -> infer template -> extract) could be adapted for Excel form processing. However, the project is small and may need significant adaptation.\n\n2. **Sparrow's pluggable VLM architecture** with JSON schema validation aligns well with ingestkit's existing patterns (Protocol types, Pydantic models, Ollama backend). The JSON schema-driven extraction with automatic validation maps cleanly to Pydantic BaseModel validation.\n\n3. **PaddleOCR's KIE module** (SER + RE) is the most mature form field extraction system in the open-source ecosystem. The SER (Semantic Entity Recognition) pattern -- classifying each text token as header/question/answer and then linking them -- is the production-proven approach for form understanding.\n\n4. **Docling** deserves attention for its native XLSX support and MIT license. If ingestkit needs to expand beyond Excel into PDFs and images, Docling provides a clean multi-format pipeline.\n\n5. **Qwen2.5-VL-7B** (Apache-2.0) via Ollama is the most practical VLM for on-premises form extraction. It already aligns with ingestkit's `classification_model` pattern and could serve as the \"form understanding\" engine in a template-driven pipeline.\n\n6. **The region-based extraction pattern** (template definition -> alignment -> crop -> OCR) is the simplest and most deterministic approach for fixed-layout forms. pdfplumber's `crop()` API or openpyxl's cell range addressing already provide the building blocks for this in the PDF and Excel domains respectively.\n\n---\n\nSources:\n- [TWIX - UC Berkeley EPIC Lab](https://github.com/ucbepic/TWIX)\n- [TWIX Research Blog Post](https://data-people-group.github.io/blogs/2025/04/30/twix/)\n- [Sparrow - Katana ML](https://github.com/katanaml/sparrow)\n- [Parsee Core](https://github.com/parsee-ai/parsee-core)\n- [form-extractor-ocr](https://github.com/HarendraKumarSingh/form-extractor-ocr)\n- [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR)\n- [PaddleOCR PP-Structure](https://github.com/PaddlePaddle/PaddleOCR/blob/release/2.7/ppstructure/README.md)\n- [Docling](https://github.com/docling-project/docling)\n- [IBM Granite-Docling-258M](https://huggingface.co/ibm-granite/granite-docling-258M)\n- [deepdoctection](https://github.com/deepdoctection/deepdoctection)\n- [Unstract](https://github.com/Zipstack/unstract)\n- [Unstructured](https://github.com/Unstructured-IO/unstructured)\n- [docTR](https://github.com/mindee/doctr)\n- [Surya](https://github.com/datalab-to/surya)\n- [Donut](https://github.com/clovaai/donut)\n- [LayoutLMv3 on Hugging Face](https://huggingface.co/microsoft/layoutlmv3-base)\n- [LayoutParser](https://github.com/Layout-Parser/layout-parser)\n- [pdfplumber](https://github.com/jsvine/pdfplumber)\n- [PyMuPDF](https://github.com/pymupdf/PyMuPDF)\n- [Camelot](https://github.com/camelot-dev/camelot)\n- [Marker](https://github.com/datalab-to/marker)\n- [MinerU](https://github.com/opendatalab/MinerU)\n- [PDF-Extract-Kit](https://github.com/opendatalab/PDF-Extract-Kit)\n- [Documind](https://github.com/DocumindHQ/documind)\n- [Open-Parse](https://github.com/Filimoa/open-parse)\n- [Qwen2.5-VL on Hugging Face](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)\n- [Microsoft Knowledge Extraction Recipes for Forms](https://github.com/microsoft/knowledge-extraction-recipes-forms)\n- [7 Python PDF Extractors Comparison](https://onlyoneaman.medium.com/i-tested-7-python-pdf-extractors-so-you-dont-have-to-2025-edition-c88013922257)\n- [Best Open Source OCR Tools 2025](https://unstract.com/blog/best-opensource-ocr-tools-in-2025/)\n- [8 Top Open-Source OCR Models Compared](https://modal.com/blog/8-top-open-source-ocr-models-compared)\n- [Comparing 6 Frameworks for Rule-based PDF Parsing](https://www.ai-bites.net/comparing-6-frameworks-for-rule-based-pdf-parsing/)\n- [Deep Dive into PDF to Markdown Tools](https://jimmysong.io/blog/pdf-to-markdown-open-source-deep-dive/)"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":1203,"cache_read_input_tokens":43049,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":1203},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY4Xhj7FkaVNmBnyic63w","type":"assistant","uuid":"baa9f369-20a1-4184-8b76-f36526c4baa8","timestamp":"2026-02-12T17:01:59.993Z"}
