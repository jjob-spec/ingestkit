---
issue: 29
agent: PLAN
date: 2026-02-14
complexity: COMPLEX
stack: backend
---

# PLAN Artifact: Issue #29 -- PDF LLM Classifier

## Executive Summary

Issue #29 implements `PDFLLMClassifier` in `packages/ingestkit-pdf/src/ingestkit_pdf/llm_classifier.py`, directly adapting the proven Excel `LLMClassifier` pattern (453 lines) for PDF-specific models. The classifier generates PII-safe structural summaries from `DocumentProfile` objects, sends them to an LLM backend, validates responses via a Pydantic schema, and handles retries/failures with fail-closed semantics. The key PDF-specific differences are: `PDFType` instead of `FileType`, page-based rather than sheet-based sub-unit classification, and a `page_types` list-to-dict conversion that is more complex than Excel's `sheet_types`. Connection/timeout errors propagate to the caller (for the router to degrade to Tier 1), while JSON/schema errors are retried internally.

## File-by-File Implementation

---

### 1. `packages/ingestkit-pdf/src/ingestkit_pdf/llm_classifier.py` (~380 lines)

#### Imports

```python
from __future__ import annotations

import json
import logging
import os
import re
from typing import Any, Literal

from pydantic import BaseModel, Field, ValidationError

from ingestkit_pdf.config import PDFProcessorConfig
from ingestkit_pdf.errors import ErrorCode, IngestError
from ingestkit_pdf.models import (
    ClassificationResult,
    ClassificationTier,
    DocumentProfile,
    PageProfile,
    PageType,
    PDFType,
)
from ingestkit_pdf.protocols import LLMBackend
```

Logger: `logger = logging.getLogger("ingestkit_pdf")`

#### 1a. `PageTypeEntry` Pydantic Model

Strict sub-model for validating individual page type entries in the LLM response.

```python
class PageTypeEntry(BaseModel):
    page: int
    type: Literal["text", "scanned", "table_heavy", "form", "mixed", "blank", "vector_only", "toc"]
```

Purpose: Used within `LLMClassificationResponse.page_types` for per-entry validation. The `Literal` values match `PageType` enum values exactly.

#### 1b. `LLMClassificationResponse` Pydantic Model

```python
class LLMClassificationResponse(BaseModel):
    type: Literal["text_native", "scanned", "complex"]
    confidence: float  # NO ge/le -- clamped manually
    reasoning: str = Field(min_length=1)
    page_types: list[PageTypeEntry] | None = None
```

Key design decisions:
- `type` uses `Literal` with the three `PDFType` enum **values** (not names)
- `confidence` has no bounds constraints -- clamped manually to allow OOB warning without rejection
- `reasoning` requires `min_length=1` to reject empty strings
- `page_types` is a list of `PageTypeEntry` objects (not a dict), matching the SPEC 10.2 LLM response format

#### 1c. `PDFLLMClassifier` Class

```python
class PDFLLMClassifier:
    def __init__(self, llm: LLMBackend, config: PDFProcessorConfig) -> None:
        self._llm = llm
        self._config = config
```

Follows the same constructor pattern as Excel's `LLMClassifier`. Accepts the LLM backend protocol and config via dependency injection.

#### 1d. `classify()` Method (Public API)

```python
def classify(
    self,
    profile: DocumentProfile,
    tier: ClassificationTier,
) -> ClassificationResult:
```

**Parameters:**
- `profile: DocumentProfile` -- the structural profile from the parser/inspector
- `tier: ClassificationTier` -- must be `LLM_BASIC` or `LLM_REASONING`

**Returns:** `ClassificationResult` with `pdf_type`, `confidence`, `tier_used`, `reasoning`, `per_page_types`, `signals=None`, `degraded=False`

**Raises:** `ValueError` if `tier == ClassificationTier.RULE_BASED`

**Flow (mirrors Excel exactly):**

1. **Validate tier** -- raise `ValueError` if `RULE_BASED`
2. **Select model** -- `LLM_BASIC` -> `config.classification_model`, `LLM_REASONING` -> `config.reasoning_model`
3. **Generate structural summary** -- call `_generate_structural_summary(profile)`
4. **Build prompt** -- call `_build_classification_prompt(summary, tier)`
5. **Retry loop** (max 2 attempts):
   - Call `self._llm.classify(prompt, model, temperature, timeout)`
   - On `json.JSONDecodeError`: log `E_LLM_MALFORMED_JSON`, append JSON correction hint to prompt, `continue`
   - On `TimeoutError`: log `E_LLM_TIMEOUT`, `continue`
   - On `ConnectionError`: **re-raise** (let router handle outage degradation -- differs from Excel pattern per SPEC 10.6)
   - On other `Exception`: log `E_LLM_MALFORMED_JSON`, append JSON correction hint, `continue`
   - If `config.log_llm_prompts`: log prompt and response at DEBUG via `_redact()`
   - Call `_validate_and_parse_response(raw_dict, profile)`
   - If validation fails (returns `None`): append schema correction hint, `continue`
   - On success: call `_to_classification_result(parsed, tier)` and return
6. **All attempts exhausted** -- return fail-closed `ClassificationResult`:
   - `pdf_type=PDFType.TEXT_NATIVE` (arbitrary; confidence=0.0 signals failure)
   - `confidence=0.0`
   - `tier_used=tier`
   - `reasoning="LLM classification failed after exhausting retries. Fail-closed."`
   - `per_page_types={}` (empty dict, since the model requires `dict[int, PageType]` not `Optional`)
   - `signals=None`
   - `degraded=False` (router sets this, not the classifier)

**Critical design decision -- ConnectionError propagation:**
Unlike the Excel classifier which catches all exceptions internally, the PDF classifier lets `ConnectionError` propagate to the caller. This matches SPEC 10.6 router pseudocode which shows a `try/except` around the classifier call. `TimeoutError` and `json.JSONDecodeError` are still retried internally. This enables the router to set `degraded=True` and fall back to Tier 1 when the LLM backend is down.

#### 1e. `_generate_structural_summary()` Method (Private)

```python
def _generate_structural_summary(self, profile: DocumentProfile) -> str:
```

**Returns:** Multi-line text summary suitable for LLM prompt inclusion.

**Content (PII-safe by default -- never includes raw page text):**

```
File: {basename of profile.file_path}
Pages: {profile.page_count}
File size: {profile.file_size_bytes} bytes
Creator: {profile.metadata.creator or "unknown"}
PDF version: {profile.metadata.pdf_version or "unknown"}

Page type distribution:
- text: 15
- scanned: 3
- table_heavy: 2

Detected languages: [en, fr]
Table of contents: {"present" | "not detected"}
{if has_toc and toc_entries: "TOC entries: {count}"}
Form fields: {"present" | "none detected"}
Encrypted: {"yes" | "no"}
{if security_warnings: "Security warnings: {count}"}

Sample page profiles:
Page 1:
  - Words: 342, Text length: 1820
  - Images: 0, Image coverage: 0.0%
  - Tables: 1
  - Fonts: 3 ({font_names joined})
  - Form fields: no
  - Multi-column: no
  - Page type: text

Page 5:
  ...
```

**Sample page selection strategy:**
- Include ALL pages if `page_count <= 10`
- If `page_count > 10`: select up to 10 representative pages using diversity sampling -- pick pages that cover the most distinct `page_type` values, then fill remaining slots round-robin from pages not yet included. This avoids bias toward any one type.
- Never include raw text content. Only structural metadata from `PageProfile`.

**Font names in summary:** Join `page.font_names` with commas. If empty, show "none".

**Extraction quality:** Do NOT include `extraction_quality` details per page in the summary. The LLM is classifying document type, not judging extraction quality.

#### 1f. `_build_classification_prompt()` Method (Private)

```python
def _build_classification_prompt(
    self,
    summary: str,
    tier: ClassificationTier,
) -> str:
```

**Returns:** Full prompt string.

**Template:**

```
You are classifying a PDF file for a document ingestion system.
Based on the structural summary below, classify this file as one of:

- "text_native": Digital PDF with extractable text. Text layer is complete and reliable.
- "scanned": Pages are primarily images requiring OCR for text extraction.
- "complex": Mix of text, tables, multi-column layouts, forms, or combined scanned/digital pages.

Respond with JSON only:
{
  "type": "text_native" | "scanned" | "complex",
  "confidence": <float between 0.0 and 1.0>,
  "reasoning": "brief explanation",
  "page_types": [{"page": 1, "type": "text"}, ...]  // optional, for complex documents
}

Valid page types: "text", "scanned", "table_heavy", "form", "mixed", "blank", "vector_only", "toc"

Structural summary:
{summary}
```

Note: The `page_types` field description lists all 8 valid `PageType` values so the LLM knows the valid vocabulary. The `tier` parameter is available for future tier-specific prompt adjustments but is not used to alter the prompt in the initial implementation (matching Excel pattern).

#### 1g. `_validate_and_parse_response()` Method (Private)

```python
def _validate_and_parse_response(
    self,
    raw: dict,
    profile: DocumentProfile,
) -> tuple[LLMClassificationResponse | None, list[IngestError]]:
```

**Flow:**

1. **Pydantic schema validation** -- `LLMClassificationResponse(**raw)`. On `ValidationError`: return `(None, [IngestError(E_LLM_SCHEMA_INVALID, ...)])`

2. **Confidence bounds check** -- if `response.confidence < 0.0 or > 1.0`:
   - Clamp to `[0.0, 1.0]`
   - Append `IngestError(E_LLM_CONFIDENCE_OOB, ...)` as warning
   - Create new response via `model_copy(update={"confidence": clamped})`
   - Do NOT trigger retry (clamping is sufficient)

3. **Validate page_types** (if present):
   - Extract set of valid page numbers from `profile.pages` (all `page.page_number`)
   - For each entry in `response.page_types`: check `entry.page` exists in valid set
   - Unknown page numbers: log warning, do NOT reject (same pattern as Excel for unknown sheet names)
   - Invalid `type` values are already caught by `PageTypeEntry` Pydantic validation in step 1

4. **Return** `(response, errors)`

#### 1h. `_to_classification_result()` Method (Private)

```python
def _to_classification_result(
    self,
    response: LLMClassificationResponse,
    tier: ClassificationTier,
) -> ClassificationResult:
```

**Flow:**

1. Convert `response.type` string to `PDFType` enum: `PDFType(response.type)`
2. Convert `page_types` list to `dict[int, PageType]`:
   - If `response.page_types is None`: use empty dict `{}`
   - Otherwise: `{entry.page: PageType(entry.type) for entry in response.page_types}`
3. Return `ClassificationResult(pdf_type=..., confidence=..., tier_used=tier, reasoning=..., per_page_types=..., signals=None, degraded=False)`

**Note:** `per_page_types` is `dict[int, PageType]` (not Optional in the PDF model), so we always provide at least an empty dict. `degraded` is always `False` -- the router sets it.

#### 1i. `_redact()` Method (Private)

```python
def _redact(self, text: str) -> str:
```

Identical to Excel pattern. Iterates over `self._config.redact_patterns`, applies `re.sub(pattern, "[REDACTED]", result)` for each.

---

### 2. `packages/ingestkit-pdf/src/ingestkit_pdf/__init__.py` (Minor Update)

Add `PDFLLMClassifier` and `LLMClassificationResponse` to the public exports:

```python
from ingestkit_pdf.llm_classifier import LLMClassificationResponse, PDFLLMClassifier
```

---

### 3. `packages/ingestkit-pdf/tests/conftest.py` (Fixtures)

Add shared fixtures used by `test_llm_classifier.py`:

#### 3a. `MockLLMBackend` Class

Identical structure to Excel's `MockLLMBackend` (see `test_llm_classifier.py:36-81`):
- `__init__(responses: list[dict | Exception] | None = None)`
- `self._responses`: queue of dicts or exceptions, popped per call
- `self.calls`: list recording all calls for assertion
- `classify(prompt, model, temperature, timeout) -> dict`
- `generate(prompt, model, temperature, timeout) -> str` (raises `NotImplementedError`)

#### 3b. `_make_page_profile()` Helper

```python
def _make_page_profile(**overrides) -> PageProfile:
```

Builds a `PageProfile` with sensible text-page defaults:
- `page_number=1`
- `text_length=1500`
- `word_count=300`
- `image_count=0`
- `image_coverage_ratio=0.0`
- `table_count=0`
- `font_count=3`
- `font_names=["Arial", "Times", "Courier"]`
- `has_form_fields=False`
- `is_multi_column=False`
- `page_type=PageType.TEXT`
- `extraction_quality=ExtractionQuality(printable_ratio=0.95, avg_words_per_page=300.0, pages_with_text=1, total_pages=1, extraction_method="pdfminer")`

#### 3c. `_make_document_profile()` Helper

```python
def _make_document_profile(pages: list[PageProfile] | None = None, **overrides) -> DocumentProfile:
```

Builds a `DocumentProfile` from a list of `PageProfile` objects:
- `file_path="/tmp/test.pdf"`
- `file_size_bytes=102400`
- `page_count=len(pages)`
- `content_hash="a" * 64`
- `metadata=DocumentMetadata(creator="TestApp", pdf_version="1.7", page_count=len(pages), file_size_bytes=102400)`
- `pages=pages`
- `page_type_distribution` auto-computed from pages
- `detected_languages=["en"]`
- `has_toc=False`
- `toc_entries=None`
- `overall_quality=ExtractionQuality(...)` with sensible defaults
- `security_warnings=[]`

#### 3d. `_valid_response()` Helper

```python
def _valid_response(
    type_: str = "text_native",
    confidence: float = 0.85,
    reasoning: str = "Digital PDF with extractable text throughout.",
    page_types: list[dict[str, Any]] | None = None,
) -> dict:
```

Builds a valid LLM response dict for test assertions.

#### 3e. Pytest Fixtures

```python
@pytest.fixture()
def pdf_config() -> PDFProcessorConfig:
    return PDFProcessorConfig()

@pytest.fixture()
def document_profile() -> DocumentProfile:
    return _make_document_profile()
```

---

### 4. `packages/ingestkit-pdf/tests/test_llm_classifier.py` (~850 lines)

All test classes use `@pytest.mark.unit` marker.

#### 4a. `TestValidResponseParsing` (7 tests)

| Test Method | What It Tests |
|-------------|---------------|
| `test_valid_text_native_response_returns_correct_result` | text_native type, confidence, reasoning correctly mapped |
| `test_valid_scanned_response` | scanned type correctly mapped to `PDFType.SCANNED` |
| `test_valid_complex_response_with_page_types` | complex type with `page_types` list correctly converted to `dict[int, PageType]` |
| `test_complex_response_per_page_types_are_pagetype_enums` | All values in `per_page_types` dict are `PageType` instances |
| `test_tier_used_reflects_llm_basic` | `tier_used == ClassificationTier.LLM_BASIC` |
| `test_tier_used_reflects_llm_reasoning` | `tier_used == ClassificationTier.LLM_REASONING` |
| `test_signals_is_none_for_llm_tiers` | `result.signals is None` |

#### 4b. `TestDegradedFlag` (2 tests)

| Test Method | What It Tests |
|-------------|---------------|
| `test_successful_classification_has_degraded_false` | Successful result has `degraded=False` |
| `test_fail_closed_result_has_degraded_false` | Even fail-closed results have `degraded=False` (router sets this) |

#### 4c. `TestMalformedJsonRetry` (4 tests)

| Test Method | What It Tests |
|-------------|---------------|
| `test_json_decode_error_triggers_retry` | JSONDecodeError on attempt 1, valid on attempt 2 -> success |
| `test_generic_exception_triggers_retry` | RuntimeError on attempt 1, valid on attempt 2 -> success |
| `test_two_json_failures_returns_fail_closed` | Two JSONDecodeErrors -> confidence=0.0 |
| `test_correction_hint_appended_to_prompt_on_retry` | Second call prompt contains "IMPORTANT" and "not valid JSON" |

#### 4d. `TestSchemaValidation` (6 tests)

| Test Method | What It Tests |
|-------------|---------------|
| `test_missing_type_field_triggers_schema_error` | No `type` key -> retry -> success |
| `test_invalid_type_value_triggers_schema_error` | `"TEXT_NATIVE"` (Python name) instead of `"text_native"` (value) -> retry |
| `test_missing_reasoning_triggers_schema_error` | No `reasoning` key -> retry -> success |
| `test_empty_reasoning_triggers_schema_error` | `reasoning=""` -> retry -> success |
| `test_schema_error_retry_then_success` | Invalid first, valid second -> correct result |
| `test_two_schema_failures_returns_fail_closed` | Two invalid schemas -> confidence=0.0 |

#### 4e. `TestConfidenceBounds` (5 tests)

| Test Method | What It Tests |
|-------------|---------------|
| `test_confidence_above_1_is_clamped` | 1.5 -> 1.0 |
| `test_confidence_below_0_is_clamped` | -0.3 -> 0.0 |
| `test_confidence_exactly_0_is_valid` | 0.0 passes without clamping |
| `test_confidence_exactly_1_is_valid` | 1.0 passes without clamping |
| `test_confidence_oob_does_not_trigger_retry` | OOB confidence -> single LLM call (clamped, not retried) |

#### 4f. `TestTimeoutHandling` (2 tests)

| Test Method | What It Tests |
|-------------|---------------|
| `test_timeout_triggers_retry` | TimeoutError on attempt 1, valid on attempt 2 -> success |
| `test_two_timeouts_returns_fail_closed` | Two TimeoutErrors -> confidence=0.0 |

#### 4g. `TestConnectionErrorPropagation` (2 tests)

| Test Method | What It Tests |
|-------------|---------------|
| `test_connection_error_propagates_to_caller` | `ConnectionError` is re-raised, not caught internally |
| `test_connection_error_not_retried` | Only 1 LLM call made before propagation |

This is the key behavioral difference from the Excel classifier. The router catches this and degrades to Tier 1.

#### 4h. `TestFailClosed` (4 tests)

| Test Method | What It Tests |
|-------------|---------------|
| `test_fail_closed_result_has_zero_confidence` | confidence=0.0 after retries exhausted |
| `test_fail_closed_result_has_correct_tier_used` | tier_used matches the requested tier |
| `test_fail_closed_reasoning_mentions_failure` | "fail" appears in reasoning text |
| `test_fail_closed_per_page_types_is_empty_dict` | `per_page_types == {}` (not None) |

#### 4i. `TestTierModelSelection` (6 tests)

| Test Method | What It Tests |
|-------------|---------------|
| `test_tier2_uses_classification_model` | `model == "qwen2.5:7b"` |
| `test_tier3_uses_reasoning_model` | `model == "deepseek-r1:14b"` |
| `test_rule_based_tier_raises_value_error` | `ValueError` with "rule_based" in message |
| `test_custom_model_names_respected` | Custom config model name passed to backend |
| `test_temperature_from_config` | Config temperature passed to backend |
| `test_timeout_from_config` | Config timeout passed to backend |

#### 4j. `TestStructuralSummary` (12 tests)

| Test Method | What It Tests |
|-------------|---------------|
| `test_summary_contains_page_count` | "Pages: N" in summary |
| `test_summary_contains_file_size` | file size bytes value in summary |
| `test_summary_contains_creator` | metadata.creator in summary |
| `test_summary_contains_pdf_version` | metadata.pdf_version in summary |
| `test_summary_contains_page_type_distribution` | Distribution counts present |
| `test_summary_contains_sample_page_profiles` | Per-page word count, image count, etc. |
| `test_summary_contains_detected_languages` | Language list present |
| `test_summary_contains_toc_info_when_present` | TOC presence noted; entry count if available |
| `test_summary_contains_form_fields_info` | "Form fields: present" when `has_form_fields=True` |
| `test_summary_contains_no_raw_text` | No raw page text content ever appears |
| `test_summary_contains_filename_not_full_path` | "test.pdf" present, "/tmp/test.pdf" absent |
| `test_summary_font_names_included` | Font names from PageProfile appear in summary |

#### 4k. `TestPromptContent` (5 tests)

| Test Method | What It Tests |
|-------------|---------------|
| `test_prompt_contains_text_native_enum_value` | `'"text_native"'` in prompt |
| `test_prompt_contains_scanned_enum_value` | `'"scanned"'` in prompt |
| `test_prompt_contains_complex_enum_value` | `'"complex"'` in prompt |
| `test_prompt_contains_structural_summary` | Filename and page info in prompt |
| `test_prompt_requests_json_only` | "Respond with JSON only" in prompt |

#### 4l. `TestPageTypesValidation` (4 tests)

| Test Method | What It Tests |
|-------------|---------------|
| `test_valid_page_types_converted_to_dict` | `[{page:1, type:"text"}, {page:5, type:"scanned"}]` -> `{1: PageType.TEXT, 5: PageType.SCANNED}` |
| `test_unknown_page_numbers_logged_as_warning` | Page number not in profile -> warning logged (not rejected) |
| `test_no_page_types_returns_empty_dict` | `page_types=None` in response -> `per_page_types={}` |
| `test_all_eight_page_types_accepted` | Each `PageType` value round-trips correctly |

#### 4m. `TestLLMClassificationResponseModel` (5 tests)

| Test Method | What It Tests |
|-------------|---------------|
| `test_valid_model_creation` | Basic model instantiation |
| `test_invalid_type_rejected` | `type="invalid"` raises `ValidationError` |
| `test_empty_reasoning_rejected` | `reasoning=""` raises `ValidationError` |
| `test_page_types_with_invalid_page_type_rejected` | `type="invalid_page"` in entry raises `ValidationError` |
| `test_confidence_not_bounded_by_field` | `confidence=5.0` does NOT raise (no ge/le) |

#### 4n. `TestRedaction` (2 tests)

| Test Method | What It Tests |
|-------------|---------------|
| `test_redact_applies_patterns` | `redact_patterns=[r"\d{3}-\d{4}"]` replaces phone numbers with `[REDACTED]` |
| `test_redact_with_no_patterns_is_identity` | Empty patterns returns text unchanged |

**Total test count:** ~59 tests across 14 test classes.

---

## Acceptance Criteria

- [ ] `PDFLLMClassifier` class in `llm_classifier.py` with `classify()` public method
- [ ] `LLMClassificationResponse` Pydantic model validates LLM JSON output
- [ ] `PageTypeEntry` sub-model for strict page type validation
- [ ] Structural summary generation from `DocumentProfile` (never raw text)
- [ ] Classification prompt template with all three `PDFType` values and all eight `PageType` values
- [ ] Tier 2 uses `classification_model`, Tier 3 uses `reasoning_model`
- [ ] `RULE_BASED` tier raises `ValueError`
- [ ] JSON retry with correction hint (max 2 attempts)
- [ ] Schema validation retry with correction hint
- [ ] Confidence out-of-bounds clamped (not rejected, not retried)
- [ ] `ConnectionError` propagates to caller (not caught internally)
- [ ] `TimeoutError` retried internally, fail-closed after 2 attempts
- [ ] Fail-closed result: `confidence=0.0`, `degraded=False`, `per_page_types={}`
- [ ] `page_types` list from LLM correctly converted to `dict[int, PageType]`
- [ ] PII-safe: no raw text in summary, `_redact()` applied when `log_llm_prompts=True`
- [ ] Logger name is `ingestkit_pdf`
- [ ] All tests pass with `pytest -m unit`
- [ ] `PDFLLMClassifier` exported from `__init__.py`

## Verification Gates (PROVE)

```bash
# All unit tests pass
pytest packages/ingestkit-pdf/tests/test_llm_classifier.py -v -m unit

# No regressions in existing tests
pytest packages/ingestkit-pdf/tests/ -v

# Import works
python -c "from ingestkit_pdf.llm_classifier import PDFLLMClassifier, LLMClassificationResponse"

# No raw text in structural summary (visual inspection of test output)
pytest packages/ingestkit-pdf/tests/test_llm_classifier.py::TestStructuralSummary -v
```

AGENT_RETURN: .agents/outputs/plan-29-021426.md
