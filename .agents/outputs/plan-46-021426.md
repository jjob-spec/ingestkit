---
issue: 46
title: "Implement ExecutionBackend protocol and distributed backend"
agent: PLAN
date: 2026-02-14
status: complete
complexity: COMPLEX
stack: backend
package: ingestkit-pdf
spec_section: "18.3"
files_to_create:
  - packages/ingestkit-pdf/src/ingestkit_pdf/execution.py
  - packages/ingestkit-pdf/tests/test_execution.py
files_to_modify:
  - packages/ingestkit-pdf/src/ingestkit_pdf/router.py
  - packages/ingestkit-pdf/src/ingestkit_pdf/config.py
  - packages/ingestkit-pdf/src/ingestkit_pdf/errors.py
  - packages/ingestkit-pdf/src/ingestkit_pdf/protocols.py
  - packages/ingestkit-pdf/src/ingestkit_pdf/__init__.py
  - packages/ingestkit-pdf/pyproject.toml
---

# PLAN: Issue #46 -- ExecutionBackend Protocol and Distributed Backend

## Executive Summary

Implement the `ExecutionBackend` Protocol (SPEC section 18.3) for the **ingestkit-pdf** package. This introduces a pluggable execution abstraction with two concrete backends: `LocalExecutionBackend` (wraps existing `ProcessPoolExecutor`-based `process_batch()` logic) and `DistributedExecutionBackend` (stub/protocol-only for queue submission). The router's `process_batch()` is refactored to delegate to the injected execution backend, with `LocalExecutionBackend` as the default. Rollback to local is a config-only change per SPEC section 24.3.

## Critical Corrections from MAP

The MAP artifact investigated the **Excel** package but this issue targets the **PDF** package. Key differences:
- PDF `router.py` already has `ProcessPoolExecutor` in `process_batch()` (lines 454-492) -- not sequential like Excel.
- SPEC section 18.3 **does exist** and defines the exact Protocol signature.
- The `_process_single_file` module-level function (lines 876-886) handles per-worker router recreation.
- PDF config is `PDFProcessorConfig`, not `ExcelProcessorConfig`.

---

## File 1: `execution.py` (CREATE)

**Full path:** `packages/ingestkit-pdf/src/ingestkit_pdf/execution.py`
**Estimated lines:** ~200

### 1.1 Module Docstring and Imports

```python
"""Pluggable execution backends for the ingestkit-pdf pipeline (SPEC section 18.3).

Provides:
- ExecutionBackend: Protocol defining the submit/get_result interface.
- LocalExecutionBackend: Default backend using ProcessPoolExecutor.
- DistributedExecutionBackend: Stub for queue-based distributed processing.
"""
from __future__ import annotations

import logging
import uuid
from concurrent.futures import ProcessPoolExecutor, as_completed
from typing import TYPE_CHECKING, Callable, Protocol, runtime_checkable

if TYPE_CHECKING:
    from ingestkit_pdf.config import PDFProcessorConfig
    from ingestkit_pdf.models import ProcessingResult

logger = logging.getLogger("ingestkit_pdf")
```

### 1.2 ExecutionBackend Protocol

Matches SPEC section 18.3 exactly:

```python
@runtime_checkable
class ExecutionBackend(Protocol):
    """Pluggable execution backend for document processing."""

    def submit(self, file_path: str, config: PDFProcessorConfig) -> str:
        """Submit a document for processing. Returns a job_id."""
        ...

    def get_result(self, job_id: str, timeout: float | None = None) -> ProcessingResult:
        """Block until result is available or timeout."""
        ...
```

### 1.3 LocalExecutionBackend

Wraps the existing `ProcessPoolExecutor` logic currently in `PDFRouter.process_batch()`. Uses a callable injection pattern to avoid circular dependency with the router.

```python
class LocalExecutionBackend:
    """v1.0 default: process via ProcessPoolExecutor with per-document isolation.

    Each submit() dispatches to a worker process that creates fresh backends
    and a new router (matching the existing process_batch behavior).
    """

    def __init__(
        self,
        process_fn: Callable[[str, dict], ProcessingResult],
        max_workers: int = 4,
    ) -> None:
        self._process_fn = process_fn
        self._max_workers = max_workers
        self._results: dict[str, ProcessingResult] = {}
        self._pending: dict[str, tuple[str, dict]] = {}
```

**`submit()` method:**
- Generates a UUID job_id.
- Stores `(file_path, config_dict)` in `self._pending`.
- Returns job_id immediately (lazy evaluation -- actual execution happens in `execute_all()`).
- Rationale for lazy: The current `process_batch()` submits all futures at once to the ProcessPoolExecutor. To preserve this batching behavior, `submit()` just queues; `execute_all()` runs the pool.

**`get_result()` method:**
- If job_id is in `self._results`, return it.
- If job_id is in `self._pending` but not yet executed, raise `IngestError` with `E_EXECUTION_NOT_FOUND`.
- If job_id is not found at all, raise `IngestError` with `E_EXECUTION_NOT_FOUND`.

**`execute_all()` method** (non-Protocol, local-backend-specific):
- Spawns `ProcessPoolExecutor(max_workers=min(len(self._pending), self._max_workers))`.
- Submits all pending jobs using `self._process_fn`.
- Collects results with timeout handling (per-job timeout from config or explicit timeout param).
- Stores results in `self._results`, clears `self._pending`.
- On timeout: stores an error `ProcessingResult` with `E_EXECUTION_TIMEOUT`.
- On exception: stores an error `ProcessingResult`.

**Alternative design (simpler, also acceptable):** Make `submit()` synchronous -- call `self._process_fn` immediately and store result. Then `get_result()` is a simple dict lookup. This is simpler but loses the ProcessPoolExecutor parallelism. The plan uses the lazy/batched approach to match existing behavior.

### 1.4 DistributedExecutionBackend (Stub)

Per the task instructions, this is a **stub/protocol-only** implementation. It satisfies the Protocol but raises `NotImplementedError` at runtime.

```python
class DistributedExecutionBackend:
    """v1.1+: submit to a queue (Redis, RabbitMQ, etc.), workers process.

    This is a stub implementation. The full distributed backend requires
    queue infrastructure (Redis, RabbitMQ) and a worker process that
    dequeues jobs and calls router.process(). See SPEC section 18.3 and
    the Phase 2 roadmap (section 24.3) for details.
    """

    def __init__(self, queue_url: str) -> None:
        self._queue_url = queue_url

    def submit(self, file_path: str, config: PDFProcessorConfig) -> str:
        """Submit a document for distributed processing.

        Raises NotImplementedError -- distributed backend is not yet implemented.
        """
        raise NotImplementedError(
            "DistributedExecutionBackend is a planned feature (SPEC section 18.3, "
            "Phase 2). Requires queue infrastructure. Use LocalExecutionBackend."
        )

    def get_result(self, job_id: str, timeout: float | None = None) -> ProcessingResult:
        """Retrieve result from distributed processing.

        Raises NotImplementedError -- distributed backend is not yet implemented.
        """
        raise NotImplementedError(
            "DistributedExecutionBackend is a planned feature (SPEC section 18.3, "
            "Phase 2). Requires queue infrastructure. Use LocalExecutionBackend."
        )
```

---

## File 2: `config.py` (MODIFY)

**Full path:** `packages/ingestkit-pdf/src/ingestkit_pdf/config.py`
**Changes:** Add 3 fields to `PDFProcessorConfig`

Add after the `backend_backoff_base` field (line 114), before the logging section:

```python
    # --- Execution Backend ---
    execution_backend: str = "local"                # "local" or "distributed"
    execution_max_workers: int = 4                  # max ProcessPoolExecutor workers
    execution_queue_url: str | None = None          # Redis/RabbitMQ URL for distributed
```

**Notes:**
- `execution_backend` defaults to `"local"` so existing code works without changes.
- `execution_max_workers` replaces the hardcoded `min(len(file_paths), 4)` in current `process_batch()`.
- `execution_queue_url` is only required when `execution_backend == "distributed"`.
- No model validator needed: if `execution_backend == "distributed"` and `execution_queue_url is None`, the `DistributedExecutionBackend.__init__()` will raise.

---

## File 3: `errors.py` (MODIFY)

**Full path:** `packages/ingestkit-pdf/src/ingestkit_pdf/errors.py`
**Changes:** Add 3 error codes to `ErrorCode` enum

Add after the existing processing errors section (after `E_PROCESS_HEADER_FOOTER`, line 56):

```python
    # Execution backend errors
    E_EXECUTION_TIMEOUT = "E_EXECUTION_TIMEOUT"
    E_EXECUTION_SUBMIT = "E_EXECUTION_SUBMIT"
    E_EXECUTION_NOT_FOUND = "E_EXECUTION_NOT_FOUND"
```

---

## File 4: `protocols.py` (MODIFY)

**Full path:** `packages/ingestkit-pdf/src/ingestkit_pdf/protocols.py`
**Changes:** Re-export `ExecutionBackend` from `execution.py`

```python
from ingestkit_pdf.execution import ExecutionBackend

__all__ = [
    "VectorStoreBackend",
    "StructuredDBBackend",
    "LLMBackend",
    "EmbeddingBackend",
    "ExecutionBackend",
]
```

**Note:** `ExecutionBackend` is defined in `ingestkit_pdf.execution`, not in `ingestkit_core.protocols`, because this is a PDF-specific orchestration concern. It is re-exported from `protocols.py` for consistency with the existing import pattern.

---

## File 5: `router.py` (MODIFY)

**Full path:** `packages/ingestkit-pdf/src/ingestkit_pdf/router.py`
**Changes:** ~40 lines modified

### 5.1 Add Import

At top of file, add:

```python
from ingestkit_pdf.execution import (
    ExecutionBackend,
    LocalExecutionBackend,
)
```

### 5.2 Modify `PDFRouter.__init__()`

Add optional `execution` parameter after `config`:

```python
    def __init__(
        self,
        vector_store: VectorStoreBackend,
        structured_db: StructuredDBBackend,
        llm: LLMBackend,
        embedder: EmbeddingBackend,
        config: PDFProcessorConfig | None = None,
        execution: ExecutionBackend | None = None,
    ) -> None:
```

At the end of `__init__`, after storing backends (after line 137), add:

```python
        # Execution backend
        if execution is not None:
            self._execution = execution
        else:
            self._execution = LocalExecutionBackend(
                process_fn=_process_single_file,
                max_workers=self._config.execution_max_workers,
            )
```

### 5.3 Refactor `process_batch()`

Replace the current `process_batch()` implementation (lines 428-492) to delegate to the execution backend:

```python
    def process_batch(
        self,
        file_paths: list[str],
    ) -> list[ProcessingResult]:
        """Process multiple PDFs via the configured execution backend.

        Uses the injected ``ExecutionBackend`` to submit and collect results.
        With ``LocalExecutionBackend`` (default), this preserves the existing
        ProcessPoolExecutor process-isolation behavior.

        Parameters
        ----------
        file_paths:
            List of filesystem paths to PDF files.

        Returns
        -------
        list[ProcessingResult]
            One result per input file, in the same order.
        """
        if not file_paths:
            return []

        config_dict = self._config.model_dump()
        timeout = self._config.per_document_timeout_seconds

        # Submit all files
        job_ids: list[str] = []
        for fp in file_paths:
            job_id = self._execution.submit(fp, self._config)
            job_ids.append(job_id)

        # For LocalExecutionBackend, trigger batch execution
        if hasattr(self._execution, "execute_all"):
            self._execution.execute_all(timeout=float(timeout))

        # Collect results in order
        results: list[ProcessingResult] = []
        for job_id in job_ids:
            result = self._execution.get_result(job_id, timeout=float(timeout))
            results.append(result)

        return results
```

### 5.4 Keep `_process_single_file` and `_build_error_result`

The module-level `_process_single_file()` function (lines 876-886) remains unchanged. It is passed as `process_fn` to `LocalExecutionBackend`.

The `_build_error_result` static/class method remains for constructing error results on timeout/failure within the execution backend.

### 5.5 Update `create_default_router()`

Add `execution` to the router kwargs set and pass it through:

```python
    router_keys = {"vector_store", "structured_db", "llm", "embedder", "config", "execution"}
```

---

## File 6: `__init__.py` (MODIFY)

**Full path:** `packages/ingestkit-pdf/src/ingestkit_pdf/__init__.py`
**Changes:** Add 3 exports

Add import:

```python
from ingestkit_pdf.execution import (
    ExecutionBackend,
    LocalExecutionBackend,
    DistributedExecutionBackend,
)
```

Add to `__all__`:

```python
    # Execution backends
    "ExecutionBackend",
    "LocalExecutionBackend",
    "DistributedExecutionBackend",
```

---

## File 7: `pyproject.toml` (MODIFY)

**Full path:** `packages/ingestkit-pdf/pyproject.toml`
**Changes:** Add optional dependency group for distributed backend

Add after the `ollama` optional dependency (line 41):

```toml
# Distributed execution backends (optional, Phase 2)
distributed = ["redis>=4.0"]
```

Update the `full` bundle to include it (optional -- only if we want full to include distributed deps).

---

## File 8: `test_execution.py` (CREATE)

**Full path:** `packages/ingestkit-pdf/tests/test_execution.py`
**Estimated lines:** ~280

### 8.1 Test Structure

```python
"""Tests for ExecutionBackend protocol, LocalExecutionBackend, and DistributedExecutionBackend.

Covers: Protocol conformance, local backend submit/execute/get_result cycle,
distributed backend stub behavior, router integration with execution backend,
error handling (timeout, not found), and config-driven backend selection.
"""
```

### 8.2 Test Cases

**Protocol conformance tests:**

| Test | Description |
|------|-------------|
| `test_local_backend_satisfies_protocol` | `isinstance(LocalExecutionBackend(...), ExecutionBackend)` is True |
| `test_distributed_backend_satisfies_protocol` | `isinstance(DistributedExecutionBackend(...), ExecutionBackend)` is True |
| `test_mock_backend_satisfies_protocol` | Custom mock satisfies `ExecutionBackend` |

**LocalExecutionBackend tests:**

| Test | Description |
|------|-------------|
| `test_submit_returns_job_id` | `submit()` returns a non-empty string |
| `test_submit_multiple_returns_unique_ids` | Multiple `submit()` calls return distinct job_ids |
| `test_execute_all_processes_pending` | After `submit()` + `execute_all()`, results are available |
| `test_get_result_returns_processing_result` | `get_result()` returns a `ProcessingResult` |
| `test_get_result_unknown_job_raises` | `get_result()` with unknown job_id raises `IngestError` with `E_EXECUTION_NOT_FOUND` |
| `test_get_result_before_execute_raises` | `get_result()` before `execute_all()` raises |
| `test_execute_all_timeout_produces_error_result` | Worker that exceeds timeout produces error result with `E_EXECUTION_TIMEOUT` |
| `test_execute_all_exception_produces_error_result` | Worker that raises produces error result |
| `test_max_workers_respected` | ProcessPoolExecutor created with correct max_workers |

**DistributedExecutionBackend tests:**

| Test | Description |
|------|-------------|
| `test_distributed_submit_raises_not_implemented` | `submit()` raises `NotImplementedError` |
| `test_distributed_get_result_raises_not_implemented` | `get_result()` raises `NotImplementedError` |
| `test_distributed_init_stores_queue_url` | Constructor stores the queue_url |

**Router integration tests:**

| Test | Description |
|------|-------------|
| `test_router_uses_injected_execution_backend` | Router with mock execution backend delegates to it |
| `test_router_defaults_to_local_backend` | Router without `execution` param uses `LocalExecutionBackend` |
| `test_process_batch_empty_returns_empty` | `process_batch([])` returns `[]` |
| `test_process_batch_delegates_to_execution` | `process_batch()` calls `submit()` + `get_result()` on the backend |

**Config tests:**

| Test | Description |
|------|-------------|
| `test_config_defaults` | `execution_backend == "local"`, `execution_max_workers == 4`, `execution_queue_url is None` |
| `test_config_distributed` | Setting `execution_backend="distributed"` and `execution_queue_url="redis://..."` works |

### 8.3 Mock Execution Backend

```python
class MockExecutionBackend:
    """Mock that satisfies the ExecutionBackend protocol."""

    def __init__(self, results: dict[str, ProcessingResult] | None = None) -> None:
        self._results = results or {}
        self._counter = 0
        self.submitted: list[tuple[str, Any]] = []

    def submit(self, file_path: str, config: PDFProcessorConfig) -> str:
        self._counter += 1
        job_id = f"mock-{self._counter}"
        self.submitted.append((file_path, config))
        return job_id

    def get_result(self, job_id: str, timeout: float | None = None) -> ProcessingResult:
        if job_id in self._results:
            return self._results[job_id]
        # Return a minimal ProcessingResult for testing
        return self._default_result()
```

### 8.4 Test Markers

All tests marked `@pytest.mark.unit`. No external services required. Tests use mock `process_fn` callable (not actual router) for `LocalExecutionBackend` tests to avoid PyMuPDF/PDF dependencies.

### 8.5 Fixtures

```python
@pytest.fixture
def mock_process_fn():
    """A simple callable that returns a minimal ProcessingResult."""
    def _fn(file_path: str, config_dict: dict) -> ProcessingResult:
        return _make_minimal_result(file_path)
    return _fn

@pytest.fixture
def local_backend(mock_process_fn):
    return LocalExecutionBackend(process_fn=mock_process_fn, max_workers=2)
```

---

## Design Decisions

### D1: Lazy submit + execute_all pattern

The `LocalExecutionBackend.submit()` queues work lazily; `execute_all()` runs the ProcessPoolExecutor batch. This preserves the current `process_batch()` behavior where all futures are submitted to the pool at once. The router calls `execute_all()` via duck-typing (`hasattr` check). The `DistributedExecutionBackend` does not need `execute_all()` because distributed submit is inherently async.

### D2: process_fn callable injection

`LocalExecutionBackend` receives `process_fn: Callable[[str, dict], ProcessingResult]` rather than a reference to `PDFRouter`. This avoids circular dependency (router creates backend, backend needs router). The existing `_process_single_file` module-level function is the process_fn.

### D3: ExecutionBackend lives in ingestkit_pdf, not ingestkit_core

This is a PDF-specific orchestration concern. The Excel package may have different execution semantics (no process isolation needed for openpyxl). If it proves reusable, extract to core later.

### D4: DistributedExecutionBackend as stub only

Per task instructions and SPEC section 24.3 (Phase 2 scope), the distributed backend is a stub. It satisfies the Protocol interface but raises `NotImplementedError`. This is sufficient to validate the abstraction layer and prove that rollback to local is a config change.

### D5: Backward compatibility

The `execution` parameter is optional in `PDFRouter.__init__()` with `None` default. Existing code that creates `PDFRouter(vector_store=..., structured_db=..., llm=..., embedder=...)` continues to work unchanged -- it gets `LocalExecutionBackend` automatically.

---

## Acceptance Criteria

- [ ] `ExecutionBackend` Protocol defined in `execution.py` with `submit()` and `get_result()` matching SPEC section 18.3 signatures
- [ ] `LocalExecutionBackend` wraps ProcessPoolExecutor logic from current `process_batch()`
- [ ] `DistributedExecutionBackend` is a stub that raises `NotImplementedError`
- [ ] Both backends satisfy `isinstance(..., ExecutionBackend)` at runtime
- [ ] `PDFProcessorConfig` has `execution_backend`, `execution_max_workers`, `execution_queue_url` fields
- [ ] `PDFRouter.__init__()` accepts optional `execution` parameter
- [ ] `PDFRouter.process_batch()` delegates to execution backend
- [ ] Default behavior (no `execution` param) matches current `process_batch()` behavior
- [ ] Rollback to local is config-only: `execution_backend="local"` works without code changes
- [ ] 3 new error codes: `E_EXECUTION_TIMEOUT`, `E_EXECUTION_SUBMIT`, `E_EXECUTION_NOT_FOUND`
- [ ] `ExecutionBackend` re-exported from `protocols.py`
- [ ] All 3 classes exported from `__init__.py`
- [ ] Unit tests pass with `pytest packages/ingestkit-pdf/tests/test_execution.py -m unit`
- [ ] No regressions in existing `test_router.py` tests
- [ ] All tests are `@pytest.mark.unit` -- no external services required

## Verification Gates for PROVE

```bash
# Unit tests for execution backends
pytest packages/ingestkit-pdf/tests/test_execution.py -v -m unit

# Regression check for router tests
pytest packages/ingestkit-pdf/tests/test_router.py -v

# Full test suite
pytest packages/ingestkit-pdf/tests/ -v --tb=short

# Import verification
python -c "from ingestkit_pdf import ExecutionBackend, LocalExecutionBackend, DistributedExecutionBackend; print('OK')"
python -c "from ingestkit_pdf.protocols import ExecutionBackend; print('OK')"
python -c "from ingestkit_pdf.config import PDFProcessorConfig; c = PDFProcessorConfig(); assert c.execution_backend == 'local'; print('OK')"
```

---

AGENT_RETURN: .agents/outputs/plan-46-021426.md
