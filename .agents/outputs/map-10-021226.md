# MAP -- Issue #10: Path C Hybrid Splitter

## SPEC SS10.3 Requirements

**Source:** `packages/ingestkit-excel/SPEC.md` lines 785-809

**Input:** FileProfile + classification confirming Type C, with per-sheet or per-region type info.

**Steps (verbatim from spec):**
1. For each sheet, detect distinct regions using multiple heuristics:
   - **Blank separators:** A run of >= 2 blank rows or columns marks a boundary.
   - **Merged cell blocks:** Large merged regions indicate header/title blocks.
   - **Formatting transitions:** Shift from numeric-heavy to text-heavy rows suggests a region boundary.
   - **Header/footer detection:** Repeated merged rows at top/bottom of sheets identified as `HEADER_BLOCK` / `FOOTER_BLOCK`.
   - **Matrix detection:** Regions with both row and column headers identified as `MATRIX_BLOCK`.
2. Each region gets a `SheetRegion` with bounding coordinates and `detection_confidence` (0.0-1.0).
3. Classify each region as Type A or Type B (using Tier 1 signals on the region's data).
4. Route each region to `StructuredDBProcessor` or `TextSerializer`.
5. All chunks/tables share the same `ingest_key` and link via `region_id` in metadata.

**Public interface (from spec):**
```python
class HybridSplitter:
    def __init__(self, structured_processor: StructuredDBProcessor,
                 text_serializer: TextSerializer, config: ExcelProcessorConfig): ...
    def process(self, file_path: str, profile: FileProfile,
                classification: ClassificationResult,
                ingest_key: str, ingest_run_id: str) -> ProcessingResult: ...
```

**Router integration (SS13.1 step 8):**
- `HYBRID` -> `HybridSplitter.process()`
- Collects `WrittenArtifacts` from processor
- Assembles `ProcessingResult` with all stage artifacts

---

## Existing Code Analysis

### Path A (StructuredDBProcessor)

**File:** `/home/jjob/projects/ingestkit/packages/ingestkit-excel/src/ingestkit_excel/processors/structured_db.py`

**Constructor:**
```python
def __init__(
    self,
    structured_db: StructuredDBBackend,
    vector_store: VectorStoreBackend,
    embedder: EmbeddingBackend,
    config: ExcelProcessorConfig,
) -> None:
```

**process() signature:**
```python
def process(
    self,
    file_path: str,
    profile: FileProfile,
    ingest_key: str,
    ingest_run_id: str,
    parse_result: ParseStageResult,
    classification_result: ClassificationStageResult,
    classification: ClassificationResult,
) -> ProcessingResult:
```

**ProcessingResult construction (lines 323-341):**
```python
return ProcessingResult(
    file_path=file_path,
    ingest_key=ingest_key,
    ingest_run_id=ingest_run_id,
    tenant_id=config.tenant_id,
    parse_result=parse_result,
    classification_result=classification_result,
    embed_result=embed_result,
    classification=classification,
    ingestion_method=IngestionMethod.SQL_AGENT,  # enum member, NOT string
    chunks_created=total_chunks,
    tables_created=len(tables),
    tables=tables,
    written=written,
    errors=errors,
    warnings=warnings,
    error_details=error_details,
    processing_time_seconds=elapsed,
)
```

**WrittenArtifacts handling:**
- Initialized at start: `written = WrittenArtifacts(vector_collection=collection)`
- DB tables appended: `written.db_table_names.append(table_name)`
- Vector point IDs appended: `written.vector_point_ids.append(chunk_id)`

**Key patterns:**
- Sheet skip logic: hidden, chart-only (row_count==0 and col_count==0), oversized (> max_rows_in_memory)
- Per-sheet error handling with try/except, continues to next sheet
- `_classify_backend_error()` maps exceptions to ErrorCode
- Chunk IDs are deterministic: `uuid5(NAMESPACE_URL, f"{ingest_key}:{chunk_hash}")`
- ChunkMetadata.region_id is always `None` in Path A
- ChunkMetadata.ingestion_method is a **string** value (e.g., `IngestionMethod.SQL_AGENT.value` = `"sql_agent"`)
- ProcessingResult.ingestion_method is an **enum member** (e.g., `IngestionMethod.SQL_AGENT`)

### Path B (TextSerializer)

**File:** `/home/jjob/projects/ingestkit/packages/ingestkit-excel/src/ingestkit_excel/processors/serializer.py`

**Constructor:**
```python
def __init__(
    self,
    vector_store: VectorStoreBackend,
    embedder: EmbeddingBackend,
    config: ExcelProcessorConfig,
) -> None:
```

**process() signature (IDENTICAL to Path A):**
```python
def process(
    self,
    file_path: str,
    profile: FileProfile,
    ingest_key: str,
    ingest_run_id: str,
    parse_result: ParseStageResult,
    classification_result: ClassificationStageResult,
    classification: ClassificationResult,
) -> ProcessingResult:
```

**ProcessingResult construction (lines 255-273):**
```python
return ProcessingResult(
    file_path=file_path,
    ingest_key=ingest_key,
    ingest_run_id=ingest_run_id,
    tenant_id=config.tenant_id,
    parse_result=parse_result,
    classification_result=classification_result,
    embed_result=embed_result,
    classification=classification,
    ingestion_method=IngestionMethod.TEXT_SERIALIZATION,
    chunks_created=total_chunks,
    tables_created=0,
    tables=[],
    written=written,
    errors=errors,
    warnings=warnings,
    error_details=error_details,
    processing_time_seconds=elapsed,
)
```

**WrittenArtifacts handling:**
- Initialized at start: `written = WrittenArtifacts(vector_collection=collection)`
- No DB tables (Path B is vector-only)
- Vector point IDs appended per chunk batch

**Key differences from Path A:**
- No StructuredDBBackend dependency
- Uses openpyxl.load_workbook directly for section detection
- `_detect_sections()` splits worksheet into Section objects
- `tables_created=0`, `tables=[]` always
- ChunkMetadata has `section_title` and `original_structure` fields set
- ChunkMetadata has `table_name=None`, `db_uri=None`, `row_count=None`, `columns=None`

---

### SheetRegion Model

**File:** `/home/jjob/projects/ingestkit/packages/ingestkit-excel/src/ingestkit_excel/models.py` (lines 176-188)

```python
class SheetRegion(BaseModel):
    """A detected region within a worksheet (used by Path C splitter)."""
    sheet_name: str
    region_id: str
    start_row: int
    end_row: int
    start_col: int
    end_col: int
    region_type: RegionType
    detection_confidence: float       # 0.0-1.0
    classified_as: FileType | None = None
```

### RegionType Enum

**File:** `/home/jjob/projects/ingestkit/packages/ingestkit-excel/src/ingestkit_excel/models.py` (lines 59-68)

```python
class RegionType(str, Enum):
    DATA_TABLE = "data_table"
    TEXT_BLOCK = "text_block"
    HEADER_BLOCK = "header_block"
    FOOTER_BLOCK = "footer_block"
    MATRIX_BLOCK = "matrix_block"
    CHART_ONLY = "chart_only"
    EMPTY = "empty"
```

### ChunkMetadata (Excel-specific)

**File:** `/home/jjob/projects/ingestkit/packages/ingestkit-excel/src/ingestkit_excel/models.py` (lines 159-174)

```python
class ChunkMetadata(BaseChunkMetadata):
    source_format: str = "xlsx"
    sheet_name: str
    region_id: str | None = None        # <-- Key field for Path C
    parser_used: str
    ingest_run_id: str                   # required (base has Optional)
    db_uri: str | None = None
    original_structure: str | None = None
```

**BaseChunkMetadata fields (from ingestkit_core):**
```python
class BaseChunkMetadata(BaseModel):
    source_uri: str
    source_format: str
    ingestion_method: str
    parser_version: str
    chunk_index: int
    chunk_hash: str
    ingest_key: str
    ingest_run_id: str | None = None
    tenant_id: str | None = None
    table_name: str | None = None
    row_count: int | None = None
    columns: list[str] | None = None
    section_title: str | None = None
```

### Config (Path C related)

**File:** `/home/jjob/projects/ingestkit/packages/ingestkit-excel/src/ingestkit_excel/config.py`

**No Path C specific parameters exist in the config.** The splitter will use:
- `max_rows_in_memory` (100,000) -- for sheet skip logic
- `default_collection` ("helpdesk") -- for vector store
- `embedding_batch_size` (64) -- for batched embedding
- `backend_timeout_seconds` (30.0) -- for backend calls
- `tenant_id` -- propagated through metadata
- `parser_version` -- in ChunkMetadata
- `row_serialization_limit` (5000) -- used by Path A sub-processor
- `clean_column_names` (True) -- used by Path A sub-processor

The blank separator threshold (>= 2 blank rows/columns) is specified in the SPEC but has no config parameter -- it should be a constant or could be added to config.

### Protocols

**File:** `/home/jjob/projects/ingestkit/packages/ingestkit-excel/src/ingestkit_excel/protocols.py`

Re-exports from `ingestkit_core.protocols`:
- `VectorStoreBackend` -- upsert_chunks, ensure_collection, create_payload_index, delete_by_ids
- `StructuredDBBackend` -- create_table_from_dataframe, drop_table, table_exists, get_table_schema, get_connection_uri
- `LLMBackend` -- classify, generate
- `EmbeddingBackend` -- embed, dimension

HybridSplitter does NOT directly use backends. It delegates to Path A and Path B sub-processors. It only needs:
- `StructuredDBProcessor` (which internally uses StructuredDBBackend + VectorStoreBackend + EmbeddingBackend)
- `TextSerializer` (which internally uses VectorStoreBackend + EmbeddingBackend)

### Error Codes

**File:** `/home/jjob/projects/ingestkit/packages/ingestkit-excel/src/ingestkit_excel/errors.py`

Relevant error code for Path C:
- `E_PROCESS_REGION_DETECT = "E_PROCESS_REGION_DETECT"` -- specific to region detection failures

### processors/__init__.py Exports

**File:** `/home/jjob/projects/ingestkit/packages/ingestkit-excel/src/ingestkit_excel/processors/__init__.py`

Currently exports:
```python
from ingestkit_excel.processors.serializer import TextSerializer
from ingestkit_excel.processors.structured_db import StructuredDBProcessor
__all__ = ["StructuredDBProcessor", "TextSerializer"]
```

Will need to add `HybridSplitter` to this.

### Package __init__.py

**File:** `/home/jjob/projects/ingestkit/packages/ingestkit-excel/src/ingestkit_excel/__init__.py`

Currently imports `StructuredDBProcessor` and `TextSerializer` from processors.
Will need to add `HybridSplitter` to imports and `__all__`.

---

## Process Signature Reconciliation

### What the issue says:
```python
def process(self, file_path: str, profile: FileProfile,
            classification: ClassificationResult,
            ingest_key: str, ingest_run_id: str) -> ProcessingResult
```

### What the SPEC SS10.3 says:
```python
def process(self, file_path: str, profile: FileProfile,
            classification: ClassificationResult,
            ingest_key: str, ingest_run_id: str) -> ProcessingResult
```

### What Path A and Path B ACTUALLY use:
```python
def process(
    self,
    file_path: str,
    profile: FileProfile,
    ingest_key: str,
    ingest_run_id: str,
    parse_result: ParseStageResult,
    classification_result: ClassificationStageResult,
    classification: ClassificationResult,
) -> ProcessingResult
```

### CRITICAL DISCREPANCY:
The SPEC/issue signature has 5 parameters. Path A and Path B have 7 parameters (additionally: `parse_result`, `classification_result`).

**Resolution:** The HybridSplitter MUST match Path A and Path B's actual signature (7 parameters) because:
1. It needs `parse_result` and `classification_result` to construct the `ProcessingResult`
2. It delegates to Path A/B sub-processors, which require these parameters
3. The router (SS13.1) passes these parameters to all processors uniformly

**HybridSplitter.process() MUST have this signature:**
```python
def process(
    self,
    file_path: str,
    profile: FileProfile,
    ingest_key: str,
    ingest_run_id: str,
    parse_result: ParseStageResult,
    classification_result: ClassificationStageResult,
    classification: ClassificationResult,
) -> ProcessingResult:
```

**NOTE on sub-processor delegation:** When the HybridSplitter calls Path A or Path B on a region, it will NOT call their `process()` methods directly (those process full files/sheets). Instead, the splitter must handle region detection and classification itself, then either:
- Option A: Call the sub-processors' `process()` with a modified profile containing only the relevant sheet/region data. This is complex because process() iterates over sheets.
- Option B: Extract the region-level processing logic and handle embedding/DB writes directly, similar to how Path A and Path B do it internally. This gives more control over region_id metadata.
- Option C (recommended): Perform region detection, then for each region classified as Type A, extract a DataFrame and use Path A's internal helpers (_auto_detect_dates, _generate_schema_description, etc.) and for Type B regions, use Path B's serialization. But this requires reaching into private methods.

**Practical approach:** The HybridSplitter should handle region detection and classification itself, then construct sub-profiles/data for each region and call the sub-processors' process() with synthetic single-sheet profiles. The challenge is that sub-processors iterate `profile.sheets` -- we can create a synthetic FileProfile with a single SheetProfile per region. After getting ProcessingResults from sub-processors, merge their WrittenArtifacts.

---

## Key Patterns to Follow

1. **Logger:** `logger = logging.getLogger(__name__)` at module level
2. **Timing:** `start_time = time.monotonic()` ... `elapsed = time.monotonic() - start_time`
3. **WrittenArtifacts:** Initialize with collection, append IDs as they're created
4. **Error accumulation:** `errors: list[str]`, `warnings: list[str]`, `error_details: list[IngestError]`
5. **Sheet skip logic:** Hidden, chart-only, oversized -- identical in Path A and Path B
6. **Per-sheet error handling:** try/except per sheet, log exception, record error, continue
7. **Chunk ID generation:** `uuid5(NAMESPACE_URL, f"{ingest_key}:{chunk_hash}")`
8. **Source URI format:** `f"file://{Path(file_path).resolve().as_posix()}"`
9. **EmbedStageResult:** Only populated if `total_texts_embedded > 0`
10. **IngestionMethod:** Enum member on ProcessingResult, string value on ChunkMetadata
11. **region_id:** Must be set in ChunkMetadata for Path C chunks (currently `None` in Path A/B)

## Test Patterns to Follow

1. **Helper factories:** `_make_sheet_profile(**overrides)`, `_make_file_profile(sheets)`, `_make_parse_result()`, `_make_classification_stage_result()`, `_make_classification_result()`
2. **Mock backends:** Lightweight classes (not MagicMock) that implement the protocol interface and track calls
3. **pytest fixtures:** For mock backends, config, and processor instance
4. **Mocking:** Use `@patch` for openpyxl/pandas calls
5. **Test organization:** Grouped into test classes by feature area (detection, classification, flow, errors, etc.)
6. **Assertions:** Check both result structure and backend interaction (what was called)

---

## Dependencies & Risks

### Dependencies (all resolved):
- **Issue #3 (models):** `SheetRegion`, `RegionType`, `ChunkMetadata.region_id` -- all exist
- **Issue #8 (Path A):** `StructuredDBProcessor` -- exists with full process() implementation
- **Issue #9 (Path B):** `TextSerializer` -- exists with full process() implementation

### Design Risks:

1. **Sub-processor delegation complexity:** Path A/B process() methods iterate over `profile.sheets` and handle full files. The HybridSplitter needs to either:
   - Create synthetic single-sheet profiles per region and call process(), OR
   - Duplicate some internal logic from Path A/B for region-level processing.
   The first approach is cleaner but requires creating synthetic FileProfile/SheetProfile objects.

2. **Region-level DataFrame extraction:** For Type A regions, the splitter needs to extract a DataFrame for just the region's row/column range. This means using `pd.read_excel` with specific row ranges or slicing after loading.

3. **Region-level openpyxl access:** For Type B regions, the splitter needs to work with openpyxl worksheet data for just the region's area.

4. **WrittenArtifacts merging:** When delegating to sub-processors, the splitter must merge WrittenArtifacts from multiple ProcessingResults into one. Key concern: vector_collection should be consistent (all use config.default_collection).

5. **EmbedStageResult merging:** Multiple sub-processor results may have EmbedStageResults that need to be aggregated (sum texts_embedded, sum durations).

6. **Chunk index continuity:** The global chunk_index must be continuous across all regions. Sub-processors start from 0, so the splitter needs to either adjust indices or manage indexing itself.

7. **Process signature mismatch:** The SPEC says 5 params, actual Path A/B use 7. Must use 7-param signature for router compatibility.

### Implementation Recommendation:

The cleanest approach is for HybridSplitter to:
1. Perform region detection (own logic)
2. Classify each region using Tier 1 signals (adapted from ExcelInspector)
3. For each region, create a synthetic single-sheet FileProfile
4. Call Path A's `process()` or Path B's `process()` with the synthetic profile
5. Merge all ProcessingResults into one final result

The alternative (handling embedding/DB writes directly) would duplicate too much code from Path A/B.

For chunk_index continuity, the splitter should track a global counter and adjust metadata.chunk_index values from sub-processor results after the fact, OR accept that chunk indices restart per region (simpler, less coupling).

---

AGENT_RETURN: map-10-021226.md
