# MAP Artifact: Issue #7 -- Tier 2/3 LLM Classifier with Schema Validation

**Issue:** #7
**Date:** 2026-02-10
**Target file:** `src/ingestkit_excel/llm_classifier.py`
**Spec reference:** SPEC.md section 9 (Tier 2 & 3 -- LLM Classifier)

---

## 1. Enum Values Reference

All enum values are the **string values** (not Python names). These are what the LLM must return and what code must compare against.

### FileType (models.py)
| Python Name | String Value |
|---|---|
| `TABULAR_DATA` | `"tabular_data"` |
| `FORMATTED_DOCUMENT` | `"formatted_document"` |
| `HYBRID` | `"hybrid"` |

### ClassificationTier (models.py)
| Python Name | String Value |
|---|---|
| `RULE_BASED` | `"rule_based"` |
| `LLM_BASIC` | `"llm_basic"` |
| `LLM_REASONING` | `"llm_reasoning"` |

### ErrorCode -- LLM-Related Only (errors.py)
| Python Name | String Value | When Used |
|---|---|---|
| `E_CLASSIFY_INCONCLUSIVE` | `"E_CLASSIFY_INCONCLUSIVE"` | All tiers failed to classify |
| `E_LLM_TIMEOUT` | `"E_LLM_TIMEOUT"` | LLM backend timed out |
| `E_LLM_MALFORMED_JSON` | `"E_LLM_MALFORMED_JSON"` | LLM returned unparseable JSON |
| `E_LLM_SCHEMA_INVALID` | `"E_LLM_SCHEMA_INVALID"` | LLM JSON failed Pydantic validation |
| `E_LLM_CONFIDENCE_OOB` | `"E_LLM_CONFIDENCE_OOB"` | Confidence outside 0.0-1.0 range |
| `W_LLM_RETRY` | `"W_LLM_RETRY"` | LLM call was retried (warning, non-fatal) |

**ENUM_VALUE RISK:** The LLM response must contain `"tabular_data"`, `"formatted_document"`, or `"hybrid"` -- NOT `"TABULAR_DATA"` or `"TabularData"`. Pydantic validation will catch this but we must set up the Literal type correctly.

---

## 2. Model Field Reference

### ClassificationResult (models.py, line 184)
```python
class ClassificationResult(BaseModel):
    file_type: FileType                              # e.g. FileType.TABULAR_DATA
    confidence: float                                # 0.0-1.0
    tier_used: ClassificationTier                    # e.g. ClassificationTier.LLM_BASIC
    reasoning: str                                   # human-readable explanation
    per_sheet_types: dict[str, FileType] | None = None  # only for hybrid
    signals: dict[str, Any] | None = None            # Tier 1 signal breakdown (None for LLM tiers)
```

### FileProfile (models.py, line 169)
```python
class FileProfile(BaseModel):
    file_path: str
    file_size_bytes: int
    sheet_count: int
    sheet_names: list[str]
    sheets: list[SheetProfile]
    has_password_protected_sheets: bool
    has_chart_only_sheets: bool
    total_merged_cells: int
    total_rows: int
    content_hash: str
```

### SheetProfile (models.py, line 148)
```python
class SheetProfile(BaseModel):
    name: str
    row_count: int
    col_count: int
    merged_cell_count: int
    merged_cell_ratio: float
    header_row_detected: bool
    header_row_index: int | None = None
    header_values: list[str]
    column_type_consistency: float
    numeric_ratio: float
    text_ratio: float
    empty_ratio: float
    sample_rows: list[list[str]]
    has_formulas: bool
    is_hidden: bool
    parser_used: ParserUsed
```

### IngestError (errors.py, line 55)
```python
class IngestError(BaseModel):
    code: ErrorCode
    message: str
    sheet_name: str | None = None
    stage: str | None = None          # "parse", "classify", "process", "embed"
    recoverable: bool = False
```

---

## 3. Config Field Reference -- LLM-Related

All fields from `ExcelProcessorConfig` (config.py) relevant to Tier 2/3:

| Field | Type | Default | Purpose |
|---|---|---|---|
| `classification_model` | `str` | `"qwen2.5:7b"` | Model for Tier 2 |
| `reasoning_model` | `str` | `"deepseek-r1:14b"` | Model for Tier 3 |
| `tier2_confidence_threshold` | `float` | `0.6` | Below this -> escalate to Tier 3 |
| `llm_temperature` | `float` | `0.1` | Temperature for classification calls |
| `enable_tier3` | `bool` | `True` | Set False to skip reasoning model |
| `max_sample_rows` | `int` | `3` | Rows to include in LLM summaries |
| `backend_timeout_seconds` | `float` | `30.0` | Per-call timeout for backends |
| `backend_max_retries` | `int` | `2` | Retries with exponential backoff |
| `backend_backoff_base` | `float` | `1.0` | Base seconds for backoff |
| `log_sample_data` | `bool` | `False` | If True, include actual sample values in summary |
| `log_llm_prompts` | `bool` | `False` | If True, log LLM prompts/responses at DEBUG |
| `redact_patterns` | `list[str]` | `[]` | Regex patterns to redact from logged text |

---

## 4. Protocol Reference -- LLMBackend

From `protocols.py`, line 65-86:

```python
@runtime_checkable
class LLMBackend(Protocol):
    def classify(
        self,
        prompt: str,
        model: str,
        temperature: float = 0.1,
        timeout: float | None = None,
    ) -> dict:
        """Send a classification prompt and return the parsed JSON response."""
        ...

    def generate(
        self,
        prompt: str,
        model: str,
        temperature: float = 0.7,
        timeout: float | None = None,
    ) -> str:
        """Send a generation prompt and return the raw text response."""
        ...
```

**Key observations:**
- `classify()` returns `dict` -- the backend is expected to parse JSON. But the SPEC says validation happens in the classifier, so we must handle the case where the backend either returns a parsed dict OR raises on malformed JSON.
- `timeout` is optional (`float | None`); falls back to `config.backend_timeout_seconds`.
- The LLMClassifier will use `classify()` method (not `generate()`), since it needs structured JSON output.
- The `model` parameter is a string -- we pass `config.classification_model` for Tier 2 and `config.reasoning_model` for Tier 3.

---

## 5. Error Codes Reference -- Exact Values

These are the **exact string values** that must be used when constructing `IngestError` objects:

| Error Code | String Value | Trigger | Behavior |
|---|---|---|---|
| `E_LLM_TIMEOUT` | `"E_LLM_TIMEOUT"` | LLM backend raises timeout | Retry per `backend_max_retries`, then fall back |
| `E_LLM_MALFORMED_JSON` | `"E_LLM_MALFORMED_JSON"` | `json.JSONDecodeError` from LLM response | Retry once with correction hint |
| `E_LLM_SCHEMA_INVALID` | `"E_LLM_SCHEMA_INVALID"` | Pydantic `ValidationError` on LLM JSON | Retry once with correction hint |
| `E_LLM_CONFIDENCE_OOB` | `"E_LLM_CONFIDENCE_OOB"` | Confidence < 0.0 or > 1.0 | Clamp to [0.0, 1.0], add warning |
| `E_CLASSIFY_INCONCLUSIVE` | `"E_CLASSIFY_INCONCLUSIVE"` | All tiers failed | Return error result, zero chunks |
| `W_LLM_RETRY` | `"W_LLM_RETRY"` | Any LLM retry triggered | Warning (non-fatal) |

**ENUM_VALUE RISK:** Use `ErrorCode.E_LLM_TIMEOUT` (Python enum member), but be aware the `.value` is `"E_LLM_TIMEOUT"`. When constructing `IngestError`, pass the enum member: `code=ErrorCode.E_LLM_TIMEOUT`.

---

## 6. Structural Summary Spec

### What goes into the summary (SPEC.md section 9.2)

The summary is a **text representation of file structure** sent to the LLM. It must NEVER contain raw data values by default.

**Summary template:**
```
File: {filename}
Sheets: {sheet_count} ({comma-separated sheet names})

Sheet "{sheet_name}":
- Rows: {row_count}, Columns: {col_count}
- Merged cells: {merged_cell_count} (ratio: {merged_cell_ratio:.3f})
- Headers: [{header_values, comma-separated}]
- Column types: [{type of each column from sample_rows}]
- Sample rows (structure only):
  Row 2: [{type, type, type, ...}]
  Row 3: [{type, type, type, ...}]
```

### PII rules
- **Default (`log_sample_data=False`):** Only include data TYPES (str, float, int) in the sample rows section -- NO actual values.
- **Opt-in (`log_sample_data=True`):** Include actual sample VALUES from `SheetProfile.sample_rows`.
- The summary is what gets sent to the LLM prompt. PII protection here prevents PII from flowing through the LLM.
- `max_sample_rows` (default: 3) controls how many sample rows to include.
- `redact_patterns` should be applied if `log_sample_data=True`.

### Fields to extract from SheetProfile for summary:
- `name` -> sheet name
- `row_count`, `col_count` -> dimensions
- `merged_cell_count`, `merged_cell_ratio` -> merge info
- `header_values` -> header names (these are structural, OK to include)
- `sample_rows` -> used to derive types OR actual values depending on config
- `has_formulas` -> mention if True
- `is_hidden` -> mention if True

### Fields to extract from FileProfile for summary:
- `file_path` -> extract filename only (not full path, to avoid leaking filesystem info)
- `sheet_count`, `sheet_names` -> file overview
- `has_password_protected_sheets` -> mention if True
- `has_chart_only_sheets` -> mention if True

---

## 7. Validation Pipeline Spec

### Step-by-step validation sequence (SPEC.md section 9.4)

The `LLMClassificationResponse` Pydantic model for validating LLM output:

```python
from typing import Literal
from pydantic import BaseModel, Field, field_validator

class LLMClassificationResponse(BaseModel):
    """Schema for validating LLM classification output."""
    type: Literal["tabular_data", "formatted_document", "hybrid"]
    confidence: float = Field(ge=0.0, le=1.0)
    reasoning: str = Field(min_length=1)
    sheet_types: dict[str, Literal["tabular_data", "formatted_document"]] | None = None

    @field_validator("confidence")
    @classmethod
    def confidence_in_bounds(cls, v: float) -> float:
        if not (0.0 <= v <= 1.0):
            raise ValueError(f"Confidence {v} outside [0.0, 1.0]")
        return v
```

**Validation flow (exact sequence):**

1. **JSON parse:** Parse LLM response string as JSON.
   - On `json.JSONDecodeError` -> record `E_LLM_MALFORMED_JSON`, retry once with correction hint.
2. **Schema validate:** Validate parsed dict against `LLMClassificationResponse`.
   - On `pydantic.ValidationError` -> record `E_LLM_SCHEMA_INVALID`, retry once with correction hint appended to prompt.
3. **Confidence bounds check:** Even though Pydantic catches this via `Field(ge=0.0, le=1.0)`, the spec says to also handle out-of-bounds explicitly:
   - If confidence < 0.0 or > 1.0 -> record `E_LLM_CONFIDENCE_OOB`, **clamp** to [0.0, 1.0] and add warning. Do NOT reject.
4. **After 2 failed attempts total:** Fall back to Tier 1 result (if available) or fail with structured error. **Never accept unvalidated LLM output.**

**Note on retry strategy:**
- Maximum 2 total attempts (1 original + 1 retry).
- On retry, append a correction hint to the prompt explaining what was wrong.
- Record `W_LLM_RETRY` warning on each retry.

**Note on confidence OOB:**
- The Pydantic model has `ge=0.0, le=1.0` on the Field, which will cause a ValidationError if OOB. But the spec also describes clamping behavior. There are two design options:
  - Option A: Remove `ge`/`le` from Field, check manually after validation, clamp + warn.
  - Option B: Catch the ValidationError for confidence specifically, clamp, and proceed.
  - **Recommended:** Option A -- use `Field()` without bounds, then check manually. This allows clamping instead of rejecting.

---

## 8. Tier Escalation Logic

### When Tier 2 is triggered:
- Tier 1 (`ExcelInspector.classify()`) returns `confidence == 0.0` (inconclusive).
- This is detected by the **router** (not the classifier itself). The classifier is called with a `tier` parameter.
- Looking at inspector.py: inconclusive results have `confidence=0.0` and reasoning starting with "Inconclusive".

### When Tier 3 is triggered:
- Tier 2 returns `confidence < config.tier2_confidence_threshold` (default: 0.6).
- AND `config.enable_tier3 == True`.
- If `enable_tier3 == False`, Tier 2 result is used as-is (even with low confidence).

### Models used:
| Tier | Config Field | Default Value | ClassificationTier Value |
|---|---|---|---|
| Tier 2 | `config.classification_model` | `"qwen2.5:7b"` | `ClassificationTier.LLM_BASIC` / `"llm_basic"` |
| Tier 3 | `config.reasoning_model` | `"deepseek-r1:14b"` | `ClassificationTier.LLM_REASONING` / `"llm_reasoning"` |

### Fail-closed behavior:
- If Tier 2 fails validation after retries AND Tier 1 had a result -> fall back to Tier 1 result.
- If Tier 2 fails and no Tier 1 result -> fail with `E_LLM_SCHEMA_INVALID`.
- If all tiers fail -> `E_CLASSIFY_INCONCLUSIVE` in the processing result, zero chunks/tables.

### LLMClassifier public interface (SPEC.md section 9.6):
```python
class LLMClassifier:
    def __init__(self, llm: LLMBackend, config: ExcelProcessorConfig): ...
    def classify(self, profile: FileProfile, tier: ClassificationTier) -> ClassificationResult: ...
```

**Key design note:** The `tier` parameter tells the classifier which tier to run. The classifier does NOT do escalation internally -- the router handles escalation logic. The classifier:
1. Generates the structural summary from the FileProfile.
2. Builds the classification prompt.
3. Calls `llm.classify()` with the appropriate model for the tier.
4. Validates the response.
5. Returns a `ClassificationResult` (or raises/returns error).

---

## 9. Dependency Map

### What `llm_classifier.py` imports FROM the codebase:

```python
# From models.py
from ingestkit_excel.models import (
    ClassificationResult,
    ClassificationTier,
    FileProfile,
    FileType,
    SheetProfile,       # for type hints in summary generation
)

# From errors.py
from ingestkit_excel.errors import ErrorCode, IngestError

# From config.py
from ingestkit_excel.config import ExcelProcessorConfig

# From protocols.py
from ingestkit_excel.protocols import LLMBackend
```

### Standard library / third-party imports needed:
```python
from __future__ import annotations

import json
import logging
import re
from typing import Any, Literal

from pydantic import BaseModel, Field, ValidationError, field_validator
```

### What imports FROM `llm_classifier.py`:
- `router.py` will import `LLMClassifier`
- `__init__.py` should export `LLMClassifier`
- Test file `test_llm_validation.py` (or similar) will import `LLMClassifier` and `LLMClassificationResponse`

---

## 10. Risk Flags

### ENUM_VALUE Pattern (26% failure rate)
- **LLM response `type` field:** Must be one of `"tabular_data"`, `"formatted_document"`, `"hybrid"`. The Literal type in `LLMClassificationResponse` enforces this. But the prompt must tell the LLM to use these exact strings.
- **`sheet_types` dict values:** Must be `"tabular_data"` or `"formatted_document"` (not `"hybrid"` -- individual sheets are A or B). The Literal type enforces this.
- **Converting LLM response to `FileType` enum:** Use `FileType(response.type)` which will work because `FileType` is `str, Enum` and the values match. Do NOT use `FileType[response.type]` (that looks up by name, not value).
- **`ClassificationTier` in result:** Use `ClassificationTier.LLM_BASIC` for Tier 2, `ClassificationTier.LLM_REASONING` for Tier 3. The `.value` strings are `"llm_basic"` and `"llm_reasoning"`.
- **`ErrorCode` usage:** Always use the enum member (`ErrorCode.E_LLM_TIMEOUT`), never construct from string.

### VERIFICATION_GAP Pattern
- **LLMBackend.classify() return type:** Returns `dict`, not a raw string. The backend parses JSON. But what happens if the backend can't parse JSON? Does it raise? Or return something else? The protocol just says `-> dict`. Implementation must handle both cases: the backend might raise `json.JSONDecodeError` (if it doesn't parse internally) or might raise some other exception. Defensive coding: wrap the call in try/except.
- **`LLMBackend.classify()` vs `LLMBackend.generate()`:** The spec says to use `classify()` which returns `dict`. This means the backend is expected to do JSON parsing. But our validation pipeline also does JSON parsing. Potential double-parsing issue. Resolution: Since `classify()` returns `dict`, skip our own `json.loads()` step and go straight to Pydantic validation of the dict. But we still need to handle the case where the backend fails to parse JSON (raises exception) -- that's our `E_LLM_MALFORMED_JSON` case.
- **Tier parameter mapping:** The `classify()` method takes a `tier: ClassificationTier` parameter. We need to map:
  - `ClassificationTier.LLM_BASIC` -> use `config.classification_model`
  - `ClassificationTier.LLM_REASONING` -> use `config.reasoning_model`
  - What if someone passes `ClassificationTier.RULE_BASED`? Should be an error or ignored. Document this.
- **`per_sheet_types` mapping:** The LLM returns `sheet_types: dict[str, str]` but `ClassificationResult.per_sheet_types` expects `dict[str, FileType]`. Must convert strings to `FileType` enum values.
- **Logging PII safety:** All logging must go through `logger = logging.getLogger("ingestkit_excel")`. Only log prompts/responses at DEBUG level if `config.log_llm_prompts` is True. Apply `config.redact_patterns` to any logged text.

---

## 11. Classification Prompt (Exact Text from SPEC)

```
You are classifying an Excel file for a document ingestion system.
Based on the structural summary below, classify this file as one of:

- "tabular_data": Rows are records, columns are fields. Consistent structure. Suitable for SQL database import.
- "formatted_document": Excel used as a layout/formatting tool. Merged cells, irregular structure, text-heavy. Suitable for text extraction.
- "hybrid": Mix of tabular and document-formatted sections. Different sheets or regions serve different purposes.

Respond with JSON only:
{
  "type": "tabular_data" | "formatted_document" | "hybrid",
  "confidence": <float between 0.0 and 1.0>,
  "reasoning": "brief explanation",
  "sheet_types": {"sheet_name": "type", ...}  // only if hybrid
}

Structural summary:
{summary}
```

---

## 12. Test Patterns Reference

From `tests/test_inspector.py` and `tests/conftest.py`:

### Test file structure:
- Module-level docstring explaining what is tested.
- Imports from `ingestkit_excel` submodules directly (not from `__init__.py`).
- Helper functions prefixed with `_` (e.g., `_make_sheet_profile`, `_make_file_profile`).
- Fixtures using `@pytest.fixture()` (with parens).
- Test classes organized by concern (e.g., `TestSignalEvaluation`, `TestSingleSheetClassification`).
- Test methods follow `test_<what>_<expected_outcome>` naming.
- Type annotations on all test methods returning `-> None`.
- Use of `ExcelProcessorConfig()` with overrides via constructor kwargs.

### Mock pattern for LLM tests (from SPEC.md section 17.1):
- `MockLLM` should support:
  - Configurable canned responses (dict return from `classify()`).
  - Simulating malformed JSON (raise exception or return unparseable).
  - Simulating schema-invalid responses (return dict with wrong fields).
  - Simulating timeouts (raise appropriate exception).
- Tests should verify:
  - Summary generation (no raw data in default mode).
  - Schema validation pass/fail.
  - Malformed JSON retry behavior.
  - Confidence bounds clamping.
  - Tier escalation logic.
  - Fail-closed after retries.

---

## 13. Implementation Checklist

Files to create:
- [ ] `src/ingestkit_excel/llm_classifier.py` -- main implementation

Classes/models to define:
- [ ] `LLMClassificationResponse(BaseModel)` -- Pydantic validation schema for LLM output
- [ ] `LLMClassifier` -- main class with `__init__` and `classify` methods

Methods to implement in `LLMClassifier`:
- [ ] `__init__(self, llm: LLMBackend, config: ExcelProcessorConfig)` -- store deps
- [ ] `classify(self, profile: FileProfile, tier: ClassificationTier) -> ClassificationResult` -- main entry point
- [ ] `_generate_structural_summary(self, profile: FileProfile) -> str` -- build PII-safe summary
- [ ] `_build_prompt(self, summary: str) -> str` -- build classification prompt
- [ ] `_validate_response(self, raw: dict) -> LLMClassificationResponse` -- Pydantic validation
- [ ] `_to_classification_result(self, response: LLMClassificationResponse, tier: ClassificationTier) -> ClassificationResult` -- convert validated response

Files to update:
- [ ] `src/ingestkit_excel/__init__.py` -- add `LLMClassifier` to imports and `__all__`

Test file:
- [ ] `tests/test_llm_classifier.py` -- comprehensive tests

---

## 14. Exact Type Inference for Sample Rows

For the structural summary, sample rows need to show data types. Given `sample_rows: list[list[str]]` in `SheetProfile`, every cell is already a string. We need a heuristic to infer types:

```python
def _infer_cell_type(value: str) -> str:
    """Infer the structural type of a cell value."""
    if not value or value.strip() == "":
        return "empty"
    try:
        int(value)
        return "int"
    except ValueError:
        pass
    try:
        float(value)
        return "float"
    except ValueError:
        pass
    return "str"
```

This is needed for the structure-only mode (`log_sample_data=False`).
