# PLAN -- Issue #12: ExcelRouter Orchestrator and Public API

---
issue: 12
agent: PLAN
date: 2026-02-12
complexity: COMPLEX
stack: backend
---

## Executive Summary

ExcelRouter is the top-level orchestrator that wires together ParserChain, ExcelInspector, LLMClassifier, and all three processors (StructuredDBProcessor, TextSerializer, HybridSplitter) into a single `process()` call. The router handles ingest key computation, tier escalation logic, fail-closed error handling, PII-safe logging, and stage artifact construction. A `create_default_router()` factory provides a zero-config entry point. Two files are created: `router.py` and `tests/test_router.py`, plus updates to `__init__.py`.

---

## 1. `router.py` -- Complete Class Outline

**File:** `packages/ingestkit-excel/src/ingestkit_excel/router.py`

```python
"""ExcelRouter: top-level orchestrator for the ingestkit-excel pipeline.

Wires together the parser chain, tiered classification, and processing
paths into a single ``process()`` call.  Handles ingest key computation,
tier escalation, fail-closed error handling, and PII-safe logging.
"""

from __future__ import annotations

import logging
import os
import time
import uuid

from ingestkit_excel.config import ExcelProcessorConfig
from ingestkit_excel.errors import ErrorCode, IngestError
from ingestkit_excel.idempotency import compute_ingest_key
from ingestkit_excel.inspector import ExcelInspector
from ingestkit_excel.llm_classifier import LLMClassifier
from ingestkit_excel.models import (
    ClassificationResult,
    ClassificationStageResult,
    ClassificationTier,
    FileProfile,
    FileType,
    IngestionMethod,
    ParseStageResult,
    ParserUsed,
    ProcessingResult,
    WrittenArtifacts,
)
from ingestkit_excel.parser_chain import ParserChain
from ingestkit_excel.processors import (
    HybridSplitter,
    StructuredDBProcessor,
    TextSerializer,
)
from ingestkit_excel.protocols import (
    EmbeddingBackend,
    LLMBackend,
    StructuredDBBackend,
    VectorStoreBackend,
)

logger = logging.getLogger("ingestkit_excel")


class ExcelRouter:
    """Top-level orchestrator for Excel file ingestion.

    Parameters
    ----------
    vector_store : VectorStoreBackend
    structured_db : StructuredDBBackend
    llm : LLMBackend
    embedder : EmbeddingBackend
    config : ExcelProcessorConfig | None
        If None, uses ExcelProcessorConfig() defaults.
    """

    def __init__(
        self,
        vector_store: VectorStoreBackend,
        structured_db: StructuredDBBackend,
        llm: LLMBackend,
        embedder: EmbeddingBackend,
        config: ExcelProcessorConfig | None = None,
    ) -> None:
        self._config = config or ExcelProcessorConfig()
        self._vector_store = vector_store
        self._structured_db = structured_db
        self._llm = llm
        self._embedder = embedder

        # Build pipeline components
        self._parser = ParserChain(self._config)
        self._inspector = ExcelInspector(self._config)
        self._llm_classifier = LLMClassifier(self._llm, self._config)

        # Build processors (all three upfront)
        self._structured_processor = StructuredDBProcessor(
            self._structured_db, self._vector_store, self._embedder, self._config
        )
        self._text_serializer = TextSerializer(
            self._vector_store, self._embedder, self._config
        )
        self._hybrid_splitter = HybridSplitter(
            self._structured_processor, self._text_serializer, self._config
        )

    # -- public API --

    def process(
        self, file_path: str, source_uri: str | None = None
    ) -> ProcessingResult:
        """Classify and process a single Excel file."""
        ...  # See section 2

    def process_batch(
        self, file_paths: list[str]
    ) -> list[ProcessingResult]:
        """Process multiple files sequentially."""
        ...  # Simple loop over process()

    # -- private helpers --

    def _build_parse_stage_result(
        self,
        profile: FileProfile,
        parse_errors: list[IngestError],
        parse_duration: float,
    ) -> ParseStageResult:
        """Construct ParseStageResult from parse output."""
        ...  # See section 4

    def _build_classification_stage_result(
        self,
        classification: ClassificationResult,
        classification_duration: float,
    ) -> ClassificationStageResult:
        """Construct ClassificationStageResult from classification output."""
        ...  # See section 5

    def _build_fail_closed_result(
        self,
        file_path: str,
        ingest_key: str,
        ingest_run_id: str,
        parse_result: ParseStageResult,
        classification_result: ClassificationStageResult,
        classification: ClassificationResult,
        errors: list[str],
        warnings: list[str],
        error_details: list[IngestError],
        elapsed: float,
    ) -> ProcessingResult:
        """Build a fail-closed ProcessingResult with zero output."""
        ...  # See section 6

    def _log_result(
        self, file_path: str, result: ProcessingResult
    ) -> None:
        """Emit PII-safe INFO log for the processing result."""
        ...  # See section 7


def create_default_router(**overrides) -> ExcelRouter:
    """Create a router with default backends (Qdrant, SQLite, Ollama)."""
    ...  # See section 8
```

---

## 2. `process()` Flow -- Step-by-Step with Error Handling

```python
def process(
    self, file_path: str, source_uri: str | None = None
) -> ProcessingResult:
    overall_start = time.monotonic()
    config = self._config
    errors: list[str] = []
    warnings: list[str] = []
    error_details: list[IngestError] = []

    # ---- Step 1: Compute ingest key ----
    ingest_key_obj = compute_ingest_key(
        file_path=file_path,
        parser_version=config.parser_version,
        tenant_id=config.tenant_id,
        source_uri=source_uri,
    )
    ingest_key = ingest_key_obj.key  # SHA-256 hex digest string
    ingest_run_id = str(uuid.uuid4())

    # ---- Step 2: Parse ----
    parse_start = time.monotonic()
    profile, parse_errors = self._parser.parse(file_path)
    parse_duration = time.monotonic() - parse_start

    # Collect parse warnings/errors into result lists
    for err in parse_errors:
        if err.code.value.startswith("W_"):
            warnings.append(err.code.value)
        else:
            errors.append(err.code.value)
        error_details.append(err)

    # Build ParseStageResult
    parse_result = self._build_parse_stage_result(
        profile, parse_errors, parse_duration
    )

    # ---- Step 2a: Check for fatal parse failure ----
    # If no sheets parsed AND there are fatal errors, return fail-closed
    has_fatal_parse = any(
        not err.recoverable for err in parse_errors
    )
    if profile.sheet_count == 0 and has_fatal_parse:
        # Build a dummy classification result for fail-closed
        classification = ClassificationResult(
            file_type=FileType.HYBRID,
            confidence=0.0,
            tier_used=ClassificationTier.RULE_BASED,
            reasoning="Parse failed: no sheets available for classification.",
            per_sheet_types=None,
            signals=None,
        )
        classification_stage = self._build_classification_stage_result(
            classification, 0.0
        )
        elapsed = time.monotonic() - overall_start
        result = self._build_fail_closed_result(
            file_path=file_path,
            ingest_key=ingest_key,
            ingest_run_id=ingest_run_id,
            parse_result=parse_result,
            classification_result=classification_stage,
            classification=classification,
            errors=errors,
            warnings=warnings,
            error_details=error_details,
            elapsed=elapsed,
        )
        self._log_result(file_path, result)
        return result

    # ---- Step 3: Tier 1 Classification (Inspector) ----
    classify_start = time.monotonic()
    classification = self._inspector.classify(profile)

    # ---- Step 4: Tier Escalation ----
    # Tier 1 inconclusive: confidence == 0.0
    if classification.confidence == 0.0:
        logger.info(
            "Tier 1 inconclusive for %s, escalating to Tier 2",
            os.path.basename(file_path),
        )
        classification = self._llm_classifier.classify(
            profile, ClassificationTier.LLM_BASIC
        )

    # Tier 2 low confidence: confidence < tier2_confidence_threshold
    # AND enable_tier3 is True
    if (
        classification.confidence < config.tier2_confidence_threshold
        and classification.confidence > 0.0  # 0.0 means LLM failure
        and config.enable_tier3
    ):
        logger.info(
            "Tier 2 confidence %.2f below threshold %.2f for %s, escalating to Tier 3",
            classification.confidence,
            config.tier2_confidence_threshold,
            os.path.basename(file_path),
        )
        classification = self._llm_classifier.classify(
            profile, ClassificationTier.LLM_REASONING
        )

    # Also check: if Tier 2 returned confidence==0.0 (LLM failure),
    # escalate to Tier 3 if enabled
    if classification.confidence == 0.0 and config.enable_tier3 and classification.tier_used == ClassificationTier.LLM_BASIC:
        logger.info(
            "Tier 2 failed for %s, escalating to Tier 3",
            os.path.basename(file_path),
        )
        classification = self._llm_classifier.classify(
            profile, ClassificationTier.LLM_REASONING
        )

    classify_duration = time.monotonic() - classify_start

    classification_stage = self._build_classification_stage_result(
        classification, classify_duration
    )

    # ---- Step 5: Fail-closed check ----
    if classification.confidence == 0.0:
        logger.error(
            "All classification tiers failed for %s -- fail-closed",
            os.path.basename(file_path),
        )
        errors.append(ErrorCode.E_CLASSIFY_INCONCLUSIVE.value)
        error_details.append(
            IngestError(
                code=ErrorCode.E_CLASSIFY_INCONCLUSIVE,
                message="All classification tiers exhausted. Fail-closed.",
                stage="classify",
                recoverable=False,
            )
        )
        elapsed = time.monotonic() - overall_start
        result = self._build_fail_closed_result(
            file_path=file_path,
            ingest_key=ingest_key,
            ingest_run_id=ingest_run_id,
            parse_result=parse_result,
            classification_result=classification_stage,
            classification=classification,
            errors=errors,
            warnings=warnings,
            error_details=error_details,
            elapsed=elapsed,
        )
        self._log_result(file_path, result)
        return result

    # ---- Step 6: Route to processor ----
    processor_args = dict(
        file_path=file_path,
        profile=profile,
        ingest_key=ingest_key,
        ingest_run_id=ingest_run_id,
        parse_result=parse_result,
        classification_result=classification_stage,
        classification=classification,
    )

    if classification.file_type == FileType.TABULAR_DATA:
        result = self._structured_processor.process(**processor_args)
    elif classification.file_type == FileType.FORMATTED_DOCUMENT:
        result = self._text_serializer.process(**processor_args)
    else:  # HYBRID
        result = self._hybrid_splitter.process(**processor_args)

    # ---- Step 7: Merge parse-stage errors/warnings into result ----
    # Processors build their own errors/warnings from their stage.
    # We prepend parse-stage errors/warnings that the router collected.
    result_errors = errors + result.errors
    result_warnings = warnings + result.warnings
    result_error_details = error_details + result.error_details

    # Replace total processing time with the full router time
    elapsed = time.monotonic() - overall_start

    # Rebuild result with merged errors and full timing
    # Use model_copy to avoid reconstructing the entire object
    result = result.model_copy(update={
        "errors": result_errors,
        "warnings": result_warnings,
        "error_details": result_error_details,
        "processing_time_seconds": elapsed,
    })

    # ---- Step 8: Log and return ----
    self._log_result(file_path, result)
    return result
```

### Key Design Decisions

1. **Tier escalation from Tier 2 failure (confidence==0.0)**: The LLMClassifier returns confidence=0.0 on complete failure (all retries exhausted). This is DISTINCT from low confidence (e.g., 0.4). Both should escalate to Tier 3 if enabled, but through different code paths.

2. **Error merging**: Parse-stage errors are collected by the router before the processor runs. Processor-stage errors are in the ProcessingResult returned by the processor. The router merges both sets, with parse errors prepended.

3. **Timing**: `processing_time_seconds` in the final result covers the full router lifecycle (parse + classify + process), not just the processor's internal time.

4. **model_copy()**: Use Pydantic v2's `model_copy(update={...})` to update the ProcessingResult fields without mutating the original or reconstructing from scratch.

---

## 3. Tier Escalation Logic -- Exact Conditions

```
Tier 1 (ExcelInspector):
  - >= 4 signals same type -> confidence 0.9
  - 3 signals same type -> confidence 0.7
  - < 3 signals either way -> confidence 0.0 (INCONCLUSIVE)
  - Sheets disagree -> HYBRID, confidence 0.9

Escalation to Tier 2:
  - IF classification.confidence == 0.0
  - Call LLMClassifier.classify(profile, ClassificationTier.LLM_BASIC)

Escalation to Tier 3:
  - IF classification.confidence < config.tier2_confidence_threshold (0.6)
    AND config.enable_tier3 is True
  - OR IF Tier 2 returned confidence == 0.0 (complete LLM failure)
    AND config.enable_tier3 is True
  - Call LLMClassifier.classify(profile, ClassificationTier.LLM_REASONING)

Fail-closed:
  - IF final classification.confidence == 0.0
  - Return ProcessingResult with E_CLASSIFY_INCONCLUSIVE, zero chunks/tables
```

**Important edge cases:**
- Tier 1 returns confidence=0.7 (medium) -> does NOT escalate. Only confidence==0.0 escalates.
- Tier 1 returns HYBRID with confidence=0.9 (sheets disagree) -> valid classification, NOT inconclusive.
- Tier 2 returns confidence=0.5 (below threshold but > 0) -> escalate to Tier 3.
- Tier 2 returns confidence=0.0 (LLM failure) -> escalate to Tier 3 (if enabled).
- Tier 3 returns confidence=0.0 -> fail-closed.
- `enable_tier3=False` -> skip Tier 3, if Tier 2 confidence < threshold AND > 0, ACCEPT the Tier 2 result. If Tier 2 confidence == 0.0, fail-closed.

**Correction to initial logic**: When `enable_tier3=False` and Tier 2 has low confidence (> 0 but < threshold), we should still ACCEPT the result rather than fail-closed. The fail-closed condition is strictly `confidence == 0.0` which means classification completely failed. A low but nonzero confidence is still a valid classification.

Revised fail-closed check:
```python
# Fail-closed: only when confidence is exactly 0.0 (meaning all tiers FAILED)
if classification.confidence == 0.0:
    # fail-closed
```

---

## 4. ParseStageResult Construction

```python
def _build_parse_stage_result(
    self,
    profile: FileProfile,
    parse_errors: list[IngestError],
    parse_duration: float,
) -> ParseStageResult:
    """Construct ParseStageResult from FileProfile + parse errors."""
    # Determine primary parser_used: most common among sheets, or OPENPYXL if no sheets
    if profile.sheets:
        parser_counts: dict[ParserUsed, int] = {}
        for sheet in profile.sheets:
            parser_counts[sheet.parser_used] = parser_counts.get(sheet.parser_used, 0) + 1
        primary_parser = max(parser_counts, key=parser_counts.get)
    else:
        primary_parser = ParserUsed.OPENPYXL

    # Determine fallback_reason_code: if any sheet used a fallback parser
    fallback_reason = None
    for err in parse_errors:
        if err.code == ErrorCode.W_PARSER_FALLBACK:
            fallback_reason = err.code.value
            break

    # Count parsed vs skipped
    sheets_parsed = len(profile.sheets)

    # Skipped sheets: those with warning errors (chart, password, truncated, etc.)
    skipped_reasons: dict[str, str] = {}
    for err in parse_errors:
        if err.sheet_name and err.code.value.startswith("W_"):
            skipped_reasons[err.sheet_name] = err.code.value

    sheets_skipped = len(skipped_reasons)

    return ParseStageResult(
        parser_used=primary_parser,
        fallback_reason_code=fallback_reason,
        sheets_parsed=sheets_parsed,
        sheets_skipped=sheets_skipped,
        skipped_reasons=skipped_reasons,
        parse_duration_seconds=parse_duration,
    )
```

---

## 5. ClassificationStageResult Construction

```python
def _build_classification_stage_result(
    self,
    classification: ClassificationResult,
    classification_duration: float,
) -> ClassificationStageResult:
    return ClassificationStageResult(
        tier_used=classification.tier_used,
        file_type=classification.file_type,
        confidence=classification.confidence,
        signals=classification.signals,
        reasoning=classification.reasoning,
        per_sheet_types=classification.per_sheet_types,
        classification_duration_seconds=classification_duration,
    )
```

---

## 6. Fail-Closed Handling

When parsing fails completely or classification is inconclusive, the router constructs a minimal `ProcessingResult`:

```python
def _build_fail_closed_result(
    self,
    file_path: str,
    ingest_key: str,
    ingest_run_id: str,
    parse_result: ParseStageResult,
    classification_result: ClassificationStageResult,
    classification: ClassificationResult,
    errors: list[str],
    warnings: list[str],
    error_details: list[IngestError],
    elapsed: float,
) -> ProcessingResult:
    return ProcessingResult(
        file_path=file_path,
        ingest_key=ingest_key,
        ingest_run_id=ingest_run_id,
        tenant_id=self._config.tenant_id,
        parse_result=parse_result,
        classification_result=classification_result,
        embed_result=None,
        classification=classification,
        ingestion_method=IngestionMethod.SQL_AGENT,  # arbitrary, since no processing occurs
        chunks_created=0,
        tables_created=0,
        tables=[],
        written=WrittenArtifacts(),
        errors=errors,
        warnings=warnings,
        error_details=error_details,
        processing_time_seconds=elapsed,
    )
```

**Key properties of a fail-closed result:**
- `chunks_created=0`, `tables_created=0`, `tables=[]`
- `written=WrittenArtifacts()` (empty -- no vector_collection, no ids)
- `embed_result=None`
- `errors` contains the relevant error code string (e.g. `"E_CLASSIFY_INCONCLUSIVE"` or `"E_PARSE_CORRUPT"`)
- `error_details` contains structured `IngestError` objects
- `ingestion_method` is set to `SQL_AGENT` as a default (the spec doesn't specify a particular value for fail-closed; any valid enum member works since no processing actually occurs)

---

## 7. PII-Safe Logging

```python
def _log_result(self, file_path: str, result: ProcessingResult) -> None:
    """Emit PII-safe INFO log matching SPEC section 15 format."""
    filename = os.path.basename(file_path)

    # Determine primary parser for log
    parser_str = result.parse_result.parser_used.value

    logger.info(
        "file=%s | ingest_key=%.8s | tier=%s | type=%s | confidence=%s "
        "| path=%s | chunks=%d | tables=%d | parser=%s | time=%.1fs",
        filename,
        result.ingest_key,
        result.classification_result.tier_used.value,
        result.classification_result.file_type.value,
        result.classification_result.confidence,
        result.ingestion_method.value,
        result.chunks_created,
        result.tables_created,
        parser_str,
        result.processing_time_seconds,
    )
```

**PII-safe by design:**
- Only filename (not full path) in logs
- Ingest key truncated to first 8 chars
- No raw cell data, no sample rows, no LLM prompts
- Config flags `log_sample_data`, `log_llm_prompts`, `log_chunk_previews` are handled by the individual components (parser, classifier, processors), not the router itself

---

## 8. `create_default_router()` Factory

```python
def create_default_router(**overrides) -> ExcelRouter:
    """Create a router with default backends (Qdrant, SQLite, Ollama).

    Accepts any ExcelProcessorConfig field as a keyword override.
    Requires qdrant-client and httpx to be installed.

    Parameters
    ----------
    **overrides
        Keyword arguments passed to ExcelProcessorConfig().

    Returns
    -------
    ExcelRouter
        A fully configured router with default backends.
    """
    config = ExcelProcessorConfig(**overrides)

    # Import concrete backends (may raise ImportError if deps missing)
    from ingestkit_excel.backends.qdrant import QdrantVectorStore
    from ingestkit_excel.backends.sqlite import SQLiteStructuredDB
    from ingestkit_excel.backends.ollama import OllamaLLM, OllamaEmbedding

    vector_store = QdrantVectorStore()
    structured_db = SQLiteStructuredDB()
    llm = OllamaLLM()
    embedder = OllamaEmbedding(
        model=config.embedding_model,
    )

    return ExcelRouter(
        vector_store=vector_store,
        structured_db=structured_db,
        llm=llm,
        embedder=embedder,
        config=config,
    )
```

**Notes:**
- Imports are inside the function body so the factory fails gracefully if optional deps are missing, without affecting the rest of the module.
- Uses default constructor args for each backend (see backends for signatures).
- The `**overrides` pattern lets callers customize config without constructing it themselves: `create_default_router(tenant_id="acme", classification_model="llama3:8b")`.

---

## 9. `process_batch()` Implementation

```python
def process_batch(
    self, file_paths: list[str]
) -> list[ProcessingResult]:
    """Process multiple files sequentially.

    Each file is processed independently. Failures in one file
    do not affect others.
    """
    results: list[ProcessingResult] = []
    for file_path in file_paths:
        try:
            result = self.process(file_path)
            results.append(result)
        except Exception as exc:
            # If process() itself raises (e.g., FileNotFoundError from
            # compute_ingest_key), log and continue
            logger.error(
                "Failed to process %s: %s",
                os.path.basename(file_path),
                exc,
            )
            # We cannot build a full ProcessingResult without parsing,
            # so we re-raise to let the caller handle it.
            # ALTERNATIVE: wrap in a try/except within process() so it
            # never raises. But FileNotFoundError from compute_ingest_key
            # is a valid exception the caller should handle.
            raise
    return results
```

**Design decision**: `process_batch()` does NOT swallow exceptions from `process()`. If `process()` raises (e.g., `FileNotFoundError`), it propagates to the caller. The `process()` method itself should handle all recoverable errors internally and return a `ProcessingResult` -- only truly unrecoverable scenarios (file not found, etc.) should raise.

**However**, per SPEC the parser chain raises `FileNotFoundError`. The router's `process()` should catch this and return a fail-closed result. Let me revise:

In `process()`, wrap the entire flow in a try/except to catch `FileNotFoundError` from `compute_ingest_key` or `parse()`:

```python
def process(self, file_path: str, source_uri: str | None = None) -> ProcessingResult:
    overall_start = time.monotonic()
    try:
        return self._process_impl(file_path, source_uri, overall_start)
    except FileNotFoundError:
        logger.error("File not found: %s", file_path)
        raise  # Re-raise -- caller must handle missing files
    except Exception as exc:
        logger.exception("Unexpected error processing %s: %s", file_path, exc)
        raise
```

Actually, `FileNotFoundError` should propagate to the caller since the file literally doesn't exist. The router should not fabricate a ProcessingResult for a non-existent file (no content_hash, no ingest_key). So `process_batch()` should handle this:

```python
def process_batch(self, file_paths: list[str]) -> list[ProcessingResult]:
    results: list[ProcessingResult] = []
    for file_path in file_paths:
        result = self.process(file_path)
        results.append(result)
    return results
```

Keep it simple -- just a sequential loop. If one file raises, the exception propagates. The caller can wrap in their own try/except if they want to continue on failure.

---

## 10. `tests/test_router.py` -- Test Plan

**File:** `packages/ingestkit-excel/tests/test_router.py`

### Mock Backends (reuse from test_structured_db.py pattern)

```python
class MockStructuredDB:  # same as test_structured_db.py
class MockVectorStore:   # same as test_structured_db.py
class MockEmbedder:      # same as test_structured_db.py
class MockLLM:           # same as test_llm_classifier.py pattern
```

### Helper Factories

```python
def _make_sheet_profile(**overrides) -> SheetProfile: ...
def _make_file_profile(sheets, **overrides) -> FileProfile: ...
```

### Test Classes

#### `TestRouterConstruction`
- `test_router_default_config` -- config=None uses defaults
- `test_router_custom_config` -- custom config passed through
- `test_router_creates_all_components` -- parser, inspector, classifier, 3 processors exist

#### `TestProcessHappyPath` (mock ParserChain.parse and pd.read_excel)
- `test_process_tabular_file` -- Tier 1 classifies as TABULAR_DATA, routes to Path A
- `test_process_formatted_document` -- Tier 1 classifies as FORMATTED_DOCUMENT, routes to Path B
- `test_process_hybrid_file` -- Tier 1 classifies as HYBRID, routes to Path C
- `test_process_returns_correct_ingest_key` -- verify ingest_key in result matches computed key
- `test_process_returns_correct_ingest_run_id` -- UUID4 format
- `test_process_returns_correct_tenant_id` -- from config
- `test_process_result_has_parse_result` -- ParseStageResult populated
- `test_process_result_has_classification_result` -- ClassificationStageResult populated
- `test_process_result_processing_time` -- processing_time_seconds > 0

#### `TestTierEscalation` (mock inspector and LLM classifier)
- `test_tier1_conclusive_no_escalation` -- Tier 1 confidence=0.9, no LLM calls
- `test_tier1_medium_confidence_no_escalation` -- Tier 1 confidence=0.7, no LLM calls
- `test_tier1_inconclusive_escalates_to_tier2` -- confidence=0.0 triggers Tier 2
- `test_tier2_high_confidence_accepted` -- Tier 2 confidence=0.8, no Tier 3
- `test_tier2_low_confidence_escalates_to_tier3` -- Tier 2 confidence=0.4 triggers Tier 3
- `test_tier2_failure_escalates_to_tier3` -- Tier 2 confidence=0.0 (LLM failure) triggers Tier 3
- `test_tier3_disabled_no_escalation` -- enable_tier3=False, Tier 2 low confidence accepted
- `test_tier3_disabled_tier2_failure_fail_closed` -- enable_tier3=False, Tier 2 confidence=0.0 -> fail-closed
- `test_all_tiers_fail_returns_fail_closed` -- Tier 1 + 2 + 3 all confidence=0.0

#### `TestFailClosed`
- `test_fail_closed_has_zero_chunks` -- chunks_created=0
- `test_fail_closed_has_zero_tables` -- tables_created=0
- `test_fail_closed_has_empty_written` -- WrittenArtifacts is empty
- `test_fail_closed_has_inconclusive_error` -- E_CLASSIFY_INCONCLUSIVE in errors
- `test_fail_closed_has_error_details` -- structured IngestError present
- `test_fail_closed_embed_result_is_none` -- embed_result=None

#### `TestParseFailure` (mock parser to return empty profile with errors)
- `test_parse_failure_returns_fail_closed` -- empty profile + fatal error -> fail-closed
- `test_parse_empty_file` -- E_PARSE_EMPTY in errors
- `test_parse_corrupt_file` -- E_PARSE_CORRUPT in errors
- `test_parse_password_protected` -- E_PARSE_PASSWORD in errors
- `test_parse_warnings_in_result` -- W_PARSER_FALLBACK in warnings

#### `TestParseStageResult`
- `test_parse_stage_primary_parser` -- most common parser selected
- `test_parse_stage_fallback_reason` -- W_PARSER_FALLBACK recorded
- `test_parse_stage_sheets_parsed_count` -- matches sheet count
- `test_parse_stage_sheets_skipped_count` -- skipped sheets counted
- `test_parse_stage_skipped_reasons` -- dict populated for skipped sheets
- `test_parse_stage_duration` -- > 0

#### `TestClassificationStageResult`
- `test_classification_stage_fields` -- all fields populated from ClassificationResult
- `test_classification_stage_duration` -- > 0

#### `TestErrorMerging`
- `test_parse_errors_merged_into_result` -- parse errors + processor errors combined
- `test_parse_warnings_merged_into_result` -- parse warnings + processor warnings combined
- `test_error_details_merged` -- error_details from both stages merged

#### `TestPIISafeLogging`
- `test_info_log_emitted` -- caplog check for INFO level message
- `test_info_log_contains_filename` -- only basename, not full path
- `test_info_log_contains_truncated_key` -- first 8 chars only
- `test_info_log_no_raw_data` -- no sample rows or cell values in log

#### `TestProcessBatch`
- `test_process_batch_returns_list` -- list of ProcessingResult
- `test_process_batch_processes_all_files` -- one result per file
- `test_process_batch_empty_list` -- empty input returns empty output

#### `TestSourceUri`
- `test_process_with_source_uri` -- custom source_uri passed to compute_ingest_key
- `test_process_without_source_uri` -- source_uri=None uses default derivation

#### `TestCreateDefaultRouter`
- `test_create_default_router_returns_router` -- returns ExcelRouter instance
- `test_create_default_router_with_overrides` -- config overrides applied
- `test_create_default_router_missing_deps` -- ImportError if qdrant-client not installed

### Mocking Strategy

The tests should mock at the right level:
1. **ParserChain.parse**: Mock to return pre-built FileProfile and IngestError lists
2. **ExcelInspector.classify**: Mock to return pre-built ClassificationResult
3. **LLMClassifier.classify**: Mock to return pre-built ClassificationResult
4. **Processor.process**: Mock to return pre-built ProcessingResult
5. **compute_ingest_key**: Mock to return pre-built IngestKey

Use `unittest.mock.patch` on the class methods or module-level functions. Example:

```python
@patch("ingestkit_excel.router.compute_ingest_key")
@patch.object(ParserChain, "parse")
@patch.object(ExcelInspector, "classify")
def test_process_tabular_file(
    self, mock_inspector_classify, mock_parse, mock_compute_key,
    router, mock_db, mock_vector_store, mock_embedder,
):
    # Setup mocks
    mock_compute_key.return_value = IngestKey(
        content_hash="a" * 64,
        source_uri="file:///tmp/test.xlsx",
        parser_version="ingestkit_excel:1.0.0",
    )
    profile = _make_file_profile([_make_sheet_profile()])
    mock_parse.return_value = (profile, [])
    mock_inspector_classify.return_value = ClassificationResult(
        file_type=FileType.TABULAR_DATA,
        confidence=0.9,
        tier_used=ClassificationTier.RULE_BASED,
        reasoning="High consistency",
    )
    # ... mock pd.read_excel for the processor ...

    result = router.process("/tmp/test.xlsx")

    assert result.ingestion_method == IngestionMethod.SQL_AGENT
    assert result.classification.file_type == FileType.TABULAR_DATA
```

---

## 11. `__init__.py` Updates

Add two new exports:

```python
from ingestkit_excel.router import ExcelRouter, create_default_router

__all__ = [
    # ... existing entries ...
    # Router
    "ExcelRouter",
    "create_default_router",
]
```

Place the import after the processors import and before the `__all__` list. Add the entries to the `__all__` list in the "Router" section (new section after "Processors").

---

## 12. Acceptance Criteria

- [ ] `router.py` exists with `ExcelRouter` class and `create_default_router` function
- [ ] `ExcelRouter.__init__` accepts (vector_store, structured_db, llm, embedder, config=None)
- [ ] `process()` computes ingest_key, generates UUID4 run_id, parses, classifies, routes to processor
- [ ] Tier escalation: Tier 1 inconclusive (conf=0.0) -> Tier 2 -> Tier 3 (if enabled)
- [ ] Fail-closed: all tiers fail -> ProcessingResult with E_CLASSIFY_INCONCLUSIVE, 0 chunks/tables
- [ ] Parse failure: empty profile with fatal errors -> fail-closed result
- [ ] ParseStageResult correctly constructed from FileProfile metadata
- [ ] ClassificationStageResult correctly constructed from ClassificationResult
- [ ] Error merging: parse-stage errors + processor errors combined in final result
- [ ] PII-safe INFO log emitted per SPEC section 15 format
- [ ] `process_batch()` processes files sequentially
- [ ] `create_default_router(**overrides)` creates router with default backends
- [ ] `__init__.py` exports ExcelRouter and create_default_router
- [ ] Logger uses `logging.getLogger("ingestkit_excel")` (NOT `__name__`)
- [ ] All new code has unit tests in `tests/test_router.py`
- [ ] No regressions in existing tests

---

## 13. File Summary

| File | Action | Lines (est.) |
|------|--------|-------------|
| `packages/ingestkit-excel/src/ingestkit_excel/router.py` | CREATE | ~280 |
| `packages/ingestkit-excel/tests/test_router.py` | CREATE | ~600 |
| `packages/ingestkit-excel/src/ingestkit_excel/__init__.py` | EDIT | +5 |

Total: ~885 lines of new code across 2 new files and 1 edit.

AGENT_RETURN: plan-12-021226.md
