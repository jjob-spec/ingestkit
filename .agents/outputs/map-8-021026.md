# MAP Artifact: Issue #8 -- Path A Structured DB Processor

**Issue:** #8
**Date:** 2026-02-11
**Target file:** `src/ingestkit_excel/processors/structured_db.py` (NEW)
**Also needed:** `src/ingestkit_excel/processors/__init__.py` (NEW)
**Spec reference:** SPEC.md section 10.1 (Path A -- Structured DB Processor)

---

## 1. Enum Values Reference

All enum values are the **string values** (not Python names). Use these exact strings when constructing metadata and results.

### IngestionMethod (models.py line 48-57)
| Python Name | String Value | Context |
|---|---|---|
| `SQL_AGENT` | `"sql_agent"` | **Path A -- this processor** |
| `TEXT_SERIALIZATION` | `"text_serialization"` | Path B |
| `HYBRID_SPLIT` | `"hybrid_split"` | Path C |

**CRITICAL ENUM_VALUE:** Use `IngestionMethod.SQL_AGENT` (Python) or `"sql_agent"` (string). **NEVER** `"SQL_AGENT"`.

### ParserUsed (models.py line 72-77)
| Python Name | String Value |
|---|---|
| `OPENPYXL` | `"openpyxl"` |
| `PANDAS_FALLBACK` | `"pandas_fallback"` |
| `RAW_TEXT_FALLBACK` | `"raw_text_fallback"` |

### FileType (models.py line 24-32)
| Python Name | String Value |
|---|---|
| `TABULAR_DATA` | `"tabular_data"` |
| `FORMATTED_DOCUMENT` | `"formatted_document"` |
| `HYBRID` | `"hybrid"` |

### ClassificationTier (models.py line 36-45)
| Python Name | String Value |
|---|---|
| `RULE_BASED` | `"rule_based"` |
| `LLM_BASIC` | `"llm_basic"` |
| `LLM_REASONING` | `"llm_reasoning"` |

### ErrorCode -- Path A Relevant (errors.py)
| Python Name | String Value | When Used |
|---|---|---|
| `E_BACKEND_DB_TIMEOUT` | `"E_BACKEND_DB_TIMEOUT"` | StructuredDBBackend timeout |
| `E_BACKEND_DB_CONNECT` | `"E_BACKEND_DB_CONNECT"` | StructuredDBBackend connection failure |
| `E_BACKEND_VECTOR_TIMEOUT` | `"E_BACKEND_VECTOR_TIMEOUT"` | VectorStoreBackend timeout |
| `E_BACKEND_VECTOR_CONNECT` | `"E_BACKEND_VECTOR_CONNECT"` | VectorStoreBackend connection failure |
| `E_BACKEND_EMBED_TIMEOUT` | `"E_BACKEND_EMBED_TIMEOUT"` | EmbeddingBackend timeout |
| `E_BACKEND_EMBED_CONNECT` | `"E_BACKEND_EMBED_CONNECT"` | EmbeddingBackend connection failure |
| `E_PROCESS_SCHEMA_GEN` | `"E_PROCESS_SCHEMA_GEN"` | Schema description generation failed |
| `W_SHEET_SKIPPED_CHART` | `"W_SHEET_SKIPPED_CHART"` | Chart-only sheet skipped |
| `W_SHEET_SKIPPED_HIDDEN` | `"W_SHEET_SKIPPED_HIDDEN"` | Hidden sheet skipped |
| `W_SHEET_SKIPPED_PASSWORD` | `"W_SHEET_SKIPPED_PASSWORD"` | Password-protected sheet skipped |
| `W_ROWS_TRUNCATED` | `"W_ROWS_TRUNCATED"` | Sheet exceeds max_rows_in_memory |
| `W_PARSER_FALLBACK` | `"W_PARSER_FALLBACK"` | Fallback parser used |

---

## 2. Model Field Reference

### ChunkMetadata (models.py lines 195-219) -- ALL FIELDS

```python
class ChunkMetadata(BaseModel):
    # Shared across all paths
    source_uri: str                    # canonical file path or URI
    source_format: str = "xlsx"        # always "xlsx" for this package
    sheet_name: str                    # worksheet name
    region_id: str | None = None       # for hybrid splits (Path C only, None for Path A)
    ingestion_method: str              # IngestionMethod VALUE string, e.g. "sql_agent"
    parser_used: str                   # ParserUsed VALUE string, e.g. "openpyxl"
    parser_version: str                # e.g. "ingestkit_excel:1.0.0"
    chunk_index: int                   # 0-based position within this source
    chunk_hash: str                    # SHA-256 of chunk text
    ingest_key: str                    # idempotency key (from IngestKey.key property)
    ingest_run_id: str                 # UUID4, unique per process() invocation
    tenant_id: str | None = None       # from config

    # Path A specific -- THESE ARE THE FIELDS WE MUST POPULATE
    table_name: str | None = None      # cleaned table name written to DB
    db_uri: str | None = None          # connection URI from structured_db.get_connection_uri()
    row_count: int | None = None       # number of rows in the table
    columns: list[str] | None = None   # list of cleaned column names

    # Path B specific -- set to None for Path A
    section_title: str | None = None
    original_structure: str | None = None
```

### ChunkPayload (models.py lines 222-228)

```python
class ChunkPayload(BaseModel):
    id: str                # deterministic UUID from chunk_hash + ingest_key
    text: str              # the text content (schema description or serialized row)
    vector: list[float]    # embedding vector
    metadata: ChunkMetadata
```

### WrittenArtifacts (models.py lines 245-250)

```python
class WrittenArtifacts(BaseModel):
    vector_point_ids: list[str] = []       # IDs upserted to vector store
    vector_collection: str | None = None   # collection name used
    db_table_names: list[str] = []         # table names created in structured DB
```

### ProcessingResult (models.py lines 253-277)

```python
class ProcessingResult(BaseModel):
    file_path: str
    ingest_key: str                        # idempotency key
    ingest_run_id: str                     # unique per invocation
    tenant_id: str | None = None

    # Stage artifacts (typed, persisted)
    parse_result: ParseStageResult
    classification_result: ClassificationStageResult
    embed_result: EmbedStageResult | None = None  # populated by this processor

    # Legacy convenience fields
    classification: ClassificationResult
    ingestion_method: IngestionMethod      # = IngestionMethod.SQL_AGENT for Path A

    # Outputs
    chunks_created: int                    # total chunks upserted to vector store
    tables_created: int                    # tables written to structured DB
    tables: list[str]                      # table name list
    written: WrittenArtifacts              # for caller-side rollback

    # Errors and warnings
    errors: list[str]                      # list of ErrorCode VALUE strings
    warnings: list[str]                    # list of ErrorCode VALUE strings
    error_details: list[IngestError] = []  # structured error objects

    processing_time_seconds: float
```

### ParseStageResult (models.py lines 112-120)

```python
class ParseStageResult(BaseModel):
    parser_used: ParserUsed
    fallback_reason_code: str | None = None
    sheets_parsed: int
    sheets_skipped: int
    skipped_reasons: dict[str, str]       # {sheet_name: reason_code}
    parse_duration_seconds: float
```

### ClassificationStageResult (models.py lines 123-132)

```python
class ClassificationStageResult(BaseModel):
    tier_used: ClassificationTier
    file_type: FileType
    confidence: float
    signals: dict[str, Any] | None = None
    reasoning: str
    per_sheet_types: dict[str, FileType] | None = None
    classification_duration_seconds: float
```

### EmbedStageResult (models.py lines 135-140)

```python
class EmbedStageResult(BaseModel):
    texts_embedded: int            # total texts embedded
    embedding_dimension: int       # dimension of vectors
    embed_duration_seconds: float
```

### SheetProfile (models.py lines 148-166) -- Input to processor

```python
class SheetProfile(BaseModel):
    name: str
    row_count: int
    col_count: int
    merged_cell_count: int
    merged_cell_ratio: float
    header_row_detected: bool
    header_row_index: int | None = None    # NOTE: actual code has this field (not in spec)
    header_values: list[str]
    column_type_consistency: float
    numeric_ratio: float
    text_ratio: float
    empty_ratio: float
    sample_rows: list[list[str]]
    has_formulas: bool
    is_hidden: bool
    parser_used: ParserUsed
```

### FileProfile (models.py lines 169-181) -- Input to processor

```python
class FileProfile(BaseModel):
    file_path: str
    file_size_bytes: int
    sheet_count: int
    sheet_names: list[str]
    sheets: list[SheetProfile]
    has_password_protected_sheets: bool
    has_chart_only_sheets: bool
    total_merged_cells: int
    total_rows: int
    content_hash: str
```

### IngestError (errors.py lines 55-67)

```python
class IngestError(BaseModel):
    code: ErrorCode
    message: str
    sheet_name: str | None = None
    stage: str | None = None          # "parse", "classify", "process", "embed"
    recoverable: bool = False
```

---

## 3. Config Field Reference -- Processor-Relevant Fields

From `ExcelProcessorConfig` (config.py):

| Field | Type | Default | Usage in Path A |
|---|---|---|---|
| `parser_version` | `str` | `"ingestkit_excel:1.0.0"` | ChunkMetadata.parser_version |
| `tenant_id` | `str \| None` | `None` | ChunkMetadata.tenant_id, ProcessingResult.tenant_id |
| `row_serialization_limit` | `int` | `5000` | Only serialize rows if table has FEWER than this many rows |
| `clean_column_names` | `bool` | `True` | Whether to clean column names (lowercase, special char replace, dedup) |
| `embedding_model` | `str` | `"nomic-embed-text"` | Model name (informational, passed through to embedder) |
| `embedding_dimension` | `int` | `768` | For EmbedStageResult and ensure_collection vector_size |
| `embedding_batch_size` | `int` | `64` | Batch size when calling embedder.embed() |
| `default_collection` | `str` | `"helpdesk"` | Vector store collection name |
| `backend_timeout_seconds` | `float` | `30.0` | Timeout for backend calls |
| `backend_max_retries` | `int` | `2` | Max retries for backend calls |
| `backend_backoff_base` | `float` | `1.0` | Base seconds for exponential backoff |
| `max_rows_in_memory` | `int` | `100_000` | Sheets exceeding this are skipped |
| `log_sample_data` | `bool` | `False` | Whether to log sample data |
| `log_chunk_previews` | `bool` | `False` | Whether to log chunk text |

---

## 4. Protocol Reference -- Exact Method Signatures

### StructuredDBBackend (protocols.py lines 40-59)

```python
@runtime_checkable
class StructuredDBBackend(Protocol):
    def create_table_from_dataframe(self, table_name: str, df: pd.DataFrame) -> None: ...
    def drop_table(self, table_name: str) -> None: ...
    def table_exists(self, table_name: str) -> bool: ...
    def get_table_schema(self, table_name: str) -> dict: ...
    def get_connection_uri(self) -> str: ...
```

**Usage in Path A:**
- `create_table_from_dataframe(table_name, df)` -- Step 4: write each sheet as a table
- `get_connection_uri()` -- for `ChunkMetadata.db_uri`
- `get_table_schema(table_name)` -- for schema description generation (Step 5)

### VectorStoreBackend (protocols.py lines 19-36)

```python
@runtime_checkable
class VectorStoreBackend(Protocol):
    def upsert_chunks(self, collection: str, chunks: list[ChunkPayload]) -> int: ...
    def ensure_collection(self, collection: str, vector_size: int) -> None: ...
    def create_payload_index(self, collection: str, field: str, field_type: str) -> None: ...
    def delete_by_ids(self, collection: str, ids: list[str]) -> int: ...
```

**Usage in Path A:**
- `ensure_collection(config.default_collection, config.embedding_dimension)` -- ensure collection exists
- `upsert_chunks(collection, chunks)` -- Step 6: upsert schema description chunks (and optional row chunks)

### EmbeddingBackend (protocols.py lines 89-101)

```python
@runtime_checkable
class EmbeddingBackend(Protocol):
    def embed(self, texts: list[str], timeout: float | None = None) -> list[list[float]]: ...
    def dimension(self) -> int: ...
```

**Usage in Path A:**
- `embed(texts, timeout)` -- Step 6: embed schema descriptions; Step 7: embed row serializations
- `dimension()` -- for EmbedStageResult.embedding_dimension and ensure_collection vector_size

---

## 5. SPEC section 10.1 -- All 7 Processing Steps

### Step 1: Load each sheet into a pandas DataFrame

- Iterate over `profile.sheets` (list of `SheetProfile`)
- For each sheet, load via `pd.read_excel(file_path, sheet_name=sheet.name)`
- Use `header=sheet.header_row_index` if header was detected, otherwise `header=None`
- Skip sheets that are chart-only, hidden (if config says so), or exceed row limits
- The DataFrame is the working data for all subsequent steps

### Step 2: Clean column names

- **Only if `config.clean_column_names` is True**
- Rules:
  1. Convert to lowercase: `col.lower()`
  2. Replace spaces and special characters with underscores: `re.sub(r'[^a-z0-9_]', '_', col)`
  3. Collapse multiple consecutive underscores: `re.sub(r'_+', '_', col)`
  4. Strip leading/trailing underscores: `col.strip('_')`
  5. Deduplicate column names: if duplicates exist after cleaning, append `_1`, `_2`, etc.
  6. If column name becomes empty after cleaning, use `column_N` (0-based index)
- Apply to DataFrame: `df.columns = cleaned_columns`

### Step 3: Auto-detect and parse date columns

- Detect date columns by two patterns:
  1. **Excel serial dates**: numeric columns where values are plausible date serials (e.g., 40000-50000 range, representing ~2009-2036). Convert via `pd.to_datetime(col, origin='1899-12-30', unit='D')`
  2. **String dates**: text columns where values parse successfully as dates. Use `pd.to_datetime(col, infer_datetime_format=True, errors='coerce')`. If a high proportion (e.g., >50%) parse successfully, convert the column.
- **Important**: Be conservative. Only convert when confident to avoid corrupting numeric data.

### Step 4: Write each sheet as a table to StructuredDBBackend

- Derive table name from sheet name using same cleaning rules as column names (lowercase, special chars to underscores, dedup)
- Call `structured_db.create_table_from_dataframe(table_name, df)`
- Track table name in `WrittenArtifacts.db_table_names`

### Step 5: Generate natural language schema description per table

- Format (from SPEC):
  ```
  Table "employee_roster" contains 342 rows with columns:
  - employee_id (integer): unique identifier, range 10042-10943
  - full_name (text): employee name
  - department (text): one of Engineering, Sales, HR, Finance, ...
  - hire_date (date): ranges from 2018-01-15 to 2024-11-30
  - salary (float): ranges from 45000.0 to 185000.0
  ```
- For each column, describe:
  - Column name and inferred type (integer, float, text, date, boolean)
  - For numeric columns: range (min to max)
  - For text columns: if cardinality is low (< ~20 unique), list the unique values; otherwise say "N unique values"
  - For date columns: range (earliest to latest)
- This description becomes the chunk text for embedding

### Step 6: Embed schema description and upsert to VectorStoreBackend

- Ensure collection exists: `vector_store.ensure_collection(collection, vector_size)`
- Embed the schema description text: `embedder.embed([schema_text])`
- Construct `ChunkMetadata` with all Path A fields populated
- Construct `ChunkPayload` with deterministic ID
- Upsert: `vector_store.upsert_chunks(collection, [chunk])`
- Track point IDs in `WrittenArtifacts.vector_point_ids`
- Track collection in `WrittenArtifacts.vector_collection`

### Step 7: Optional row serialization

- **Condition**: Only if table row count < `config.row_serialization_limit` (default: 5000)
- Convert each row to a natural language sentence
- Format: `"In table '{table_name}', row {i}: {col1} is {val1}, {col2} is {val2}, ..."` (or similar)
- Embed in batches of `config.embedding_batch_size`
- Upsert as individual chunks, each with its own `ChunkMetadata` (incrementing `chunk_index`)
- Track all point IDs in `WrittenArtifacts.vector_point_ids`

---

## 6. Column Cleaning Rules -- Detailed

The cleaning logic for column names (Step 2) must also be applied to derive table names from sheet names (Step 4).

```python
import re

def clean_name(raw: str) -> str:
    """Clean a raw name (column or sheet) for use as a DB identifier."""
    name = raw.lower()
    name = re.sub(r'[^a-z0-9_]', '_', name)
    name = re.sub(r'_+', '_', name)
    name = name.strip('_')
    return name

def deduplicate_names(names: list[str]) -> list[str]:
    """Deduplicate a list of cleaned names by appending _1, _2, etc."""
    seen: dict[str, int] = {}
    result: list[str] = []
    for i, name in enumerate(names):
        if not name:
            name = f"column_{i}"
        if name in seen:
            seen[name] += 1
            result.append(f"{name}_{seen[name]}")
        else:
            seen[name] = 0
            result.append(name)
    return result
```

---

## 7. Date Parsing Rules -- Detailed

### Excel Serial Dates
- Excel stores dates as floating-point numbers: days since 1899-12-30
- Typical range for modern dates: ~40000 (2009) to ~50000 (2036)
- Detection heuristic: column is numeric, all values in a plausible serial date range
- Conversion: `pd.to_datetime(series, origin='1899-12-30', unit='D')`

### String Dates
- Text columns that contain date-like strings
- Detection: try `pd.to_datetime(series, infer_datetime_format=True, errors='coerce')`
- If >= 50% of non-null values successfully parse, convert the column
- Be conservative to avoid converting IDs or codes that happen to look like dates

### Important Notes
- Date detection should happen AFTER column cleaning but BEFORE writing to DB
- Only convert columns where there's high confidence (to avoid data corruption)
- Log date conversions at DEBUG level for traceability

---

## 8. Schema Description Format -- Detailed

The schema description is a natural language text chunk that will be embedded for semantic search. It serves as the bridge between SQL tables and the vector store -- when a user asks a question, the schema description helps the system identify which table to query.

### Format Template

```
Table "{table_name}" contains {row_count} rows with columns:
- {col_name} ({col_type}): {description}
- {col_name} ({col_type}): {description}
...
```

### Column Type Mapping

| pandas dtype | Schema type string |
|---|---|
| `int64`, `int32`, etc. | `"integer"` |
| `float64`, `float32`, etc. | `"float"` |
| `object` (string) | `"text"` |
| `datetime64[ns]` | `"date"` |
| `bool` | `"boolean"` |
| other | `"text"` (fallback) |

### Column Description Generation

- **Integer/Float columns**: `"range {min} to {max}"` -- e.g., `"range 10042 to 10943"`
- **Text columns (low cardinality, <20 unique)**: `"one of {val1}, {val2}, {val3}, ..."` -- e.g., `"one of Engineering, Sales, HR, Finance"`
- **Text columns (high cardinality, >=20 unique)**: `"{N} unique values"` -- e.g., `"342 unique values"`
- **Date columns**: `"ranges from {earliest} to {latest}"` -- e.g., `"ranges from 2018-01-15 to 2024-11-30"`
- **Boolean columns**: `"true/false"`

---

## 9. ChunkMetadata Assembly -- Exact Field Sources

For the **schema description chunk** (one per table):

| ChunkMetadata field | Value source |
|---|---|
| `source_uri` | Passed as parameter (from router, canonical file path, e.g. `"file:///path/to/file.xlsx"`) |
| `source_format` | `"xlsx"` (default) |
| `sheet_name` | `sheet_profile.name` |
| `region_id` | `None` (Path A does not use regions) |
| `ingestion_method` | `"sql_agent"` (the string VALUE of `IngestionMethod.SQL_AGENT`) |
| `parser_used` | `sheet_profile.parser_used.value` (e.g. `"openpyxl"`) |
| `parser_version` | `config.parser_version` (e.g. `"ingestkit_excel:1.0.0"`) |
| `chunk_index` | `0` for schema chunk; increments for row serialization chunks |
| `chunk_hash` | `hashlib.sha256(text.encode()).hexdigest()` |
| `ingest_key` | Passed as parameter (from `IngestKey.key` property, a SHA-256 hex string) |
| `ingest_run_id` | Passed as parameter (UUID4 string from router) |
| `tenant_id` | `config.tenant_id` |
| `table_name` | Cleaned table name derived from sheet name |
| `db_uri` | `structured_db.get_connection_uri()` |
| `row_count` | `len(df)` |
| `columns` | Cleaned column name list |
| `section_title` | `None` (Path B only) |
| `original_structure` | `None` (Path B only) |

### ChunkPayload.id Generation

The `id` field must be deterministic for idempotency:
```python
import hashlib
raw = f"{chunk_hash}:{ingest_key}"
chunk_id = hashlib.sha256(raw.encode()).hexdigest()
```

Or use `uuid5` with a namespace derived from `ingest_key`:
```python
import uuid
chunk_id = str(uuid.uuid5(uuid.NAMESPACE_URL, f"{ingest_key}:{chunk_hash}"))
```

---

## 10. Row Serialization -- Detailed

### When Triggered
- Only when `len(df) < config.row_serialization_limit` (default: 5000)
- This is a per-table decision (some tables may serialize, others may not)

### Format
Each row becomes a natural language sentence:
```
In table '{table_name}', row {row_number}: {col1} is {val1}, {col2} is {val2}, {col3} is {val3}.
```

Example:
```
In table 'employee_roster', row 1: employee_id is 10042, full_name is John Smith, department is Engineering, hire_date is 2020-03-15, salary is 95000.0.
```

### Chunking Strategy
- Each row is an individual chunk
- `chunk_index` continues from where schema description chunks left off
- Each row chunk gets its own `chunk_hash` based on its text content
- Embed in batches using `config.embedding_batch_size` (default: 64)

### ChunkMetadata for Row Chunks
- Same as schema description chunk EXCEPT:
  - `chunk_index`: incremented per row (starting after schema chunk)
  - `chunk_hash`: SHA-256 of the row text
  - `text`: the serialized row text

---

## 11. WrittenArtifacts Tracking -- For Rollback

The processor must carefully track everything it writes so the caller can roll back.

### What Gets Tracked

```python
written = WrittenArtifacts(
    vector_point_ids=[...],       # ALL chunk IDs upserted to vector store
    vector_collection="helpdesk", # config.default_collection
    db_table_names=[...],         # ALL table names created in structured DB
)
```

### Tracking Points
1. After `create_table_from_dataframe()` succeeds: append table_name to `db_table_names`
2. After `upsert_chunks()` succeeds: extend `vector_point_ids` with all chunk IDs
3. Set `vector_collection` to `config.default_collection`

### Rollback Contract (caller-side, not implemented in processor)
```python
# Caller uses WrittenArtifacts to undo:
vector_store.delete_by_ids(written.vector_collection, written.vector_point_ids)
for table in written.db_table_names:
    structured_db.drop_table(table)
```

---

## 12. Dependency Map -- All Imports Needed

### Standard Library
```python
from __future__ import annotations

import hashlib
import logging
import re
import time
import uuid
from typing import TYPE_CHECKING
```

### Third-Party
```python
import pandas as pd
```

### Internal
```python
from ingestkit_excel.config import ExcelProcessorConfig
from ingestkit_excel.errors import ErrorCode, IngestError
from ingestkit_excel.models import (
    ChunkMetadata,
    ChunkPayload,
    ClassificationResult,
    ClassificationStageResult,
    EmbedStageResult,
    FileProfile,
    IngestionMethod,
    ParseStageResult,
    ProcessingResult,
    WrittenArtifacts,
)
```

### TYPE_CHECKING Only (for Protocol references)
```python
if TYPE_CHECKING:
    from ingestkit_excel.protocols import (
        EmbeddingBackend,
        StructuredDBBackend,
        VectorStoreBackend,
    )
```

**Note**: Do NOT import protocols at runtime to avoid circular imports. The constructor takes them as parameters with runtime type hints.

### Actual Constructor Type Hints

Since protocols use `runtime_checkable`, you can import them directly (they are not in a circular path with the processor):

```python
from ingestkit_excel.protocols import (
    EmbeddingBackend,
    StructuredDBBackend,
    VectorStoreBackend,
)
```

The import chain is: `protocols.py` imports `ChunkPayload` only under `TYPE_CHECKING`, so there is NO circular import risk for the processor module.

---

## 13. Risk Flags

### ENUM_VALUE Risk

| Location | Risk | Prevention |
|---|---|---|
| `ChunkMetadata.ingestion_method` | Must be `"sql_agent"` not `"SQL_AGENT"` | Use `IngestionMethod.SQL_AGENT.value` or the literal string `"sql_agent"` |
| `ChunkMetadata.parser_used` | Must be `"openpyxl"` not `"OPENPYXL"` | Use `sheet_profile.parser_used.value` |
| `ProcessingResult.ingestion_method` | Must be `IngestionMethod.SQL_AGENT` (the enum member) | Use `IngestionMethod.SQL_AGENT` directly |
| `ProcessingResult.errors` / `warnings` | Must be ErrorCode VALUE strings | Use `ErrorCode.E_BACKEND_DB_TIMEOUT.value` |

### VERIFICATION_GAP Risk

| Assumption | Must Verify |
|---|---|
| SheetProfile has `header_row_index` field | VERIFIED: models.py line 157 -- `header_row_index: int \| None = None` |
| FileProfile has `content_hash` field | VERIFIED: models.py line 181 -- `content_hash: str` |
| ProcessingResult accepts `embed_result` as optional | VERIFIED: models.py line 263 -- `embed_result: EmbedStageResult \| None = None` |
| `StructuredDBBackend.get_connection_uri()` returns `str` | VERIFIED: protocols.py line 59 |
| `EmbeddingBackend.embed()` returns `list[list[float]]` | VERIFIED: protocols.py line 93-95 |
| `VectorStoreBackend.upsert_chunks()` returns `int` (count) | VERIFIED: protocols.py line 22 |
| `WrittenArtifacts.vector_point_ids` is `list[str]` | VERIFIED: models.py line 248 |
| `WrittenArtifacts.db_table_names` is `list[str]` | VERIFIED: models.py line 250 |
| `ProcessingResult` requires `parse_result` and `classification_result` | VERIFIED: models.py lines 261-262 -- both are required (no default) |
| Config field `row_serialization_limit` default is 5000 | VERIFIED: config.py line 44 |

### COMPONENT_API Risk

| Component | Must-Use Signature |
|---|---|
| `structured_db.create_table_from_dataframe(table_name: str, df: pd.DataFrame) -> None` | Exact |
| `structured_db.get_connection_uri() -> str` | Exact |
| `vector_store.ensure_collection(collection: str, vector_size: int) -> None` | Exact |
| `vector_store.upsert_chunks(collection: str, chunks: list[ChunkPayload]) -> int` | Exact |
| `embedder.embed(texts: list[str], timeout: float \| None = None) -> list[list[float]]` | Exact |
| `embedder.dimension() -> int` | Exact |

---

## 14. Public Interface (from SPEC)

```python
class StructuredDBProcessor:
    def __init__(
        self,
        structured_db: StructuredDBBackend,
        vector_store: VectorStoreBackend,
        embedder: EmbeddingBackend,
        config: ExcelProcessorConfig,
    ) -> None: ...

    def process(
        self,
        file_path: str,
        profile: FileProfile,
        ingest_key: str,
        ingest_run_id: str,
    ) -> ProcessingResult: ...
```

**Key observations:**
- The processor does NOT receive `ParseStageResult` or `ClassificationStageResult` -- these must be reconstructed or passed differently. Looking at SPEC section 13.1 (router flow), the router assembles the final `ProcessingResult`. The processor may return a partial result or a tuple of its outputs.
- **DECISION**: The processor should return a `ProcessingResult` fully assembled. The `parse_result` and `classification_result` fields will need to be passed in OR the processor builds a partial result and the router fills in the rest. Given the SPEC shows `process()` returning `ProcessingResult`, the processor must accept enough info to build the full result, OR the caller (router) augments it after.
- **Recommended approach**: The processor's `process()` returns `ProcessingResult`. For `parse_result` and `classification_result`, either:
  1. Accept them as additional parameters, OR
  2. Return a simpler intermediate result that the router wraps

Looking at the SPEC example signature: `process(self, file_path, profile, ingest_key, ingest_run_id) -> ProcessingResult`, it does NOT include parse/classification results. This means the processor either:
- Has access to them via some other mechanism, OR
- Only populates the fields it owns and the router fills in the rest

**Best approach**: Accept `classification` and `parse_result` as additional parameters beyond the SPEC signature, since `ProcessingResult` requires them. Alternatively, make them part of a context object. The simplest deviation: add `classification_result` and `parse_result` params.

---

## 15. Test Patterns (from conftest.py and test_inspector.py)

### Fixture Style
- Use `@pytest.fixture()` with explicit parens
- Return typed objects
- Use helper functions like `_make_sheet_profile(**overrides)` and `_make_file_profile(sheets, **overrides)`

### Helper Pattern
```python
def _make_sheet_profile(**overrides: object) -> SheetProfile:
    defaults: dict = dict(
        name="Sheet1",
        row_count=100,
        col_count=5,
        # ... all fields with sensible defaults
    )
    defaults.update(overrides)
    return SheetProfile(**defaults)
```

### Test Style
- Tests grouped by functionality with section headers using `# ---------------------------------------------------------------------------`
- Docstrings on test files and test classes
- Import only what's needed from `ingestkit_excel.models`
- No mock framework visible in conftest -- just plain fixtures

---

## 16. Files to Create

### `src/ingestkit_excel/processors/__init__.py` (NEW)
- Empty or with re-exports of `StructuredDBProcessor`

### `src/ingestkit_excel/processors/structured_db.py` (NEW)
- The main processor class
- All 7 steps implemented
- Helper functions for column cleaning, date detection, schema description generation, row serialization

### `tests/test_structured_db.py` (NEW, if tests are part of this issue)
- Tests for column cleaning, date parsing, schema description, row serialization, full process flow
- Use mock backends

---

## 17. Process Flow Summary

```
process(file_path, profile, ingest_key, ingest_run_id)
    |
    +-- for each sheet in profile.sheets:
    |       |
    |       +-- Step 1: pd.read_excel(file_path, sheet_name=sheet.name)
    |       +-- Step 2: clean_column_names(df) if config.clean_column_names
    |       +-- Step 3: auto_detect_dates(df)
    |       +-- Step 4: derive table_name, structured_db.create_table_from_dataframe(table_name, df)
    |       +-- Step 5: generate schema_description(table_name, df)
    |       +-- Step 6: embed schema, build ChunkPayload, upsert to vector store
    |       +-- Step 7: if len(df) < row_serialization_limit:
    |               +-- serialize rows, embed in batches, upsert to vector store
    |
    +-- Assemble ProcessingResult with WrittenArtifacts
    +-- Return
```

---

## 18. source_uri Convention

From `idempotency.py` line 67-68:
```python
if source_uri is None:
    source_uri = Path(file_path).resolve().as_posix()
```

The processor receives `file_path` as a parameter. For `ChunkMetadata.source_uri`, use the same pattern: resolve the path to an absolute POSIX path. The SPEC example shows `"file:///path/to/roster.xlsx"` format. The idempotency module uses just the POSIX path without the `file://` prefix. Need to decide on convention -- follow what the router passes (likely `file_path` resolved to absolute).

Looking at the SPEC section 10.1 example:
```python
source_uri="file:///path/to/roster.xlsx"
```

The `source_uri` in ChunkMetadata should use the `file://` protocol prefix for local files. The processor should accept `source_uri` as a parameter OR derive it from `file_path` with the `file://` prefix.

**Recommended**: Accept `source_uri` as an optional parameter to `process()`. If not provided, derive as `f"file://{Path(file_path).resolve().as_posix()}"`. The router will likely pass it from the ingest key computation.

---

## 19. Embedding Batching Strategy

When embedding multiple texts (schema descriptions + optional row serializations):

1. Collect all texts to embed
2. Split into batches of `config.embedding_batch_size` (default: 64)
3. Call `embedder.embed(batch, timeout=config.backend_timeout_seconds)` per batch
4. Reassemble vectors in order
5. Track total `texts_embedded` for `EmbedStageResult`
6. Track total `embed_duration_seconds`

---

## 20. Error Handling Strategy

The processor should:
1. Catch backend exceptions and wrap them in `IngestError` with appropriate `ErrorCode`
2. For non-fatal errors (e.g., one sheet fails but others succeed), continue processing and accumulate errors
3. For fatal errors (e.g., all DB writes fail), stop and return result with errors
4. Use `stage="process"` for processing errors and `stage="embed"` for embedding errors
5. `E_PROCESS_SCHEMA_GEN` if schema description generation fails
6. Backend errors (`E_BACKEND_*`) if backend calls fail after retries
