---
issue: 40
agent: PLAN
date: 2026-02-14
complexity: COMPLEX
stack: backend
---

# PLAN Artifact: Issue #40 -- Table Extractor

## Executive Summary

Issue #40 implements `TableExtractor` in `processors/table_extractor.py`, providing pdfplumber-based table detection, pandas DataFrame extraction, dual routing (NL serialization for small tables, StructuredDB for large tables), and multi-page table stitching via column count + header similarity. The implementation follows the proven Excel `StructuredDBProcessor` pattern for schema description generation, row NL serialization, chunk ID generation, and batch embedding. Four files are touched: one new module, one new test file, and two edits (processors `__init__.py` and `conftest.py` for mock backends).

## File-by-File Implementation

---

### 1. `packages/ingestkit-pdf/src/ingestkit_pdf/processors/table_extractor.py` (~350 lines)

#### Imports

```python
from __future__ import annotations

import hashlib
import logging
import re
import uuid
from difflib import SequenceMatcher
from pathlib import Path

import pandas as pd
import pdfplumber

from ingestkit_core.models import (
    BaseChunkMetadata,
    ChunkPayload,
    EmbedStageResult,
    WrittenArtifacts,
)
from ingestkit_pdf.config import PDFProcessorConfig
from ingestkit_pdf.errors import ErrorCode, IngestError
from ingestkit_pdf.models import (
    ContentType,
    IngestionMethod,
    PDFChunkMetadata,
    TableResult,
)
from ingestkit_pdf.protocols import (
    EmbeddingBackend,
    StructuredDBBackend,
    VectorStoreBackend,
)
```

Logger: `logger = logging.getLogger("ingestkit_pdf")`

#### 1a. `TableExtractionResult` Pydantic Model

Define in-module (not in models.py) since it is an internal result container used only by this module and the future `ComplexProcessor`.

```python
from pydantic import BaseModel

class TableExtractionResult(BaseModel):
    """Result container for table extraction from a PDF."""
    tables: list[TableResult] = []
    chunks: list[ChunkPayload] = []
    table_names: list[str] = []           # DB table names written (large tables only)
    warnings: list[str] = []
    errors: list[IngestError] = []
    texts_embedded: int = 0
    embed_duration_seconds: float = 0.0
```

Fields `texts_embedded` and `embed_duration_seconds` enable the caller (`ComplexProcessor`) to aggregate into a document-level `EmbedStageResult`.

#### 1b. Module-Level Helpers

Reuse the `clean_name()` and `deduplicate_names()` pattern from Excel `structured_db.py` (see `structured_db.py:43-75`). Copy these two functions into this module rather than extracting to core -- they are small (~30 lines) and the PDF variant may diverge (e.g., table naming uses `pdf_` prefix).

```python
def clean_name(raw: str) -> str:
    """Same logic as Excel structured_db.clean_name -- see structured_db.py:43-56."""
    name = raw.lower()
    name = re.sub(r"[^a-z0-9_]", "_", name)
    name = re.sub(r"_+", "_", name)
    return name.strip("_")

def deduplicate_names(names: list[str]) -> list[str]:
    """Same logic as Excel structured_db.deduplicate_names -- see structured_db.py:59-75."""
    seen: dict[str, int] = {}
    result: list[str] = []
    for i, name in enumerate(names):
        if not name:
            name = f"column_{i}"
        if name in seen:
            seen[name] += 1
            result.append(f"{name}_{seen[name]}")
        else:
            seen[name] = 0
            result.append(name)
    return result
```

#### 1c. `TableExtractor` Class

```python
class TableExtractor:
    """Extracts tables from PDF pages using pdfplumber.

    Handles dual routing (NL serialization vs StructuredDB), multi-page
    table stitching, and chunk embedding with table metadata.
    """

    def __init__(
        self,
        config: PDFProcessorConfig,
        structured_db: StructuredDBBackend | None = None,
        vector_store: VectorStoreBackend | None = None,
        embedder: EmbeddingBackend | None = None,
    ) -> None:
        self._config = config
        self._db = structured_db
        self._vector_store = vector_store
        self._embedder = embedder
```

Backends are optional because:
- Small tables (NL serialization) need `embedder` + `vector_store` but NOT `structured_db`.
- Large tables need all three.
- Pure extraction (no DB/embedding) is valid -- caller may pass None.

#### 1d. `extract_tables()` -- Public Entry Point

```python
def extract_tables(
    self,
    file_path: str,
    page_numbers: list[int],
    ingest_key: str,
    ingest_run_id: str,
) -> TableExtractionResult:
```

**Algorithm:**

1. Open PDF via `pdfplumber.open(file_path)`.
2. For each page number in `page_numbers`:
   a. Access `pdf.pages[page_number - 1]` (pdfplumber is 0-indexed, our page numbers are 1-indexed).
   b. Call `page.extract_tables()` which returns `list[list[list[str | None]]]`.
   c. For each raw table:
      - If no rows after header row, skip (empty table).
      - Convert to DataFrame: `pd.DataFrame(table_data[1:], columns=table_data[0])`.
      - Handle None/empty headers: replace with `column_0`, `column_1`, etc.
      - Clean column names via `clean_name()` + `deduplicate_names()`.
      - Build a `_RawTable` namedtuple/dataclass: `(page_number, table_index, df, headers)`.
3. Run multi-page stitching via `_stitch_tables()` (step 1e below).
4. For each final table (stitched or standalone):
   - Route via `_route_table()` (step 1f below).
5. Return assembled `TableExtractionResult`.

Wrap each per-page extraction in try/except. On failure: log `E_PROCESS_TABLE_EXTRACT`, record error, continue to next page.

#### 1e. `_stitch_tables()` -- Multi-Page Table Stitching

```python
def _stitch_tables(
    self,
    raw_tables: list[_RawTable],
) -> list[_FinalTable]:
```

**Internal data structures:**

```python
from dataclasses import dataclass

@dataclass
class _RawTable:
    page_number: int
    table_index: int
    df: pd.DataFrame
    headers: list[str]

@dataclass
class _FinalTable:
    page_numbers: list[int]
    table_index: int          # index of the first table in the group
    df: pd.DataFrame
    headers: list[str]
    is_continuation: bool
    continuation_group_id: str | None
```

**Algorithm:**

1. Group raw tables by page, preserving order.
2. Iterate pages in order. For consecutive page pairs (N, N+1):
   - Get the LAST table on page N and the FIRST table on page N+1.
   - Check: `last.df.shape[1] == first.df.shape[1]` (column count match).
   - Check: `SequenceMatcher(None, last.headers, first.headers).ratio() >= config.table_continuation_column_match_threshold`.
   - If both match:
     - Concatenate DataFrames: skip the first row of the continuation table if it matches the header (repeated header detection).
     - Assign a shared `continuation_group_id = str(uuid.uuid4())`.
     - Mark the continuation table with `is_continuation=True`.
     - Merge page numbers into a single list.
     - Record `W_TABLE_CONTINUATION` warning.
   - If no match: finalize the current table group, start a new one.
3. All non-continuation tables become standalone `_FinalTable` entries.

**Header repetition detection for continuation tables:**
Compare the first data row of the continuation table against the header list. If all values match (case-insensitive), skip that row when concatenating.

```python
first_row = continuation_df.iloc[0].astype(str).tolist()
if [v.lower().strip() for v in first_row] == [h.lower().strip() for h in base_headers]:
    continuation_df = continuation_df.iloc[1:]
```

#### 1f. `_route_table()` -- Dual Routing Logic

```python
def _route_table(
    self,
    table: _FinalTable,
    ingest_key: str,
    ingest_run_id: str,
    source_uri: str,
    chunk_index_start: int,
) -> tuple[list[ChunkPayload], list[str], int]:
    """Route a table to NL serialization or StructuredDB.

    Returns: (chunks, db_table_names, next_chunk_index)
    """
```

**Decision boundary** (from SPEC and config.py):
- `len(df) <= config.table_max_rows_for_serialization` (default 20): NL serialization path.
- `len(df) > config.table_min_rows_for_db` (default 20): StructuredDB path.
- Boundary is clean: exactly 20 rows goes to NL serialization; 21+ goes to DB.

**NL Serialization Path** (small tables, <= 20 rows):
1. For each row, serialize as: `"In table '{table_name}', row {N}: {col} is {val}, ..."` (same format as Excel `structured_db.py:544`).
2. Handle NaN values: `"{col} is N/A"`.
3. Generate chunk ID: `uuid5(NAMESPACE_URL, "{ingest_key}:{chunk_hash}")` where `chunk_hash = sha256(text)`.
4. Build `PDFChunkMetadata` with:
   - `content_type = ContentType.TABLE.value` ("table")
   - `table_index = table.table_index`
   - `page_numbers = table.page_numbers`
   - `ingestion_method = IngestionMethod.COMPLEX_PROCESSING.value`
   - `source_format = "pdf"`
   - `ingest_run_id`, `ingest_key`, `tenant_id` from config
5. If `embedder` and `vector_store` are available:
   - Embed all row texts in batches of `config.embedding_batch_size`.
   - Upsert to vector store.
6. Return chunks list + empty db_table_names.

**StructuredDB Path** (large tables, > 20 rows):
1. Generate table name: `pdf_{ingest_key[:8]}_p{page}_t{table_index}`.
   - Apply `clean_name()` and deduplicate against already-used names.
2. If `structured_db` is available:
   - Call `self._db.create_table_from_dataframe(table_name, df)`.
3. Generate schema description via `_generate_schema_description()` (same pattern as Excel `structured_db.py:449-468`).
4. Generate one chunk for the schema description.
5. If `embedder` and `vector_store` are available:
   - Embed schema text.
   - Upsert to vector store.
6. Return chunks list + [table_name].

#### 1g. `_generate_schema_description()` -- Schema Text for Embedding

Reuse the exact same pattern as Excel `structured_db.py:449-510`:

```python
def _generate_schema_description(self, table_name: str, df: pd.DataFrame) -> str:
    lines = [f'Table "{table_name}" contains {len(df)} rows with columns:']
    for col in df.columns:
        series = df[col].dropna()
        col_type = self._infer_type_label(df[col])
        desc = self._describe_column(series, col_type)
        lines.append(f"- {col} ({col_type}): {desc}")
    return "\n".join(lines)
```

Include the static helpers `_infer_type_label()` and `_describe_column()` -- same logic as Excel `structured_db.py:470-510`.

#### 1h. `_build_table_result()` -- TableResult Model Population

```python
def _build_table_result(self, table: _FinalTable) -> TableResult:
    return TableResult(
        page_number=table.page_numbers[0],
        table_index=table.table_index,
        row_count=len(table.df),
        col_count=table.df.shape[1],
        headers=table.headers,
        is_continuation=table.is_continuation,
        continuation_group_id=table.continuation_group_id,
    )
```

#### 1i. Error Classification

```python
@staticmethod
def _classify_backend_error(exc: Exception) -> ErrorCode:
```

Same pattern as Excel `structured_db.py:584-605`, but maps to PDF error codes (`E_BACKEND_*` codes from `ingestkit_pdf.errors`).

---

### 2. `packages/ingestkit-pdf/tests/test_table_extractor.py` (~300 lines)

#### Test Structure

All tests are `@pytest.mark.unit`. All pdfplumber usage is mocked via `unittest.mock.patch`.

#### 2a. Fixtures (local to test file)

```python
@pytest.fixture
def config():
    return PDFProcessorConfig()

@pytest.fixture
def mock_structured_db():
    return MockStructuredDBBackend()

@pytest.fixture
def mock_vector_store():
    return MockVectorStoreBackend()

@pytest.fixture
def mock_embedder():
    return MockEmbeddingBackend()

@pytest.fixture
def extractor(config, mock_structured_db, mock_vector_store, mock_embedder):
    return TableExtractor(
        config=config,
        structured_db=mock_structured_db,
        vector_store=mock_vector_store,
        embedder=mock_embedder,
    )
```

#### 2b. Test Cases (R-PC-2: Table Extraction)

| Test | Description |
|------|-------------|
| `test_single_table_small_nl_serialization` | Table with 5 rows on one page -> NL serialization path. Verify 5 row chunks created with `content_type="table"`. |
| `test_single_table_large_db_routing` | Table with 25 rows -> StructuredDB path. Verify `create_table_from_dataframe` called, 1 schema chunk created. |
| `test_boundary_20_rows_nl_path` | Exactly 20 rows -> NL serialization (boundary condition). |
| `test_boundary_21_rows_db_path` | Exactly 21 rows -> StructuredDB (boundary condition). |
| `test_empty_table_skipped` | Table with header only (0 data rows) -> skipped, no chunks. |
| `test_multiple_tables_single_page` | Page with 2 tables -> both extracted with correct `table_index` (0, 1). |
| `test_multiple_pages_no_stitching` | Tables on different pages with different column counts -> no stitching. |
| `test_none_headers_fallback` | pdfplumber returns None in header cells -> replaced with `column_N`. |
| `test_extraction_error_per_table` | pdfplumber raises on one table -> error recorded, other tables still processed. |
| `test_no_backends_pure_extraction` | All backends None -> tables extracted, TableResult populated, no chunks/DB writes. |

#### 2c. Test Cases (R-PC-3: Multi-Page Table Stitching)

| Test | Description |
|------|-------------|
| `test_continuation_stitching_basic` | Same headers on page 1 last table and page 2 first table -> stitched. Verify `is_continuation=True`, shared `continuation_group_id`, `W_TABLE_CONTINUATION` warning. |
| `test_continuation_skip_repeated_header` | Continuation table starts with repeated header row -> that row is skipped in concat. Row count is correct. |
| `test_continuation_below_threshold` | Header similarity at 0.7 (below 0.8 threshold) -> NOT stitched. Two separate tables. |
| `test_continuation_column_count_mismatch` | Different column counts -> NOT stitched even if header text is similar. |
| `test_continuation_three_pages` | Table spans pages 1, 2, 3 -> all stitched into one. Single `continuation_group_id`. |
| `test_stitched_table_routing` | Stitched table with 30 total rows (15 per page) -> DB path. Page numbers list contains both pages. |

#### 2d. Mock pdfplumber Pattern

```python
from unittest.mock import MagicMock, patch

def _make_mock_pdf(pages_tables: dict[int, list[list[list[str | None]]]]):
    """Build a mock pdfplumber PDF object.

    Args:
        pages_tables: {page_index_0based: [table1_data, table2_data, ...]}
            Each table_data is list-of-lists (first row = headers).
    """
    mock_pdf = MagicMock()
    mock_pages = []
    for i in range(max(pages_tables.keys()) + 1):
        page = MagicMock()
        page.extract_tables.return_value = pages_tables.get(i, [])
        mock_pages.append(page)
    mock_pdf.pages = mock_pages
    mock_pdf.__enter__ = MagicMock(return_value=mock_pdf)
    mock_pdf.__exit__ = MagicMock(return_value=False)
    return mock_pdf
```

Usage in tests:
```python
@patch("ingestkit_pdf.processors.table_extractor.pdfplumber")
def test_single_table_small_nl_serialization(mock_plumber, extractor):
    table_data = [["Name", "Age"], ["Alice", "30"], ["Bob", "25"]]
    mock_plumber.open.return_value = _make_mock_pdf({0: [table_data]})

    result = extractor.extract_tables(
        file_path="/tmp/test.pdf",
        page_numbers=[1],
        ingest_key="abc123",
        ingest_run_id="run-001",
    )

    assert len(result.tables) == 1
    assert result.tables[0].row_count == 2
    assert len(result.chunks) == 2  # one per row
    assert result.chunks[0].metadata.content_type == "table"
```

---

### 3. `packages/ingestkit-pdf/src/ingestkit_pdf/processors/__init__.py` (edit)

Add export for `TableExtractor` and `TableExtractionResult`:

```python
from ingestkit_pdf.processors.table_extractor import (
    TableExtractionResult,
    TableExtractor,
)

__all__ = ["TableExtractor", "TableExtractionResult"]
```

---

### 4. `packages/ingestkit-pdf/tests/conftest.py` (edit)

Add three mock backends needed for table extraction tests. These will also be reused by future processor tests (ComplexProcessor, etc.).

#### 4a. `MockStructuredDBBackend`

```python
class MockStructuredDBBackend:
    """Mock structured DB backend for testing."""

    def __init__(self) -> None:
        self.tables: dict[str, Any] = {}   # table_name -> DataFrame
        self.dropped: list[str] = []

    def create_table_from_dataframe(self, table_name: str, df: Any) -> None:
        self.tables[table_name] = df

    def drop_table(self, table_name: str) -> None:
        self.tables.pop(table_name, None)
        self.dropped.append(table_name)

    def table_exists(self, table_name: str) -> bool:
        return table_name in self.tables

    def get_table_schema(self, table_name: str) -> dict:
        if table_name not in self.tables:
            return {}
        df = self.tables[table_name]
        return {col: str(df[col].dtype) for col in df.columns}

    def get_connection_uri(self) -> str:
        return "sqlite:///:memory:"
```

#### 4b. `MockVectorStoreBackend`

```python
class MockVectorStoreBackend:
    """Mock vector store backend for testing."""

    def __init__(self) -> None:
        self.collections: dict[str, int] = {}      # collection -> vector_size
        self.chunks: list[Any] = []                 # all upserted chunks
        self.indexes: list[tuple[str, str, str]] = []

    def upsert_chunks(self, collection: str, chunks: list[Any]) -> int:
        self.chunks.extend(chunks)
        return len(chunks)

    def ensure_collection(self, collection: str, vector_size: int) -> None:
        self.collections[collection] = vector_size

    def create_payload_index(self, collection: str, field: str, field_type: str) -> None:
        self.indexes.append((collection, field, field_type))

    def delete_by_ids(self, collection: str, ids: list[str]) -> int:
        return 0
```

#### 4c. `MockEmbeddingBackend`

```python
class MockEmbeddingBackend:
    """Mock embedding backend for testing."""

    def __init__(self, dim: int = 768) -> None:
        self._dim = dim
        self.calls: list[list[str]] = []

    def embed(self, texts: list[str], timeout: float | None = None) -> list[list[float]]:
        self.calls.append(texts)
        return [[0.1] * self._dim for _ in texts]

    def dimension(self) -> int:
        return self._dim
```

#### 4d. Pytest Fixtures

Add fixtures for each mock backend:

```python
@pytest.fixture()
def mock_structured_db() -> MockStructuredDBBackend:
    return MockStructuredDBBackend()

@pytest.fixture()
def mock_vector_store() -> MockVectorStoreBackend:
    return MockVectorStoreBackend()

@pytest.fixture()
def mock_embedder() -> MockEmbeddingBackend:
    return MockEmbeddingBackend()
```

---

## Acceptance Criteria

- [ ] `TableExtractor` class with `extract_tables()` method exists at `processors/table_extractor.py`
- [ ] pdfplumber table detection on each page, converting raw table data to pandas DataFrames
- [ ] Null/empty headers replaced with `column_N` fallback names
- [ ] Dual routing: tables <= 20 rows go to NL serialization, tables > 20 rows go to StructuredDB
- [ ] NL serialization format: `"In table '{name}', row {N}: {col} is {val}, ..."` (matches Excel pattern)
- [ ] StructuredDB path: `create_table_from_dataframe()` called, schema description embedded as single chunk
- [ ] Multi-page table stitching: column count match + header similarity >= 0.8 threshold
- [ ] Stitched tables: `is_continuation=True`, shared `continuation_group_id` (UUID), `W_TABLE_CONTINUATION` warning
- [ ] Repeated header row on continuation page detected and skipped
- [ ] `TableResult` model populated for each extracted table
- [ ] Chunks tagged with `content_type="table"`, `table_index`, `page_numbers`
- [ ] Per-table error handling: one table failing does not block others (`E_PROCESS_TABLE_EXTRACT`)
- [ ] All backends optional (None-safe): pure extraction works without DB/embedding
- [ ] Mock backends added to conftest.py (MockStructuredDBBackend, MockVectorStoreBackend, MockEmbeddingBackend)
- [ ] All tests pass with `pytest -m unit`
- [ ] No regressions in existing tests

## Verification Gates

```bash
# Unit tests pass
pytest packages/ingestkit-pdf/tests/test_table_extractor.py -v -m unit

# No regressions
pytest packages/ingestkit-pdf/tests/ -v

# Import check
python -c "from ingestkit_pdf.processors import TableExtractor, TableExtractionResult; print('OK')"
```

AGENT_RETURN: .agents/outputs/plan-40-021426.md
