---
issue: 9
title: "Implement Path B text serializer (Backend)"
agent: map
timestamp: 2026-02-12
status: complete
branch: feature/issue-9-path-b-serializer
complexity: COMPLEX
stack: backend
files_to_create:
  - packages/ingestkit-excel/src/ingestkit_excel/processors/serializer.py
  - packages/ingestkit-excel/tests/test_serializer.py
files_to_modify:
  - packages/ingestkit-excel/src/ingestkit_excel/processors/__init__.py
  - packages/ingestkit-excel/src/ingestkit_excel/__init__.py
dependencies:
  - "issue #3 (models, protocols, config) -- implemented"
  - "issue #8 (Path A StructuredDBProcessor) -- implemented"
---

# MAP: Issue #9 -- Path B Text Serializer

## 1. Spec Requirements (SPEC.md Section 10.2)

### 1.1 Exact Spec Text

**Location:** `packages/ingestkit-excel/SPEC.md`, lines 745-783

**Input:** FileProfile + classification confirming Type B.

**Steps (verbatim from spec):**

1. Parse with openpyxl, preserving merged cell structure.
2. Detect logical sections: look for merged header rows, blank row separators, indentation patterns.
3. For each section, determine its sub-structure:
   - **Small table** -> serialize rows as sentences.
   - **Checklist** -> "Item X: status is Y, due date is Z, responsible party is W."
   - **Matrix** -> serialize with row/column header context.
   - **Free text** -> extract as-is, preserving paragraph breaks.
4. Embed each section/chunk via `EmbeddingBackend`.
5. Upsert to `VectorStoreBackend` with standardized `ChunkMetadata`.

**ChunkMetadata example from spec (line 759-774):**

```python
ChunkMetadata(
    source_uri="file:///path/to/onboarding.xlsx",
    sheet_name="Onboarding Checklist",
    ingestion_method="text_serialization",
    parser_used="openpyxl",
    parser_version="ingestkit_excel:1.0.0",
    chunk_index=3,
    chunk_hash="sha256:...",
    ingest_key="...",
    ingest_run_id="...",
    tenant_id="...",
    section_title="IT Setup Requirements",
    original_structure="checklist",
)
```

**Public interface (spec line 778-783):**

```python
class TextSerializer:
    def __init__(self, vector_store: VectorStoreBackend, embedder: EmbeddingBackend,
                 config: ExcelProcessorConfig): ...
    def process(self, file_path: str, profile: FileProfile,
                ingest_key: str, ingest_run_id: str) -> ProcessingResult: ...
```

### 1.2 ChunkMetadata Path B-Specific Fields

From `models.py` lines 218-219 and SPEC line 303-304:

- `section_title: str | None = None` -- title of the detected logical section (e.g., "IT Setup Requirements")
- `original_structure: str | None = None` -- one of: `"table"`, `"checklist"`, `"matrix"`, `"free_text"`

### 1.3 Error Code

From `errors.py` line 43 and SPEC line 398:

- `E_PROCESS_SERIALIZE = "E_PROCESS_SERIALIZE"` -- text serialization failed

### 1.4 IngestionMethod

From `models.py` line 55:

- `IngestionMethod.TEXT_SERIALIZATION = "text_serialization"` -- Path B

### 1.5 Test Coverage Requirements

From SPEC line 1096:

> `serializer.py` | Merged cell handling; section detection; checklist/matrix/free-text serialization; ChunkMetadata correctness

## 2. Existing Processor Pattern Analysis (Path A)

### 2.1 Class Structure

**File:** `packages/ingestkit-excel/src/ingestkit_excel/processors/structured_db.py`

```python
class StructuredDBProcessor:
    def __init__(
        self,
        structured_db: StructuredDBBackend,  # Path A specific
        vector_store: VectorStoreBackend,
        embedder: EmbeddingBackend,
        config: ExcelProcessorConfig,
    ) -> None:
        self._db = structured_db
        self._vector_store = vector_store
        self._embedder = embedder
        self._config = config
```

### 2.2 Process Method Signature

**SPEC says** (line 741-742):
```python
def process(self, file_path: str, profile: FileProfile,
            ingest_key: str, ingest_run_id: str) -> ProcessingResult: ...
```

**Actual Path A implementation** (line 105-113):
```python
def process(
    self,
    file_path: str,
    profile: FileProfile,
    ingest_key: str,
    ingest_run_id: str,
    parse_result: ParseStageResult,
    classification_result: ClassificationStageResult,
    classification: ClassificationResult,
) -> ProcessingResult:
```

**RECONCILIATION NEEDED:** The actual Path A implementation takes 3 additional parameters (`parse_result`, `classification_result`, `classification`) beyond what the spec defines. This is because `ProcessingResult` requires these fields. The TextSerializer must follow the same pattern as the actual Path A implementation (not the spec's simplified signature) to produce a valid `ProcessingResult`.

### 2.3 Process Method Flow Pattern

1. Record `start_time = time.monotonic()`
2. Compute `source_uri = f"file://{Path(file_path).resolve().as_posix()}"`
3. Initialize empty lists: `errors`, `warnings`, `error_details`
4. Initialize `WrittenArtifacts(vector_collection=collection)`
5. Call `self._vector_store.ensure_collection(collection, vector_size)`
6. Maintain a `chunk_index_counter = 0` (global across all sheets)
7. Loop over `profile.sheets`:
   - Skip hidden sheets -> `W_SHEET_SKIPPED_HIDDEN`
   - Skip chart-only sheets -> `W_SHEET_SKIPPED_CHART`
   - Skip oversized sheets -> `W_ROWS_TRUNCATED`
   - Try/except per-sheet processing (continue on error)
   - On error: classify error, append to errors/error_details, log, continue
8. Build `EmbedStageResult` if anything was embedded
9. Calculate elapsed time
10. Return `ProcessingResult` with all fields populated

### 2.4 Error Handling Pattern

```python
except Exception as exc:
    error_code = self._classify_backend_error(exc)
    errors.append(error_code.value)
    error_details.append(
        IngestError(
            code=error_code,
            message=str(exc),
            sheet_name=sheet.name,
            stage="process",
            recoverable=False,
        )
    )
    logger.exception("Error processing sheet %s: %s", sheet.name, exc)
    continue
```

### 2.5 Chunk ID Generation Pattern

Deterministic UUID5 from ingest_key + chunk_hash:

```python
chunk_hash = hashlib.sha256(text.encode()).hexdigest()
chunk_id = str(uuid.uuid5(uuid.NAMESPACE_URL, f"{ingest_key}:{chunk_hash}"))
```

### 2.6 Embedding Batch Pattern

```python
for batch_start in range(0, len(chunks), config.embedding_batch_size):
    batch = chunks[batch_start : batch_start + config.embedding_batch_size]
    texts = [c.text for c in batch]
    vectors = self._embedder.embed(texts, timeout=config.backend_timeout_seconds)
    for c, vec in zip(batch, vectors):
        c.vector = vec
    self._vector_store.upsert_chunks(collection, list(batch))
    for c in batch:
        written.vector_point_ids.append(c.id)
```

### 2.7 Backend Error Classification (Reusable)

The `_classify_backend_error` static method in Path A inspects exception messages for timeout/connection patterns. This should be reused or extracted as a shared utility.

## 3. Models Available

### 3.1 Models from `models.py`

| Model | Purpose | Key Fields |
|-------|---------|-----------|
| `ProcessingResult` | Final output | file_path, ingest_key, ingest_run_id, tenant_id, parse_result, classification_result, embed_result, classification, ingestion_method, chunks_created, tables_created, tables, written, errors, warnings, error_details, processing_time_seconds |
| `ChunkMetadata` | Per-chunk metadata | source_uri, sheet_name, ingestion_method, parser_used, parser_version, chunk_index, chunk_hash, ingest_key, ingest_run_id, tenant_id, **section_title**, **original_structure** |
| `ChunkPayload` | Chunk for upsert | id, text, vector, metadata |
| `WrittenArtifacts` | Rollback IDs | vector_point_ids, vector_collection, db_table_names |
| `FileProfile` | Input profile | file_path, sheets (list[SheetProfile]), etc. |
| `SheetProfile` | Per-sheet profile | name, row_count, col_count, merged_cell_count, merged_cell_ratio, header_row_detected, header_row_index, header_values, is_hidden, parser_used, sample_rows, etc. |
| `EmbedStageResult` | Embed stage output | texts_embedded, embedding_dimension, embed_duration_seconds |
| `ParseStageResult` | Parse stage output | parser_used, sheets_parsed, sheets_skipped, etc. |
| `ClassificationStageResult` | Classification output | tier_used, file_type, confidence, etc. |
| `ClassificationResult` | Simplified classification | file_type, confidence, tier_used, reasoning |
| `IngestionMethod` | Enum | TEXT_SERIALIZATION = "text_serialization" |

### 3.2 Protocols

| Protocol | Methods | Used By Path B |
|----------|---------|----------------|
| `VectorStoreBackend` | `upsert_chunks(collection, chunks)`, `ensure_collection(collection, vector_size)`, `create_payload_index(collection, field, field_type)`, `delete_by_ids(collection, ids)` | Yes -- upsert chunks |
| `EmbeddingBackend` | `embed(texts, timeout)`, `dimension()` | Yes -- embed section chunks |
| `StructuredDBBackend` | `create_table_from_dataframe`, etc. | NO -- Path B does not write to structured DB |
| `LLMBackend` | `classify`, `generate` | NO -- Path B does not use LLM |

### 3.3 Error Codes

| Code | When |
|------|------|
| `E_PROCESS_SERIALIZE` | Text serialization failed for a section/sheet |
| `E_BACKEND_VECTOR_TIMEOUT` | Vector store timeout |
| `E_BACKEND_VECTOR_CONNECT` | Vector store connection error |
| `E_BACKEND_EMBED_TIMEOUT` | Embedding timeout |
| `E_BACKEND_EMBED_CONNECT` | Embedding connection error |
| `W_SHEET_SKIPPED_HIDDEN` | Hidden sheet skipped |
| `W_SHEET_SKIPPED_CHART` | Chart-only sheet skipped |
| `W_ROWS_TRUNCATED` | Sheet exceeds max_rows_in_memory |

### 3.4 Config Parameters

| Parameter | Default | Relevance |
|-----------|---------|-----------|
| `parser_version` | `"ingestkit_excel:1.0.0"` | Goes into ChunkMetadata |
| `tenant_id` | `None` | Propagated to ChunkMetadata |
| `embedding_model` | `"nomic-embed-text"` | Embedding model reference |
| `embedding_dimension` | `768` | Vector size for collection |
| `embedding_batch_size` | `64` | Batch size for embed calls |
| `default_collection` | `"helpdesk"` | Vector store collection name |
| `max_rows_in_memory` | `100_000` | Skip threshold for oversized sheets |
| `backend_timeout_seconds` | `30.0` | Timeout for backend calls |
| `log_sample_data` | `False` | PII-safe logging control |
| `log_chunk_previews` | `False` | PII-safe logging control |

## 4. Implementation Design

### 4.1 Constructor

```python
class TextSerializer:
    def __init__(
        self,
        vector_store: VectorStoreBackend,
        embedder: EmbeddingBackend,
        config: ExcelProcessorConfig,
    ) -> None:
```

**Note:** Unlike Path A, Path B does NOT take a `StructuredDBBackend` -- it only writes to vector store.

### 4.2 Process Method Signature (Reconciled)

Must follow actual Path A pattern (not simplified spec) to produce valid `ProcessingResult`:

```python
def process(
    self,
    file_path: str,
    profile: FileProfile,
    ingest_key: str,
    ingest_run_id: str,
    parse_result: ParseStageResult,
    classification_result: ClassificationStageResult,
    classification: ClassificationResult,
) -> ProcessingResult:
```

### 4.3 Core Algorithm: Section Detection

The key algorithmic challenge. Must implement:

1. **Load workbook with openpyxl** (not pandas) to preserve merged cell info
2. **Detect merged header rows** -- rows where a merged cell spans multiple columns
3. **Detect blank row separators** -- rows where all cells are None/empty
4. **Detect indentation patterns** -- cells where content starts in later columns

Each detected section gets:
- A title (from the merged header or first non-empty row)
- A sub-structure classification: `"table"`, `"checklist"`, `"matrix"`, `"free_text"`
- The raw cell data for serialization

### 4.4 Sub-Structure Classification Heuristics

For each detected section:

| Sub-Structure | Detection Heuristic |
|--------------|---------------------|
| **Small table** | Has a header row, consistent column types, 2+ rows of data |
| **Checklist** | Has status-like columns (yes/no, done/pending, checkmarks), item column, possibly due date and responsible party columns |
| **Matrix** | Has both row headers (col A) AND column headers (row 1), with data in the intersection cells |
| **Free text** | Long text in few cells, no tabular structure, low column consistency |

### 4.5 Serialization Formats

| Sub-Structure | Output Format |
|--------------|---------------|
| **Small table** | `"In section '{title}', {col} is {val}, {col} is {val}."` (one sentence per row) |
| **Checklist** | `"Item X: status is Y, due date is Z, responsible party is W."` (spec-exact) |
| **Matrix** | `"For {row_header}, {col_header} is {value}."` (one sentence per cell) |
| **Free text** | Extract as-is, preserving paragraph breaks |

### 4.6 ProcessingResult Assembly

For Path B:
- `ingestion_method = IngestionMethod.TEXT_SERIALIZATION`
- `tables_created = 0` (Path B creates no DB tables)
- `tables = []` (no DB tables)
- `written.db_table_names = []` (no DB tables)
- `written.vector_point_ids = [...]` (all chunk IDs)
- `written.vector_collection = config.default_collection`

## 5. Test Design

### 5.1 Test Pattern (from test_structured_db.py)

- Factory functions: `_make_sheet_profile()`, `_make_file_profile()`, `_make_parse_result()`, `_make_classification_stage_result()`, `_make_classification_result()`
- Mock backends: `MockVectorStore`, `MockEmbedder` (reusable from test_structured_db.py or conftest)
- Fixture for processor: creates `TextSerializer` with mock backends
- `@patch` openpyxl loading to provide controlled cell data
- Test classes grouped by concern: section detection, serialization formats, metadata, error handling

### 5.2 Key Test Cases (from SPEC line 1096)

1. **Merged cell handling** -- verify merged cells are detected and used for section titles
2. **Section detection** -- blank rows split sections; merged headers identify titles
3. **Checklist serialization** -- produces "Item X: status is Y, due date is Z" format
4. **Matrix serialization** -- produces "For {row_header}, {col_header} is {value}" format
5. **Free-text serialization** -- preserves paragraph breaks, extracts as-is
6. **Small table serialization** -- serializes rows as sentences
7. **ChunkMetadata correctness** -- section_title, original_structure, ingestion_method="text_serialization"
8. **Multi-sheet processing** -- chunk_index global across sheets
9. **Error handling** -- per-sheet errors logged and continue
10. **WrittenArtifacts tracking** -- vector_point_ids populated, db_table_names empty
11. **Embedding batching** -- respects embedding_batch_size
12. **Sheet skipping** -- hidden/chart-only/oversized sheets skipped with warnings
13. **Tenant ID propagation** -- tenant_id flows to ChunkMetadata

## 6. Processors __init__.py and Package Exports

### 6.1 Current processors/__init__.py

```python
from ingestkit_excel.processors.structured_db import StructuredDBProcessor
__all__ = ["StructuredDBProcessor"]
```

Must add:
```python
from ingestkit_excel.processors.serializer import TextSerializer
__all__ = ["StructuredDBProcessor", "TextSerializer"]
```

### 6.2 Current Package __init__.py

```python
from ingestkit_excel.processors import StructuredDBProcessor
```

Must add:
```python
from ingestkit_excel.processors import TextSerializer
```

And add `"TextSerializer"` to `__all__`.

## 7. Risks and Reconciliation

### 7.1 Process Signature Mismatch (HIGH)

**Spec says:** `process(file_path, profile, ingest_key, ingest_run_id)`
**Path A actual:** `process(file_path, profile, ingest_key, ingest_run_id, parse_result, classification_result, classification)`

**Resolution:** Follow Path A actual implementation. The extra parameters are needed because `ProcessingResult` requires `parse_result`, `classification_result`, and `classification` as fields. The spec's simplified signature is aspirational but the actual code requires these.

### 7.2 openpyxl Direct Usage (MEDIUM)

Path B must parse with openpyxl directly (not pandas) to preserve merged cell structure. This means:
- Using `openpyxl.load_workbook(file_path)` directly
- Iterating `ws.merged_cells.ranges` to find merged regions
- Reading cell values while respecting merge groups

This is different from Path A which uses `pd.read_excel()`. The spec explicitly requires openpyxl for merged cell preservation.

### 7.3 Section Detection Complexity (MEDIUM)

The section detection algorithm is the most complex part. Heuristics needed:
- Merged header row detection (a row with a merged cell spanning 2+ columns)
- Blank row separator detection (N consecutive blank rows)
- Indentation pattern detection (cells starting in column B+ instead of A)

Must be robust but not over-engineered. Keep heuristics simple and testable.

### 7.4 Sub-Structure Classification Ambiguity (LOW)

Some sections may not clearly fit one sub-structure type. Default to `"free_text"` when uncertain (fail-closed principle).

### 7.5 No StructuredDBBackend (LOW)

Path B does not use `StructuredDBBackend`. The `ProcessingResult` fields `tables_created` and `tables` should be `0` and `[]` respectively. `written.db_table_names` should be `[]`.

### 7.6 Error Code Usage (LOW)

Path B should use `E_PROCESS_SERIALIZE` for serialization failures, not `E_PROCESS_SCHEMA_GEN` which is Path A specific.

## 8. Files Summary

| File | Action | Description |
|------|--------|-------------|
| `processors/serializer.py` | CREATE | TextSerializer class with section detection + serialization |
| `tests/test_serializer.py` | CREATE | Unit tests for merged cells, section detection, all 4 sub-structure types, metadata, errors |
| `processors/__init__.py` | MODIFY | Add TextSerializer export |
| `__init__.py` | MODIFY | Add TextSerializer to package exports and __all__ |

---

AGENT_RETURN: map-9-021226.md
