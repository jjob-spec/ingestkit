# MAP: Issue #47 — Integration Test Suite and Benchmark Report

**Date:** 2026-02-14
**Complexity:** COMPLEX
**SPEC sections:** 17 (Testing Strategy) — Note: Sections 22.3, 22.5, 25 do not exist in the SPEC. The SPEC has 19 sections total.

---

## 1. Issue Requirements Summary

1. **`test_integration.py`** — `@pytest.mark.integration` tests: full pipeline for each doc type (A, B, C) with real backends (Qdrant, Ollama, SQLite)
2. **`test_benchmark.py`** — Benchmark tests measuring throughput against SLO targets
3. **`scripts/benchmark.py`** — Standalone benchmark script producing `benchmark-report-<date>.json`
4. SLO targets: Path A >= 50 pages/sec, Path B >= 10 pages/sec
5. Per-stage latency budgets: security < 100ms, profile < 2s, Tier 1 < 200ms
6. Phase 2 gate checklist

---

## 2. SPEC Coverage

### Section 17 — Testing Strategy (the relevant section)

- **17.1 Mock Backends:** Already implemented in `conftest.py` (MockVectorStore, MockStructuredDB, MockLLM, MockEmbedding)
- **17.2 Test Fixtures:** All 6 `.xlsx` fixtures already exist as session-scoped programmatic generators in `conftest.py`
- **17.3 Test Coverage:** Lists `backends/` tests requiring "Protocol compliance; timeout/retry behavior; Qdrant/SQLite/Ollama basic operations (integration, marked)"
- **17.4 Markers:** Defines `@pytest.mark.unit` and `@pytest.mark.integration` — both already registered in `pyproject.toml`

**Important:** The SPEC does NOT explicitly define SLO targets (50 pages/sec, 10 pages/sec) or per-stage latency budgets. These come from the issue itself. The SPEC also does not mention a Phase 2 gate or benchmark report format. These are issue-defined requirements.

---

## 3. Existing Infrastructure

### 3.1 Test Files (all in `packages/ingestkit-excel/tests/`)

| File | Status |
|------|--------|
| `conftest.py` | Complete: 4 mock backends, 7 xlsx fixtures, test config |
| `test_router.py` | Complete: 26 unit tests covering all paths, tier escalation, fail-closed |
| `test_parser_chain.py` | Exists |
| `test_inspector.py` | Exists |
| `test_llm_classifier.py` | Exists |
| `test_serializer.py` | Exists |
| `test_splitter.py` | Exists |
| `test_backends.py` | Exists |
| `test_structured_db.py` | Exists |
| `test_models.py` | Exists |
| `test_config.py` | Exists |
| `test_idempotency.py` | Exists |
| `test_conftest.py` | Exists |
| **`test_integration.py`** | **DOES NOT EXIST** |
| **`test_benchmark.py`** | **DOES NOT EXIST** |

### 3.2 Scripts Directory

`packages/ingestkit-excel/scripts/` does not exist. Needs to be created.

### 3.3 pytest Configuration (`pyproject.toml`)

```toml
[tool.pytest.ini_options]
testpaths = ["tests"]
markers = [
    "unit: Unit tests (no external services)",
    "integration: Integration tests (require external services)",
]
```

Both markers are already registered. No `pytest-benchmark` dependency listed.

---

## 4. Full Pipeline Flow (from `router.py`)

The `ExcelRouter.process()` flow is:

1. **Compute IngestKey** — `compute_ingest_key(file_path, parser_version, tenant_id, source_uri)`
2. **Generate ingest_run_id** — `uuid.uuid4()`
3. **Parse** — `self._parser_chain.parse(file_path)` returns `(FileProfile, list[IngestError])`
4. **Build ParseStageResult** — static helper
5. **Classify (tiered):**
   - Tier 1: `self._inspector.classify(profile)` — rule-based
   - If confidence == 0.0: Tier 2: `self._llm_classifier.classify(profile, LLM_BASIC)`
   - If confidence < threshold and tier3 enabled: Tier 3: `self._llm_classifier.classify(profile, LLM_REASONING)`
6. **Fail-closed check** — if still confidence == 0.0, return error result
7. **Select processor** — `_select_processor(file_type)` returns StructuredDBProcessor | TextSerializer | HybridSplitter
8. **Process** — `processor.process(file_path, profile, ingest_key, ...)`
9. **Merge parse errors** and finalize timing

### Processor Signatures (all three share the same API)

```python
def process(
    self,
    file_path: str,
    profile: FileProfile,
    ingest_key: str,
    ingest_run_id: str,
    parse_result: ParseStageResult,
    classification_result: ClassificationStageResult,
    classification: ClassificationResult,
) -> ProcessingResult:
```

---

## 5. Backend Availability

### 5.1 Concrete Backends

| Backend | Class | Location | Dependency |
|---------|-------|----------|------------|
| Qdrant | `QdrantVectorStore` | `backends/qdrant.py` | `qdrant-client>=1.7` (optional) |
| SQLite | `SQLiteStructuredDB` | `backends/sqlite.py` | stdlib `sqlite3` (always available) |
| Ollama LLM | `OllamaLLM` | `backends/ollama.py` | `httpx>=0.27` (optional) |
| Ollama Embedding | `OllamaEmbedding` | `backends/ollama.py` | `httpx>=0.27` (optional) |

### 5.2 Backend Constructor Defaults

- **QdrantVectorStore:** `url="http://localhost:6333"`, accepts optional `config`
- **SQLiteStructuredDB:** `db_path=":memory:"` (in-memory by default)
- **OllamaLLM:** `base_url="http://localhost:11434"`, accepts optional `config`
- **OllamaEmbedding:** `base_url="http://localhost:11434"`, `model="nomic-embed-text"`, `embedding_dimension=768`

### 5.3 Factory Helper

`create_default_router(**overrides)` in `router.py` builds an ExcelRouter with all default backends. Supports overriding any backend or config param.

### 5.4 Integration Test Prerequisites

Integration tests need running services:
- **Qdrant** on `localhost:6333`
- **Ollama** on `localhost:11434` with models: `qwen2.5:7b`, `deepseek-r1:14b`, `nomic-embed-text`
- **SQLite** always available (in-memory)

---

## 6. Available Test Fixtures (from `conftest.py`)

| Fixture | Scope | Description |
|---------|-------|-------------|
| `type_a_simple_xlsx` | session | 3 columns, 20 rows, clean tabular |
| `type_b_checklist_xlsx` | session | Merged header, checklist items |
| `type_c_hybrid_xlsx` | session | Sheet1=tabular (10 rows), Sheet2=formatted text with merges |
| `edge_empty_xlsx` | session | Empty workbook |
| `edge_chart_only_xlsx` | session | Data + embedded chart |
| `top_customers_2024_xlsx` | session | 10-row tabular with totals row |
| `edge_large_xlsx` | session | 100,001 rows |
| `sample_config` | function | Default `ExcelProcessorConfig` |
| `test_config` | function | Config with `tenant_id="test_tenant"`, `log_sample_data=True` |
| `mock_vector_store` | function | MockVectorStore |
| `mock_structured_db` | function | MockStructuredDB |
| `mock_llm` | function | MockLLM |
| `mock_embedding` | function | MockEmbedding |

---

## 7. Key Decisions / Gaps

### 7.1 No Benchmark/SLO in SPEC

The SPEC does not define throughput SLOs or latency budgets. The issue defines these targets:
- Path A >= 50 pages/sec
- Path B >= 10 pages/sec
- Security < 100ms (no security stage exists in the pipeline)
- Profile < 2s (parsing stage)
- Tier 1 < 200ms (inspector)

**Note on "security" stage:** There is no security/PII-scanning stage in the current pipeline. The issue may reference a future feature or may mean the idempotency key computation step. The implementation should either skip this or treat it as the ingest key computation step.

**Note on "pages":** Excel files don't have "pages" in the traditional sense. For benchmarking, "pages" likely means "sheets" or should be defined as a throughput metric (e.g., rows/sec or files/sec).

### 7.2 No Phase 2 Gate in SPEC

The SPEC has no concept of phase gates. This is an issue-level requirement. The benchmark report and gate checklist are new artifacts.

### 7.3 `pytest-benchmark` Not Listed as Dependency

Need to either:
- Add `pytest-benchmark` to dev dependencies
- Implement manual timing with `time.monotonic()` (simpler, no new dependency)

### 7.4 Integration Tests Need Service Skip Logic

Tests should be skipped gracefully when backends are unavailable. Pattern:
```python
@pytest.mark.integration
def test_...():
    pytest.importorskip("qdrant_client")
    # + connection check
```

---

## 8. ProcessingResult Fields (for assertion targets)

```python
class ProcessingResult(BaseModel):
    file_path: str
    ingest_key: str
    ingest_run_id: str
    tenant_id: str | None = None
    parse_result: ParseStageResult
    classification_result: ClassificationStageResult
    embed_result: EmbedStageResult | None = None
    classification: ClassificationResult
    ingestion_method: IngestionMethod
    chunks_created: int
    tables_created: int
    tables: list[str]
    written: WrittenArtifacts
    errors: list[str]
    warnings: list[str]
    error_details: list[IngestError] = []
    processing_time_seconds: float
```

Key assertions for integration tests:
- `chunks_created > 0` for all paths
- `tables_created > 0` for Path A and Path C
- `errors == []` for clean files
- `written.vector_point_ids` populated
- `written.vector_collection == "helpdesk"`
- `classification.confidence > 0` (not fail-closed)
- `ingest_key` is a valid hex string
- `processing_time_seconds > 0`

---

## 9. Files to Create

| File | Purpose |
|------|---------|
| `packages/ingestkit-excel/tests/test_integration.py` | Integration tests with real backends |
| `packages/ingestkit-excel/tests/test_benchmark.py` | Benchmark tests with SLO assertions |
| `packages/ingestkit-excel/scripts/benchmark.py` | Standalone benchmark script -> JSON report |

---

## 10. Risk Assessment

| Risk | Mitigation |
|------|------------|
| Qdrant/Ollama not running in CI | Skip logic with `pytest.importorskip` + connection health check |
| "pages/sec" metric undefined for Excel | Define as files/sec or sheets/sec in benchmark |
| "security" stage doesn't exist | Treat as ingest key computation or skip |
| LLM classification is non-deterministic | Use temperature=0.0 or accept confidence > 0 (not exact match) |
| Large file benchmark (100K rows) may be slow | Use `edge_large_xlsx` fixture cautiously; set generous timeout |
| No `pytest-benchmark` dependency | Use manual `time.monotonic()` timing, avoid new dependency |

---

## 11. Approach Recommendation

### test_integration.py

1. **Helper function** to check backend availability (Qdrant ping, Ollama health check)
2. **Fixtures** for real backends (session-scoped to avoid repeated connections)
3. **Three core tests** (one per path): `test_path_a_full_pipeline`, `test_path_b_full_pipeline`, `test_path_c_full_pipeline`
4. Each test: create router with real backends -> call `process()` with appropriate fixture xlsx -> assert ProcessingResult correctness
5. **Cleanup**: delete created Qdrant collections and SQLite tables after each test
6. All tests marked `@pytest.mark.integration`

### test_benchmark.py

1. Use `time.monotonic()` for timing
2. **Per-stage latency tests**: parse, classify (Tier 1), embed — measure against budgets
3. **Throughput tests**: process N files, compute files/sec
4. Generate structured timing data
5. Marked `@pytest.mark.integration` (requires real backends for meaningful benchmarks)

### scripts/benchmark.py

1. Standalone CLI script using `argparse`
2. Runs full pipeline N times per path
3. Collects per-stage timing from `ProcessingResult` (parse_duration, classification_duration, embed_duration, total processing_time)
4. Outputs `benchmark-report-<date>.json` with:
   - Throughput per path
   - Per-stage latency (p50, p95, max)
   - SLO pass/fail
   - Phase 2 gate checklist items
5. Uses `create_default_router()` factory

---

AGENT_RETURN: .agents/outputs/map-47-021426.md
