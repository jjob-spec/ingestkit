# MAP-PLAN: Issue #6 — Tier 1 Rule-Based Inspector

**Issue**: Implement `ExcelInspector` class in `inspector.py`
**Spec Section**: SPEC.md section 8
**Date**: 2026-02-10

---

## 1. ENUM VALUES (use these string values, NOT Python names)

### FileType (str, Enum) — `/packages/ingestkit-excel/src/ingestkit_excel/models.py` line 24

| Python Name | String VALUE (use this) | Alias |
|---|---|---|
| `FileType.TABULAR_DATA` | `"tabular_data"` | Type A |
| `FileType.FORMATTED_DOCUMENT` | `"formatted_document"` | Type B |
| `FileType.HYBRID` | `"hybrid"` | Type C |

### ClassificationTier (str, Enum) — same file, line 36

| Python Name | String VALUE (use this) | Tier |
|---|---|---|
| `ClassificationTier.RULE_BASED` | `"rule_based"` | Tier 1 |
| `ClassificationTier.LLM_BASIC` | `"llm_basic"` | Tier 2 |
| `ClassificationTier.LLM_REASONING` | `"llm_reasoning"` | Tier 3 |

### ParserUsed (str, Enum) — same file, line 72

| Python Name | String VALUE |
|---|---|
| `ParserUsed.OPENPYXL` | `"openpyxl"` |
| `ParserUsed.PANDAS_FALLBACK` | `"pandas_fallback"` |
| `ParserUsed.RAW_TEXT_FALLBACK` | `"raw_text_fallback"` |

---

## 2. MODEL FIELD NAMES AND TYPES

### SheetProfile — models.py line 148

These are the fields the inspector reads per-sheet to evaluate signals:

| Field | Type | Description |
|---|---|---|
| `name` | `str` | Sheet name |
| `row_count` | `int` | Total rows (including header) |
| `col_count` | `int` | Total columns |
| `merged_cell_count` | `int` | Number of merged cell ranges |
| `merged_cell_ratio` | `float` | merged_cells / total_cells |
| `header_row_detected` | `bool` | Whether a header row was found |
| `header_row_index` | `int \| None` | 0-based index of header row (NOT in spec but in actual model) |
| `header_values` | `list[str]` | Detected header cell values |
| `column_type_consistency` | `float` | 0.0-1.0, how uniform column dtypes are |
| `numeric_ratio` | `float` | Proportion of numeric cells |
| `text_ratio` | `float` | Proportion of text cells |
| `empty_ratio` | `float` | Proportion of empty cells |
| `sample_rows` | `list[list[str]]` | First N rows as strings |
| `has_formulas` | `bool` | Whether formulas exist |
| `is_hidden` | `bool` | Whether sheet is hidden |
| `parser_used` | `ParserUsed` | Which parser succeeded |

### FileProfile — models.py line 169

Input to `classify()`:

| Field | Type | Description |
|---|---|---|
| `file_path` | `str` | Path to the file |
| `file_size_bytes` | `int` | File size |
| `sheet_count` | `int` | Number of parsed sheets |
| `sheet_names` | `list[str]` | Names of parsed sheets |
| `sheets` | `list[SheetProfile]` | Per-sheet profiles |
| `has_password_protected_sheets` | `bool` | |
| `has_chart_only_sheets` | `bool` | |
| `total_merged_cells` | `int` | |
| `total_rows` | `int` | |
| `content_hash` | `str` | SHA-256 |

### ClassificationResult — models.py line 184

Output of `classify()`:

| Field | Type | Default | Description |
|---|---|---|---|
| `file_type` | `FileType` | **required** | The classification |
| `confidence` | `float` | **required** | 0.0-1.0 |
| `tier_used` | `ClassificationTier` | **required** | Which tier produced this |
| `reasoning` | `str` | **required** | Human-readable explanation |
| `per_sheet_types` | `dict[str, FileType] \| None` | `None` | Per-sheet types (for hybrid) |
| `signals` | `dict[str, Any] \| None` | `None` | Signal breakdown |

---

## 3. CONFIG THRESHOLD FIELDS AND DEFAULTS

### ExcelProcessorConfig — config.py line 16

Tier 1 relevant thresholds:

| Field Name | Type | Default | Usage in Inspector |
|---|---|---|---|
| `tier1_high_confidence_signals` | `int` | `4` | >= this many signals match one type -> confidence 0.9 |
| `tier1_medium_confidence_signals` | `int` | `3` | == this many signals match -> confidence 0.7 |
| `merged_cell_ratio_threshold` | `float` | `0.05` | Signal 2: above this leans Type B |
| `numeric_ratio_threshold` | `float` | `0.3` | Signal 5: above this leans Type A |
| `column_consistency_threshold` | `float` | `0.7` | Signal 3: above this leans Type A |
| `min_row_count_for_tabular` | `int` | `5` | Signal 1: >= this leans Type A |

---

## 4. SIGNAL EVALUATION LOGIC (from SPEC.md section 8.2)

Five binary signals per sheet. Each signal scores toward Type A (tabular) or Type B (document):

| # | Signal Name | Type A condition (tabular) | Type B condition (document) |
|---|---|---|---|
| 1 | `row_count` | `sheet.row_count >= config.min_row_count_for_tabular` | `sheet.row_count < config.min_row_count_for_tabular` |
| 2 | `merged_cell_ratio` | `sheet.merged_cell_ratio < config.merged_cell_ratio_threshold` | `sheet.merged_cell_ratio >= config.merged_cell_ratio_threshold` |
| 3 | `column_type_consistency` | `sheet.column_type_consistency >= config.column_consistency_threshold` | `sheet.column_type_consistency < config.column_consistency_threshold` |
| 4 | `header_detected` | `sheet.header_row_detected is True` | `sheet.header_row_detected is False` |
| 5 | `numeric_ratio` | `sheet.numeric_ratio >= config.numeric_ratio_threshold` | `sheet.numeric_ratio < config.numeric_ratio_threshold` (text-dominant) |

### Decision logic per sheet:

1. Count signals matching Type A and count signals matching Type B.
   - Each signal is binary: it matches exactly one of Type A or Type B.
   - So `type_a_count + type_b_count == 5` always.
2. If `type_a_count >= config.tier1_high_confidence_signals` (default 4): **Type A, confidence 0.9**
3. If `type_b_count >= config.tier1_high_confidence_signals` (default 4): **Type B, confidence 0.9**
4. If `type_a_count >= config.tier1_medium_confidence_signals` (default 3): **Type A, confidence 0.7**
5. If `type_b_count >= config.tier1_medium_confidence_signals` (default 3): **Type B, confidence 0.7**
6. Otherwise (split or < 3): **inconclusive**, escalate to Tier 2.

### Multi-sheet logic:

1. Classify each sheet independently using the above logic.
2. If any sheet is inconclusive, the whole file is inconclusive (escalate to Tier 2).
3. If all sheets agree on the same type:
   - File type = that type
   - Confidence = minimum confidence across sheets (conservative)
4. If sheets disagree (some Type A, some Type B):
   - File type = `"hybrid"` (Type C)
   - Confidence = 0.9 (high confidence that it IS hybrid since we have clear evidence)
   - Record `per_sheet_types` dict mapping sheet name to its FileType

---

## 5. IMPLEMENTATION PLAN: `inspector.py`

**File**: `/home/jjob/projects/ingestkit/packages/ingestkit-excel/src/ingestkit_excel/inspector.py`

### Imports

```python
from __future__ import annotations

import logging
from typing import Any

from ingestkit_excel.config import ExcelProcessorConfig
from ingestkit_excel.models import (
    ClassificationResult,
    ClassificationTier,
    FileProfile,
    FileType,
    SheetProfile,
)
```

### Class: `ExcelInspector`

```python
class ExcelInspector:
    """Tier 1 rule-based structural inspector for Excel files.

    Evaluates 5 binary signals per sheet and uses threshold-based
    decision logic to classify files without any LLM call.
    """

    def __init__(self, config: ExcelProcessorConfig) -> None:
        self._config = config

    def classify(self, profile: FileProfile) -> ClassificationResult:
        """Classify a file based on structural signals from its profile."""
        ...

    def _classify_sheet(self, sheet: SheetProfile) -> tuple[FileType | None, float, dict[str, Any]]:
        """Classify a single sheet. Returns (file_type_or_None, confidence, signals_dict).

        Returns file_type=None when inconclusive.
        """
        ...

    def _evaluate_signals(self, sheet: SheetProfile) -> dict[str, bool]:
        """Evaluate the 5 binary signals for a sheet.

        Returns a dict mapping signal name -> True if Type A, False if Type B.
        """
        ...
```

### Method: `_evaluate_signals(sheet)`

Returns a dict with 5 keys, each mapping to `True` (leans Type A) or `False` (leans Type B):

```python
{
    "row_count": sheet.row_count >= self._config.min_row_count_for_tabular,
    "merged_cell_ratio": sheet.merged_cell_ratio < self._config.merged_cell_ratio_threshold,
    "column_type_consistency": sheet.column_type_consistency >= self._config.column_consistency_threshold,
    "header_detected": sheet.header_row_detected,
    "numeric_ratio": sheet.numeric_ratio >= self._config.numeric_ratio_threshold,
}
```

### Method: `_classify_sheet(sheet)`

1. Call `_evaluate_signals(sheet)` to get the 5 signal booleans.
2. Count `type_a_count = sum(1 for v in signals.values() if v)`.
3. `type_b_count = 5 - type_a_count`.
4. Decision:
   - If `type_a_count >= self._config.tier1_high_confidence_signals`: return `(FileType.TABULAR_DATA, 0.9, signals)`
   - If `type_b_count >= self._config.tier1_high_confidence_signals`: return `(FileType.FORMATTED_DOCUMENT, 0.9, signals)`
   - If `type_a_count >= self._config.tier1_medium_confidence_signals`: return `(FileType.TABULAR_DATA, 0.7, signals)`
   - If `type_b_count >= self._config.tier1_medium_confidence_signals`: return `(FileType.FORMATTED_DOCUMENT, 0.7, signals)`
   - Otherwise: return `(None, 0.0, signals)` -- inconclusive

### Method: `classify(profile)`

1. If `profile.sheet_count == 0` or `len(profile.sheets) == 0`:
   - Return inconclusive result with confidence 0.0, reasoning about no sheets.
2. Iterate over `profile.sheets`, call `_classify_sheet()` for each.
3. Collect per-sheet results: `list[tuple[str, FileType | None, float, dict]]`.
4. Build aggregated `signals` dict for the overall result:
   ```python
   signals = {
       "per_sheet": {
           sheet_name: {
               "type": file_type.value if file_type else None,
               "confidence": confidence,
               "signals": sheet_signals,
           }
           for sheet_name, file_type, confidence, sheet_signals in sheet_results
       }
   }
   ```
5. **If any sheet is inconclusive** (file_type is None):
   - Return `ClassificationResult` with:
     - `file_type=FileType.TABULAR_DATA` (placeholder -- doesn't matter since it's inconclusive)
     - `confidence=0.0`
     - `tier_used=ClassificationTier.RULE_BASED`
     - `reasoning="Inconclusive: sheet '{name}' could not be classified with sufficient confidence."`
     - `signals=signals`
   - NOTE: The router checks for low confidence to decide tier escalation. The spec says "inconclusive, escalate to Tier 2" which means we need confidence low enough to trigger escalation. Using `confidence=0.0` ensures this.
   - CORRECTION on file_type for inconclusive: We still need a valid FileType. Looking at how the router will use this, it checks confidence to decide escalation, not file_type. Use `FileType.HYBRID` as a safe default since inconclusive results will be overridden by Tier 2 anyway. Actually, looking at the ClassificationResult model, `file_type` is required (no default). The safest approach: pick the type with the most signals across sheets, or default to `FileType.HYBRID`. Since the confidence is 0.0, the router will always escalate.
6. **If all sheets agree on the same type**:
   - `file_type` = that type
   - `confidence` = minimum confidence across all sheets (conservative)
   - `reasoning` = "All {n} sheet(s) classified as {type.value} with {confidence} confidence by Tier 1 rule-based inspector."
   - `per_sheet_types` = None (all agree, so no need)
   - `signals` = aggregated signals dict
7. **If sheets disagree** (mix of Type A and Type B):
   - `file_type = FileType.HYBRID`
   - `confidence = 0.9` (high confidence it IS hybrid)
   - `per_sheet_types` = `{sheet_name: sheet_file_type for ...}`
   - `reasoning` = "Sheets disagree: {type_a_names} are tabular_data, {type_b_names} are formatted_document. Classified as hybrid."
   - `signals` = aggregated signals dict
8. Always set `tier_used = ClassificationTier.RULE_BASED`.

### Logging

- Use logger `logging.getLogger("ingestkit_excel")` (same as parser_chain.py).
- DEBUG: log individual signal evaluations per sheet.
- INFO: log final classification result (file_type, confidence, tier).

---

## 6. `__init__.py` UPDATE

**File**: `/home/jjob/projects/ingestkit/packages/ingestkit-excel/src/ingestkit_excel/__init__.py`

Add import and export:

```python
from ingestkit_excel.inspector import ExcelInspector
```

Add `"ExcelInspector"` to the `__all__` list, under a new `# Inspector` comment section.

---

## 7. TEST PLAN: `test_inspector.py`

**File**: `/home/jjob/projects/ingestkit/packages/ingestkit-excel/tests/test_inspector.py`

### Test Style (from existing tests)

- Use `from __future__ import annotations`
- Group tests in classes with descriptive names
- Use section separators with `# ---------------------------------------------------------------------------`
- Use `pytest.fixture()` with parentheses
- Helper functions prefixed with `_`
- Type annotations on fixtures and test methods (return `-> None`)
- Import from `ingestkit_excel.*` directly (not the package `__init__`)

### Helper: `_make_sheet_profile(**overrides) -> SheetProfile`

Reuse the pattern from `test_models.py` (line 268):
```python
def _make_sheet_profile(**overrides: object) -> SheetProfile:
    defaults: dict = dict(
        name="Sheet1",
        row_count=100,
        col_count=5,
        merged_cell_count=0,
        merged_cell_ratio=0.0,
        header_row_detected=True,
        header_values=["A", "B", "C", "D", "E"],
        column_type_consistency=0.9,
        numeric_ratio=0.4,
        text_ratio=0.5,
        empty_ratio=0.1,
        sample_rows=[["1", "a", "x", "2.0", "y"]],
        has_formulas=False,
        is_hidden=False,
        parser_used=ParserUsed.OPENPYXL,
    )
    defaults.update(overrides)
    return SheetProfile(**defaults)
```

### Helper: `_make_file_profile(sheets: list[SheetProfile], **overrides) -> FileProfile`

Build a FileProfile from a list of SheetProfiles:
```python
def _make_file_profile(sheets: list[SheetProfile], **overrides: object) -> FileProfile:
    defaults: dict = dict(
        file_path="/tmp/test.xlsx",
        file_size_bytes=1024,
        sheet_count=len(sheets),
        sheet_names=[s.name for s in sheets],
        sheets=sheets,
        has_password_protected_sheets=False,
        has_chart_only_sheets=False,
        total_merged_cells=sum(s.merged_cell_count for s in sheets),
        total_rows=sum(s.row_count for s in sheets),
        content_hash="a" * 64,
    )
    defaults.update(overrides)
    return FileProfile(**defaults)
```

### Fixtures

```python
@pytest.fixture()
def config() -> ExcelProcessorConfig:
    return ExcelProcessorConfig()

@pytest.fixture()
def inspector(config: ExcelProcessorConfig) -> ExcelInspector:
    return ExcelInspector(config)
```

### Test Classes

#### `TestSignalEvaluation`
Test that individual signals are evaluated correctly.

1. **`test_row_count_signal_type_a`** — row_count=100 (>= 5) -> signal is True (Type A)
2. **`test_row_count_signal_type_b`** — row_count=3 (< 5) -> signal is False (Type B)
3. **`test_merged_cell_ratio_signal_type_a`** — ratio=0.0 (< 0.05) -> True
4. **`test_merged_cell_ratio_signal_type_b`** — ratio=0.1 (>= 0.05) -> True maps to Type B (False)
5. **`test_column_consistency_signal_type_a`** — consistency=0.9 (>= 0.7) -> True
6. **`test_column_consistency_signal_type_b`** — consistency=0.4 (< 0.7) -> False
7. **`test_header_detected_signal_type_a`** — header_row_detected=True -> True
8. **`test_header_detected_signal_type_b`** — header_row_detected=False -> False
9. **`test_numeric_ratio_signal_type_a`** — numeric_ratio=0.5 (>= 0.3) -> True
10. **`test_numeric_ratio_signal_type_b`** — numeric_ratio=0.1 (< 0.3) -> False

#### `TestSingleSheetClassification`
Test decision logic for a single sheet.

1. **`test_all_5_signals_type_a_high_confidence`** — All 5 signals lean Type A -> `"tabular_data"`, confidence 0.9
   - SheetProfile: row_count=100, merged_cell_ratio=0.0, column_type_consistency=0.9, header_row_detected=True, numeric_ratio=0.5
2. **`test_all_5_signals_type_b_high_confidence`** — All 5 signals lean Type B -> `"formatted_document"`, confidence 0.9
   - SheetProfile: row_count=3, merged_cell_ratio=0.2, column_type_consistency=0.3, header_row_detected=False, numeric_ratio=0.1
3. **`test_4_signals_type_a_high_confidence`** — 4 of 5 signals lean Type A -> `"tabular_data"`, confidence 0.9
   - Make one signal lean Type B (e.g., numeric_ratio=0.1 while rest are Type A)
4. **`test_4_signals_type_b_high_confidence`** — 4 signals lean Type B -> `"formatted_document"`, confidence 0.9
5. **`test_3_signals_type_a_medium_confidence`** — 3 signals lean Type A -> `"tabular_data"`, confidence 0.7
6. **`test_3_signals_type_b_medium_confidence`** — 3 signals lean Type B -> `"formatted_document"`, confidence 0.7
7. **`test_inconclusive_split_signals`** — Not possible with 5 binary signals (one side always has >= 3). This case is effectively "< 3 signals match one type" which can't happen with binary signals summing to 5. The "< 3 or split" from the spec means: if we consider a different threshold config. Test with custom config: `tier1_medium_confidence_signals=4, tier1_high_confidence_signals=5` so that 3 signals is NOT enough -> inconclusive.

#### `TestMultiSheetAgreement`
Test multi-sheet logic when sheets agree.

1. **`test_two_sheets_both_type_a`** — Both sheets classified as Type A -> file is Type A
2. **`test_two_sheets_both_type_b`** — Both sheets classified as Type B -> file is Type B
3. **`test_confidence_is_minimum_across_sheets`** — One sheet at 0.9, another at 0.7 -> file confidence is 0.7
4. **`test_three_sheets_all_agree`** — Three sheets all Type A -> file is Type A

#### `TestMultiSheetDisagreement`
Test multi-sheet logic when sheets disagree.

1. **`test_two_sheets_disagree_produces_hybrid`** — Sheet1 is Type A, Sheet2 is Type B -> `"hybrid"`, confidence 0.9
2. **`test_per_sheet_types_populated_on_hybrid`** — Verify `per_sheet_types` dict has correct mapping
3. **`test_three_sheets_two_type_a_one_type_b_is_hybrid`** — Mixed -> hybrid

#### `TestInconclusiveEscalation`
Test that inconclusive results have low confidence for tier escalation.

1. **`test_inconclusive_sheet_produces_low_confidence`** — One inconclusive sheet -> confidence 0.0
2. **`test_inconclusive_sheet_among_clear_sheets`** — If one sheet is inconclusive among clear ones, whole file is inconclusive

#### `TestEmptyProfile`
Edge cases with empty/no sheets.

1. **`test_empty_sheets_list_returns_inconclusive`** — No sheets -> inconclusive result
2. **`test_zero_sheet_count_returns_inconclusive`** — sheet_count=0

#### `TestClassificationResultFields`
Verify output fields are correctly populated.

1. **`test_tier_used_is_always_rule_based`** — tier_used is always `ClassificationTier.RULE_BASED`
2. **`test_signals_dict_populated`** — signals dict has per_sheet key with signal breakdown
3. **`test_reasoning_is_non_empty_string`** — reasoning is always non-empty
4. **`test_file_type_uses_enum_values`** — Verify file_type.value is `"tabular_data"` / `"formatted_document"` / `"hybrid"`

#### `TestCustomConfig`
Test with non-default config thresholds.

1. **`test_custom_min_row_count_threshold`** — min_row_count_for_tabular=50, row_count=30 -> signal leans Type B
2. **`test_custom_merged_cell_ratio_threshold`** — merged_cell_ratio_threshold=0.2, ratio=0.1 -> still Type A
3. **`test_custom_confidence_signal_counts`** — tier1_high_confidence_signals=5, tier1_medium_confidence_signals=4 -> only 5/5 is high, 4/5 is medium, 3/5 is inconclusive

#### `TestBoundaryValues`
Test exact boundary conditions.

1. **`test_row_count_exactly_at_threshold`** — row_count=5 (== min_row_count_for_tabular=5) -> Type A signal (>= threshold)
2. **`test_merged_ratio_exactly_at_threshold`** — merged_cell_ratio=0.05 (== threshold) -> Type B signal (>= threshold)
3. **`test_column_consistency_exactly_at_threshold`** — column_type_consistency=0.7 (== threshold) -> Type A signal (>= threshold)
4. **`test_numeric_ratio_exactly_at_threshold`** — numeric_ratio=0.3 (== threshold) -> Type A signal (>= threshold)

---

## 8. CRITICAL ENUM VALUE REMINDERS

When constructing `ClassificationResult`:
- `file_type=FileType.TABULAR_DATA` (the enum member, NOT the string)
- `tier_used=ClassificationTier.RULE_BASED` (the enum member)

When checking in tests:
- `result.file_type == FileType.TABULAR_DATA` (compare enum to enum)
- `result.file_type.value == "tabular_data"` (check string VALUE)
- NEVER compare `result.file_type == "TABULAR_DATA"` (Python name, WRONG)
- NEVER compare `result.file_type.value == "TABULAR_DATA"` (Python name, WRONG)

When populating `per_sheet_types`:
- `{"Sheet1": FileType.TABULAR_DATA, "Sheet2": FileType.FORMATTED_DOCUMENT}` (enum members as values)
- When serialized, these become `{"Sheet1": "tabular_data", "Sheet2": "formatted_document"}`

When populating `signals`:
- Use descriptive string keys like `"row_count"`, `"merged_cell_ratio"`, etc.
- Store boolean values (True = leans Type A, False = leans Type B)

---

## 9. FILE DEPENDENCY GRAPH

```
config.py (ExcelProcessorConfig)    models.py (SheetProfile, FileProfile,
         |                                     ClassificationResult, FileType,
         |                                     ClassificationTier)
         v                                     |
    inspector.py (ExcelInspector) <-------------+
         |
         v
    __init__.py (export ExcelInspector)
```

No new dependencies. Inspector only depends on config and models, both already implemented.

---

## 10. IMPLEMENTATION CHECKLIST

- [ ] Create `inspector.py` with `ExcelInspector` class
  - [ ] `__init__(self, config: ExcelProcessorConfig)`
  - [ ] `_evaluate_signals(self, sheet: SheetProfile) -> dict[str, bool]`
  - [ ] `_classify_sheet(self, sheet: SheetProfile) -> tuple[FileType | None, float, dict[str, Any]]`
  - [ ] `classify(self, profile: FileProfile) -> ClassificationResult`
  - [ ] Add logging (DEBUG for signals, INFO for final result)
  - [ ] Add module docstring following parser_chain.py style
- [ ] Create `test_inspector.py` with all test classes
  - [ ] Helper functions `_make_sheet_profile`, `_make_file_profile`
  - [ ] Fixtures: `config`, `inspector`
  - [ ] TestSignalEvaluation (10 tests)
  - [ ] TestSingleSheetClassification (7 tests)
  - [ ] TestMultiSheetAgreement (4 tests)
  - [ ] TestMultiSheetDisagreement (3 tests)
  - [ ] TestInconclusiveEscalation (2 tests)
  - [ ] TestEmptyProfile (2 tests)
  - [ ] TestClassificationResultFields (4 tests)
  - [ ] TestCustomConfig (3 tests)
  - [ ] TestBoundaryValues (4 tests)
- [ ] Update `__init__.py` to export `ExcelInspector`
- [ ] Run tests and verify all pass
