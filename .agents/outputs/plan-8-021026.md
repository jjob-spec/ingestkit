# PLAN: Issue #8 -- Path A Structured DB Processor

**Issue:** #8
**Date:** 2026-02-11
**MAP artifact:** `map-8-021026.md`
**Status:** PLAN ready for IMPLEMENT agent

---

## 1. Directory Structure

```
src/ingestkit_excel/
    processors/
        __init__.py           # NEW -- re-exports StructuredDBProcessor
        structured_db.py      # NEW -- main processor class + helpers
tests/
    test_structured_db.py     # NEW -- full test suite
```

No existing files are modified except `src/ingestkit_excel/__init__.py` (add processor to public API).

---

## 2. File: `src/ingestkit_excel/processors/__init__.py`

```python
"""Processing-path implementations for ingestkit-excel."""

from ingestkit_excel.processors.structured_db import StructuredDBProcessor

__all__ = ["StructuredDBProcessor"]
```

---

## 3. File: `src/ingestkit_excel/processors/structured_db.py`

### 3.1 Imports (exact)

```python
"""Path A processor: structured DB ingestion with optional row serialization."""

from __future__ import annotations

import hashlib
import logging
import re
import time
import uuid
from pathlib import Path

import pandas as pd

from ingestkit_excel.config import ExcelProcessorConfig
from ingestkit_excel.errors import ErrorCode, IngestError
from ingestkit_excel.models import (
    ChunkMetadata,
    ChunkPayload,
    ClassificationResult,
    ClassificationStageResult,
    EmbedStageResult,
    FileProfile,
    IngestionMethod,
    ParseStageResult,
    ProcessingResult,
    SheetProfile,
    WrittenArtifacts,
)
from ingestkit_excel.protocols import (
    EmbeddingBackend,
    StructuredDBBackend,
    VectorStoreBackend,
)

logger = logging.getLogger(__name__)
```

### 3.2 Module-level helpers (not methods -- used by tests directly)

#### `clean_name(raw: str) -> str`

```python
def clean_name(raw: str) -> str:
    """Clean a raw name (column or sheet) for use as a DB identifier.

    Rules:
    1. Lowercase
    2. Replace non-alphanumeric/underscore with underscore
    3. Collapse consecutive underscores
    4. Strip leading/trailing underscores
    """
    name = raw.lower()
    name = re.sub(r"[^a-z0-9_]", "_", name)
    name = re.sub(r"_+", "_", name)
    name = name.strip("_")
    return name
```

#### `deduplicate_names(names: list[str]) -> list[str]`

```python
def deduplicate_names(names: list[str]) -> list[str]:
    """Deduplicate a list of cleaned names by appending _1, _2, etc.

    Empty names after cleaning become ``column_N`` where N is the 0-based index.
    """
    seen: dict[str, int] = {}
    result: list[str] = []
    for i, name in enumerate(names):
        if not name:
            name = f"column_{i}"
        if name in seen:
            seen[name] += 1
            result.append(f"{name}_{seen[name]}")
        else:
            seen[name] = 0
            result.append(name)
    return result
```

### 3.3 Class: `StructuredDBProcessor`

#### Constructor

```python
class StructuredDBProcessor:
    """Path A processor: writes each sheet to a structured DB table,
    generates NL schema descriptions, embeds them, and optionally
    serializes rows for direct vector search.
    """

    def __init__(
        self,
        structured_db: StructuredDBBackend,
        vector_store: VectorStoreBackend,
        embedder: EmbeddingBackend,
        config: ExcelProcessorConfig,
    ) -> None:
        self._db = structured_db
        self._vector_store = vector_store
        self._embedder = embedder
        self._config = config
```

#### `process()` -- Main entry point

**Signature:**
```python
def process(
    self,
    file_path: str,
    profile: FileProfile,
    ingest_key: str,
    ingest_run_id: str,
    parse_result: ParseStageResult,
    classification_result: ClassificationStageResult,
    classification: ClassificationResult,
) -> ProcessingResult:
```

**IMPORTANT on `ingest_key` parameter**: The MAP section 14 shows the `process()` signature accepting `ingest_key: str` (the deterministic SHA-256 hex string from `IngestKey.key` property), NOT an `IngestKey` object. This matches `ProcessingResult.ingest_key: str`.

**IMPORTANT on additional parameters**: `ProcessingResult` **requires** `parse_result`, `classification_result`, and `classification` (no defaults). The processor must receive these from the caller (router) and pass them through. The MAP section 14 recommends accepting them as additional parameters.

**Pseudocode:**

```python
def process(self, file_path, profile, ingest_key, ingest_run_id,
            parse_result, classification_result, classification):
    start_time = time.monotonic()

    config = self._config
    collection = config.default_collection
    source_uri = f"file://{Path(file_path).resolve().as_posix()}"
    db_uri = self._db.get_connection_uri()

    errors: list[str] = []
    warnings: list[str] = []
    error_details: list[IngestError] = []

    written = WrittenArtifacts(vector_collection=collection)
    tables: list[str] = []
    total_chunks = 0
    total_texts_embedded = 0
    embed_duration = 0.0

    # Ensure vector collection exists
    vector_size = self._embedder.dimension()
    self._vector_store.ensure_collection(collection, vector_size)

    chunk_index_counter = 0  # global chunk index across all sheets

    for sheet in profile.sheets:
        # --- Skip logic ---
        if sheet.is_hidden:
            warnings.append(ErrorCode.W_SHEET_SKIPPED_HIDDEN.value)
            logger.info("Skipping hidden sheet: %s", sheet.name)
            continue
        if sheet.row_count == 0 and sheet.col_count == 0:
            # chart-only heuristic
            warnings.append(ErrorCode.W_SHEET_SKIPPED_CHART.value)
            logger.info("Skipping chart-only sheet: %s", sheet.name)
            continue
        if sheet.row_count > config.max_rows_in_memory:
            warnings.append(ErrorCode.W_ROWS_TRUNCATED.value)
            logger.warning(
                "Sheet %s has %d rows, exceeds max_rows_in_memory (%d), skipping",
                sheet.name, sheet.row_count, config.max_rows_in_memory,
            )
            continue

        try:
            # --- Step 1: Load DataFrame ---
            header_arg = sheet.header_row_index if sheet.header_row_detected else None
            df = pd.read_excel(
                file_path, sheet_name=sheet.name, header=header_arg,
            )

            # --- Step 2: Clean column names ---
            if config.clean_column_names:
                cleaned = [clean_name(str(c)) for c in df.columns]
                cleaned = deduplicate_names(cleaned)
                df.columns = cleaned

            # --- Step 3: Auto-detect dates ---
            df = self._auto_detect_dates(df)

            # --- Step 4: Write to DB ---
            table_name = clean_name(sheet.name)
            if not table_name:
                table_name = f"sheet_{profile.sheets.index(sheet)}"
            # Deduplicate table names across sheets
            if table_name in tables:
                suffix = 1
                while f"{table_name}_{suffix}" in tables:
                    suffix += 1
                table_name = f"{table_name}_{suffix}"

            self._db.create_table_from_dataframe(table_name, df)
            written.db_table_names.append(table_name)
            tables.append(table_name)

            # --- Step 5: Generate schema description ---
            schema_text = self._generate_schema_description(table_name, df)

            # --- Step 6: Embed schema + upsert ---
            embed_start = time.monotonic()
            vectors = self._embedder.embed(
                [schema_text], timeout=config.backend_timeout_seconds,
            )
            embed_duration += time.monotonic() - embed_start
            total_texts_embedded += 1

            chunk_hash = hashlib.sha256(schema_text.encode()).hexdigest()
            chunk_id = str(uuid.uuid5(
                uuid.NAMESPACE_URL, f"{ingest_key}:{chunk_hash}",
            ))
            columns_list = list(df.columns)

            metadata = ChunkMetadata(
                source_uri=source_uri,
                source_format="xlsx",
                sheet_name=sheet.name,
                region_id=None,
                ingestion_method=IngestionMethod.SQL_AGENT.value,  # "sql_agent"
                parser_used=sheet.parser_used.value,               # "openpyxl"
                parser_version=config.parser_version,
                chunk_index=chunk_index_counter,
                chunk_hash=chunk_hash,
                ingest_key=ingest_key,
                ingest_run_id=ingest_run_id,
                tenant_id=config.tenant_id,
                table_name=table_name,
                db_uri=db_uri,
                row_count=len(df),
                columns=columns_list,
            )
            chunk = ChunkPayload(
                id=chunk_id,
                text=schema_text,
                vector=vectors[0],
                metadata=metadata,
            )

            self._vector_store.upsert_chunks(collection, [chunk])
            written.vector_point_ids.append(chunk_id)
            chunk_index_counter += 1
            total_chunks += 1

            # --- Step 7: Optional row serialization ---
            if len(df) < config.row_serialization_limit:
                row_chunks = self._serialize_rows(
                    table_name=table_name,
                    df=df,
                    start_chunk_index=chunk_index_counter,
                    sheet=sheet,
                    source_uri=source_uri,
                    db_uri=db_uri,
                    ingest_key=ingest_key,
                    ingest_run_id=ingest_run_id,
                )
                # Embed in batches
                for batch_start in range(0, len(row_chunks), config.embedding_batch_size):
                    batch = row_chunks[batch_start:batch_start + config.embedding_batch_size]
                    texts = [c.text for c in batch]
                    embed_start = time.monotonic()
                    vectors = self._embedder.embed(
                        texts, timeout=config.backend_timeout_seconds,
                    )
                    embed_duration += time.monotonic() - embed_start
                    total_texts_embedded += len(texts)
                    for c, vec in zip(batch, vectors):
                        c.vector = vec

                    self._vector_store.upsert_chunks(collection, list(batch))
                    for c in batch:
                        written.vector_point_ids.append(c.id)
                    total_chunks += len(batch)

                chunk_index_counter += len(row_chunks)

        except Exception as exc:
            # Per-sheet error handling: log, record, continue to next sheet
            error_code = self._classify_backend_error(exc)
            errors.append(error_code.value)
            error_details.append(IngestError(
                code=error_code,
                message=str(exc),
                sheet_name=sheet.name,
                stage="process",
                recoverable=False,
            ))
            logger.exception("Error processing sheet %s: %s", sheet.name, exc)
            continue

    # --- Assemble EmbedStageResult ---
    embed_result = None
    if total_texts_embedded > 0:
        embed_result = EmbedStageResult(
            texts_embedded=total_texts_embedded,
            embedding_dimension=self._embedder.dimension(),
            embed_duration_seconds=embed_duration,
        )

    elapsed = time.monotonic() - start_time

    return ProcessingResult(
        file_path=file_path,
        ingest_key=ingest_key,
        ingest_run_id=ingest_run_id,
        tenant_id=config.tenant_id,
        parse_result=parse_result,
        classification_result=classification_result,
        embed_result=embed_result,
        classification=classification,
        ingestion_method=IngestionMethod.SQL_AGENT,  # enum member, NOT string
        chunks_created=total_chunks,
        tables_created=len(tables),
        tables=tables,
        written=written,
        errors=errors,
        warnings=warnings,
        error_details=error_details,
        processing_time_seconds=elapsed,
    )
```

### 3.4 Private methods

#### `_auto_detect_dates(self, df: pd.DataFrame) -> pd.DataFrame`

```python
def _auto_detect_dates(self, df: pd.DataFrame) -> pd.DataFrame:
    """Auto-detect and convert date columns in-place.

    Two heuristics:
    1. Excel serial dates: numeric columns with values in 40000..50000 range
    2. String dates: object columns where >50% of non-null values parse as dates
    """
    SERIAL_MIN = 35_000   # ~1995
    SERIAL_MAX = 55_000   # ~2050
    DATE_PARSE_THRESHOLD = 0.5

    for col in df.columns:
        series = df[col]
        non_null = series.dropna()
        if len(non_null) == 0:
            continue

        # Heuristic 1: Excel serial dates
        if pd.api.types.is_numeric_dtype(series):
            if (non_null >= SERIAL_MIN).all() and (non_null <= SERIAL_MAX).all():
                try:
                    df[col] = pd.to_datetime(
                        series, origin="1899-12-30", unit="D", errors="coerce",
                    )
                    logger.debug("Converted column '%s' from Excel serial date", col)
                except Exception:
                    pass  # leave as-is if conversion fails
            continue

        # Heuristic 2: String dates (object dtype only)
        if series.dtype == object:
            try:
                parsed = pd.to_datetime(non_null, format="mixed", errors="coerce")
                success_ratio = parsed.notna().sum() / len(non_null)
                if success_ratio >= DATE_PARSE_THRESHOLD:
                    df[col] = pd.to_datetime(
                        series, format="mixed", errors="coerce",
                    )
                    logger.debug(
                        "Converted column '%s' from string dates (%.0f%% parsed)",
                        col, success_ratio * 100,
                    )
            except Exception:
                pass  # leave as-is

    return df
```

**NOTE on `infer_datetime_format`**: The MAP references `infer_datetime_format=True` but this parameter is deprecated in pandas >= 2.0. Use `format="mixed"` instead, which is the modern equivalent. The IMPLEMENT agent should verify pandas version in the project's dependencies and use `format="mixed"` if pandas >= 2.0 (which it will be, given the project uses Python 3.12).

#### `_generate_schema_description(self, table_name: str, df: pd.DataFrame) -> str`

```python
def _generate_schema_description(self, table_name: str, df: pd.DataFrame) -> str:
    """Generate a natural language schema description for embedding.

    Format:
        Table "{table_name}" contains {N} rows with columns:
        - col_name (type): description
        ...
    """
    LOW_CARDINALITY_THRESHOLD = 20
    lines = [f'Table "{table_name}" contains {len(df)} rows with columns:']

    for col in df.columns:
        series = df[col].dropna()
        col_type = self._infer_type_label(df[col])
        desc = self._describe_column(series, col_type, LOW_CARDINALITY_THRESHOLD)
        lines.append(f"- {col} ({col_type}): {desc}")

    return "\n".join(lines)
```

#### `_infer_type_label(self, series: pd.Series) -> str`

```python
@staticmethod
def _infer_type_label(series: pd.Series) -> str:
    """Map a pandas dtype to a human-readable type label."""
    dtype = series.dtype
    if pd.api.types.is_integer_dtype(dtype):
        return "integer"
    if pd.api.types.is_float_dtype(dtype):
        return "float"
    if pd.api.types.is_bool_dtype(dtype):
        return "boolean"
    if pd.api.types.is_datetime64_any_dtype(dtype):
        return "date"
    return "text"
```

#### `_describe_column(self, series: pd.Series, col_type: str, low_card_threshold: int) -> str`

```python
@staticmethod
def _describe_column(
    series: pd.Series, col_type: str, low_card_threshold: int,
) -> str:
    """Generate a human-readable description of a column's values."""
    if len(series) == 0:
        return "no data"

    if col_type in ("integer", "float"):
        return f"range {series.min()} to {series.max()}"

    if col_type == "date":
        return f"ranges from {series.min()} to {series.max()}"

    if col_type == "boolean":
        return "true/false"

    # text
    n_unique = series.nunique()
    if n_unique < low_card_threshold:
        unique_vals = ", ".join(str(v) for v in series.unique()[:low_card_threshold])
        return f"one of {unique_vals}"
    return f"{n_unique} unique values"
```

#### `_serialize_rows(self, ...) -> list[ChunkPayload]`

```python
def _serialize_rows(
    self,
    table_name: str,
    df: pd.DataFrame,
    start_chunk_index: int,
    sheet: SheetProfile,
    source_uri: str,
    db_uri: str,
    ingest_key: str,
    ingest_run_id: str,
) -> list[ChunkPayload]:
    """Serialize each row into a natural language sentence.

    Returns ChunkPayload objects with vectors set to empty lists.
    Vectors are populated later during batch embedding.

    Format: "In table '{table_name}', row {N}: {col} is {val}, ..."
    """
    config = self._config
    columns_list = list(df.columns)
    chunks: list[ChunkPayload] = []

    for row_idx, row in df.iterrows():
        # Build row text
        parts = []
        for col in df.columns:
            val = row[col]
            if pd.isna(val):
                parts.append(f"{col} is N/A")
            else:
                parts.append(f"{col} is {val}")
        row_number = row_idx + 1 if isinstance(row_idx, int) else row_idx
        text = f"In table '{table_name}', row {row_number}: {', '.join(parts)}."

        chunk_hash = hashlib.sha256(text.encode()).hexdigest()
        chunk_id = str(uuid.uuid5(
            uuid.NAMESPACE_URL, f"{ingest_key}:{chunk_hash}",
        ))
        chunk_index = start_chunk_index + len(chunks)

        metadata = ChunkMetadata(
            source_uri=source_uri,
            source_format="xlsx",
            sheet_name=sheet.name,
            region_id=None,
            ingestion_method=IngestionMethod.SQL_AGENT.value,  # "sql_agent"
            parser_used=sheet.parser_used.value,
            parser_version=config.parser_version,
            chunk_index=chunk_index,
            chunk_hash=chunk_hash,
            ingest_key=ingest_key,
            ingest_run_id=ingest_run_id,
            tenant_id=config.tenant_id,
            table_name=table_name,
            db_uri=db_uri,
            row_count=len(df),
            columns=columns_list,
        )
        chunks.append(ChunkPayload(
            id=chunk_id,
            text=text,
            vector=[],  # placeholder; populated during batch embedding
            metadata=metadata,
        ))

    return chunks
```

#### `_classify_backend_error(self, exc: Exception) -> ErrorCode`

```python
@staticmethod
def _classify_backend_error(exc: Exception) -> ErrorCode:
    """Map an exception to the most appropriate ErrorCode.

    Inspects exception type and message to differentiate timeout vs
    connection errors for each backend type.
    """
    msg = str(exc).lower()
    if "timeout" in msg or "timed out" in msg:
        if "embed" in msg:
            return ErrorCode.E_BACKEND_EMBED_TIMEOUT
        if "vector" in msg or "qdrant" in msg or "collection" in msg:
            return ErrorCode.E_BACKEND_VECTOR_TIMEOUT
        return ErrorCode.E_BACKEND_DB_TIMEOUT
    if "connect" in msg or "connection" in msg:
        if "embed" in msg:
            return ErrorCode.E_BACKEND_EMBED_CONNECT
        if "vector" in msg or "qdrant" in msg or "collection" in msg:
            return ErrorCode.E_BACKEND_VECTOR_CONNECT
        return ErrorCode.E_BACKEND_DB_CONNECT
    # Default to schema gen error for unknown exceptions during processing
    return ErrorCode.E_PROCESS_SCHEMA_GEN
```

---

## 4. ChunkMetadata Field Mapping (complete reference)

| ChunkMetadata field | Schema chunk source | Row chunk source |
|---|---|---|
| `source_uri` | `f"file://{Path(file_path).resolve().as_posix()}"` | same |
| `source_format` | `"xlsx"` (default) | same |
| `sheet_name` | `sheet.name` (SheetProfile) | same |
| `region_id` | `None` | `None` |
| `ingestion_method` | `IngestionMethod.SQL_AGENT.value` = `"sql_agent"` | same |
| `parser_used` | `sheet.parser_used.value` (e.g. `"openpyxl"`) | same |
| `parser_version` | `config.parser_version` (e.g. `"ingestkit_excel:1.0.0"`) | same |
| `chunk_index` | `0` for first schema chunk, incrementing globally | continues after schema chunk |
| `chunk_hash` | `hashlib.sha256(schema_text.encode()).hexdigest()` | `hashlib.sha256(row_text.encode()).hexdigest()` |
| `ingest_key` | passed parameter (SHA-256 hex string from `IngestKey.key`) | same |
| `ingest_run_id` | passed parameter (UUID4 string) | same |
| `tenant_id` | `config.tenant_id` | same |
| `table_name` | cleaned table name from `clean_name(sheet.name)` | same |
| `db_uri` | `self._db.get_connection_uri()` | same |
| `row_count` | `len(df)` | same |
| `columns` | `list(df.columns)` after cleaning | same |
| `section_title` | `None` (Path B only) | `None` |
| `original_structure` | `None` (Path B only) | `None` |

### ChunkPayload.id generation

```python
chunk_id = str(uuid.uuid5(uuid.NAMESPACE_URL, f"{ingest_key}:{chunk_hash}"))
```

Deterministic: same content + same ingest_key = same ID (idempotent upserts).

---

## 5. Enum Values Used (complete reference)

| Location | Python expression | Resulting string |
|---|---|---|
| `ChunkMetadata.ingestion_method` | `IngestionMethod.SQL_AGENT.value` | `"sql_agent"` |
| `ChunkMetadata.parser_used` | `sheet.parser_used.value` | `"openpyxl"` or `"pandas_fallback"` |
| `ProcessingResult.ingestion_method` | `IngestionMethod.SQL_AGENT` | enum member (not string) |
| Warnings list entries | `ErrorCode.W_SHEET_SKIPPED_HIDDEN.value` | `"W_SHEET_SKIPPED_HIDDEN"` |
| Warnings list entries | `ErrorCode.W_SHEET_SKIPPED_CHART.value` | `"W_SHEET_SKIPPED_CHART"` |
| Warnings list entries | `ErrorCode.W_ROWS_TRUNCATED.value` | `"W_ROWS_TRUNCATED"` |
| Errors list entries | `ErrorCode.E_BACKEND_DB_TIMEOUT.value` | `"E_BACKEND_DB_TIMEOUT"` |
| Errors list entries | `ErrorCode.E_BACKEND_DB_CONNECT.value` | `"E_BACKEND_DB_CONNECT"` |
| Errors list entries | `ErrorCode.E_BACKEND_VECTOR_TIMEOUT.value` | `"E_BACKEND_VECTOR_TIMEOUT"` |
| Errors list entries | `ErrorCode.E_BACKEND_EMBED_TIMEOUT.value` | `"E_BACKEND_EMBED_TIMEOUT"` |
| Errors list entries | `ErrorCode.E_PROCESS_SCHEMA_GEN.value` | `"E_PROCESS_SCHEMA_GEN"` |
| `IngestError.stage` | literal string | `"process"` or `"embed"` |

---

## 6. File: `src/ingestkit_excel/__init__.py` Update

Add after the `LLMClassifier` import and before `protocols`:

```python
from ingestkit_excel.processors import StructuredDBProcessor
```

Add to `__all__` list after `"LLMClassifier"`:

```python
    # Processors
    "StructuredDBProcessor",
```

---

## 7. File: `tests/test_structured_db.py`

### 7.1 Imports

```python
"""Tests for the StructuredDBProcessor (Path A).

Covers column cleaning, date detection, schema description generation,
row serialization, full process flow, multi-sheet processing, error
handling, and WrittenArtifacts tracking.
"""

from __future__ import annotations

import hashlib
import uuid
from unittest.mock import MagicMock, patch

import pandas as pd
import pytest

from ingestkit_excel.config import ExcelProcessorConfig
from ingestkit_excel.errors import ErrorCode, IngestError
from ingestkit_excel.models import (
    ChunkMetadata,
    ChunkPayload,
    ClassificationResult,
    ClassificationStageResult,
    ClassificationTier,
    EmbedStageResult,
    FileProfile,
    FileType,
    IngestionMethod,
    ParseStageResult,
    ParserUsed,
    ProcessingResult,
    SheetProfile,
    WrittenArtifacts,
)
from ingestkit_excel.processors.structured_db import (
    StructuredDBProcessor,
    clean_name,
    deduplicate_names,
)
```

### 7.2 Test Helper Factories

```python
def _make_sheet_profile(**overrides: object) -> SheetProfile:
    """Build a SheetProfile with sensible tabular defaults."""
    defaults: dict = dict(
        name="Sheet1",
        row_count=100,
        col_count=5,
        merged_cell_count=0,
        merged_cell_ratio=0.0,
        header_row_detected=True,
        header_row_index=0,
        header_values=["A", "B", "C", "D", "E"],
        column_type_consistency=0.9,
        numeric_ratio=0.4,
        text_ratio=0.5,
        empty_ratio=0.1,
        sample_rows=[["1", "a", "x", "2.0", "y"]],
        has_formulas=False,
        is_hidden=False,
        parser_used=ParserUsed.OPENPYXL,
    )
    defaults.update(overrides)
    return SheetProfile(**defaults)


def _make_file_profile(
    sheets: list[SheetProfile], **overrides: object,
) -> FileProfile:
    """Build a FileProfile from a list of SheetProfiles."""
    defaults: dict = dict(
        file_path="/tmp/test.xlsx",
        file_size_bytes=1024,
        sheet_count=len(sheets),
        sheet_names=[s.name for s in sheets],
        sheets=sheets,
        has_password_protected_sheets=False,
        has_chart_only_sheets=False,
        total_merged_cells=sum(s.merged_cell_count for s in sheets),
        total_rows=sum(s.row_count for s in sheets),
        content_hash="a" * 64,
    )
    defaults.update(overrides)
    return FileProfile(**defaults)


def _make_parse_result(**overrides: object) -> ParseStageResult:
    defaults: dict = dict(
        parser_used=ParserUsed.OPENPYXL,
        fallback_reason_code=None,
        sheets_parsed=1,
        sheets_skipped=0,
        skipped_reasons={},
        parse_duration_seconds=0.1,
    )
    defaults.update(overrides)
    return ParseStageResult(**defaults)


def _make_classification_stage_result(**overrides: object) -> ClassificationStageResult:
    defaults: dict = dict(
        tier_used=ClassificationTier.RULE_BASED,
        file_type=FileType.TABULAR_DATA,
        confidence=0.95,
        signals=None,
        reasoning="High column consistency, low merged cells",
        per_sheet_types=None,
        classification_duration_seconds=0.05,
    )
    defaults.update(overrides)
    return ClassificationStageResult(**defaults)


def _make_classification_result(**overrides: object) -> ClassificationResult:
    defaults: dict = dict(
        file_type=FileType.TABULAR_DATA,
        confidence=0.95,
        tier_used=ClassificationTier.RULE_BASED,
        reasoning="High column consistency, low merged cells",
        per_sheet_types=None,
        signals=None,
    )
    defaults.update(overrides)
    return ClassificationResult(**defaults)
```

### 7.3 Mock Backend Design

```python
class MockStructuredDB:
    """Mock StructuredDBBackend for testing."""

    def __init__(self, connection_uri: str = "sqlite:///test.db"):
        self._uri = connection_uri
        self.tables_created: list[tuple[str, pd.DataFrame]] = []
        self.fail_on: str | None = None  # table name to fail on

    def create_table_from_dataframe(self, table_name: str, df: pd.DataFrame) -> None:
        if self.fail_on and table_name == self.fail_on:
            raise RuntimeError(f"DB connection error for table {table_name}")
        self.tables_created.append((table_name, df.copy()))

    def drop_table(self, table_name: str) -> None:
        pass

    def table_exists(self, table_name: str) -> bool:
        return any(t[0] == table_name for t in self.tables_created)

    def get_table_schema(self, table_name: str) -> dict:
        return {}

    def get_connection_uri(self) -> str:
        return self._uri


class MockVectorStore:
    """Mock VectorStoreBackend for testing."""

    def __init__(self):
        self.collections_ensured: list[tuple[str, int]] = []
        self.upserted: list[tuple[str, list[ChunkPayload]]] = []
        self.fail_on_upsert: bool = False

    def upsert_chunks(self, collection: str, chunks: list[ChunkPayload]) -> int:
        if self.fail_on_upsert:
            raise RuntimeError("Vector store connection error")
        self.upserted.append((collection, chunks))
        return len(chunks)

    def ensure_collection(self, collection: str, vector_size: int) -> None:
        self.collections_ensured.append((collection, vector_size))

    def create_payload_index(self, collection: str, field: str, field_type: str) -> None:
        pass

    def delete_by_ids(self, collection: str, ids: list[str]) -> int:
        return len(ids)


class MockEmbedder:
    """Mock EmbeddingBackend for testing."""

    def __init__(self, dim: int = 768):
        self._dim = dim
        self.embed_calls: list[list[str]] = []
        self.fail_on_embed: bool = False

    def embed(self, texts: list[str], timeout: float | None = None) -> list[list[float]]:
        if self.fail_on_embed:
            raise RuntimeError("Embedding timeout error")
        self.embed_calls.append(texts)
        return [[0.1] * self._dim for _ in texts]

    def dimension(self) -> int:
        return self._dim
```

### 7.4 Fixtures

```python
@pytest.fixture()
def mock_db() -> MockStructuredDB:
    return MockStructuredDB()


@pytest.fixture()
def mock_vector_store() -> MockVectorStore:
    return MockVectorStore()


@pytest.fixture()
def mock_embedder() -> MockEmbedder:
    return MockEmbedder()


@pytest.fixture()
def config() -> ExcelProcessorConfig:
    return ExcelProcessorConfig()


@pytest.fixture()
def processor(
    mock_db: MockStructuredDB,
    mock_vector_store: MockVectorStore,
    mock_embedder: MockEmbedder,
    config: ExcelProcessorConfig,
) -> StructuredDBProcessor:
    return StructuredDBProcessor(mock_db, mock_vector_store, mock_embedder, config)
```

### 7.5 Complete Test Case List

#### Section: Column Name Cleaning (`clean_name` and `deduplicate_names`)

| # | Test name | What it verifies |
|---|---|---|
| 1 | `test_clean_name_lowercase` | `"Hello World"` -> `"hello_world"` |
| 2 | `test_clean_name_special_chars` | `"Col #1 (USD)"` -> `"col_1_usd"` |
| 3 | `test_clean_name_consecutive_underscores` | `"a___b"` -> `"a_b"` |
| 4 | `test_clean_name_leading_trailing_underscores` | `"__name__"` -> `"name"` |
| 5 | `test_clean_name_empty_string` | `""` -> `""` (handled by deduplicate_names) |
| 6 | `test_clean_name_unicode` | `"Gebuhrenstatus"` (with umlaut) -> ASCII underscore replacement |
| 7 | `test_clean_name_numeric_only` | `"123"` -> `"123"` |
| 8 | `test_deduplicate_names_no_dupes` | `["a", "b", "c"]` -> `["a", "b", "c"]` |
| 9 | `test_deduplicate_names_with_dupes` | `["a", "a", "a"]` -> `["a", "a_1", "a_2"]` |
| 10 | `test_deduplicate_names_empty_becomes_column_n` | `["", "", "a"]` -> `["column_0", "column_1", "a"]` |
| 11 | `test_deduplicate_names_mixed` | `["id", "", "id", "name"]` -> `["id", "column_1", "id_1", "name"]` |

#### Section: Date Detection (`_auto_detect_dates`)

| # | Test name | What it verifies |
|---|---|---|
| 12 | `test_auto_detect_excel_serial_dates` | Numeric column with values ~44000 converted to datetime |
| 13 | `test_auto_detect_string_dates` | Object column with `"2023-01-15"` strings converted |
| 14 | `test_auto_detect_skips_integers_outside_range` | Column with values 1-100 NOT converted (not in serial range) |
| 15 | `test_auto_detect_mixed_dates_below_threshold` | Object column where <50% parse as dates left unconverted |
| 16 | `test_auto_detect_empty_column_ignored` | All-NaN column not modified |
| 17 | `test_auto_detect_preserves_non_date_columns` | Integer and text columns preserved |

#### Section: Schema Description Generation

| # | Test name | What it verifies |
|---|---|---|
| 18 | `test_schema_description_contains_table_name` | Output starts with `Table "table_name"` |
| 19 | `test_schema_description_integer_column_range` | Integer col shows `"range X to Y"` |
| 20 | `test_schema_description_float_column_range` | Float col shows `"range X to Y"` |
| 21 | `test_schema_description_text_low_cardinality` | Text col with <20 unique shows `"one of ..."` |
| 22 | `test_schema_description_text_high_cardinality` | Text col with >=20 unique shows `"N unique values"` |
| 23 | `test_schema_description_date_column_range` | Date col shows `"ranges from X to Y"` |
| 24 | `test_schema_description_boolean_column` | Bool col shows `"true/false"` |
| 25 | `test_schema_description_row_count` | Output contains `"contains N rows"` |

#### Section: Row Serialization

| # | Test name | What it verifies |
|---|---|---|
| 26 | `test_serialize_rows_format` | Each row text matches `"In table '...', row N: col is val, ..."` |
| 27 | `test_serialize_rows_handles_nan` | NaN values rendered as `"N/A"` |
| 28 | `test_serialize_rows_chunk_metadata_correct` | Each chunk has correct table_name, chunk_index, sheet_name |
| 29 | `test_serialize_rows_chunk_ids_deterministic` | Same content + ingest_key -> same chunk_id |
| 30 | `test_serialize_rows_chunk_index_continues` | chunk_index starts at `start_chunk_index` |

#### Section: Full Process Flow (with `pd.read_excel` mocked)

| # | Test name | What it verifies |
|---|---|---|
| 31 | `test_process_single_sheet_happy_path` | DB table created, vector upserted, ProcessingResult fields correct |
| 32 | `test_process_result_ingestion_method` | `result.ingestion_method == IngestionMethod.SQL_AGENT` (enum member) |
| 33 | `test_process_chunk_metadata_ingestion_method` | metadata value is `"sql_agent"` (string) |
| 34 | `test_process_chunk_metadata_parser_used` | metadata value is `"openpyxl"` (string) |
| 35 | `test_process_written_artifacts_populated` | `written.db_table_names` and `written.vector_point_ids` non-empty |
| 36 | `test_process_written_artifacts_collection` | `written.vector_collection == "helpdesk"` (from config default) |
| 37 | `test_process_source_uri_format` | ChunkMetadata.source_uri starts with `"file://"` |
| 38 | `test_process_db_uri_from_backend` | ChunkMetadata.db_uri matches `mock_db.get_connection_uri()` |
| 39 | `test_process_embed_result_populated` | `result.embed_result` is not None, texts_embedded > 0 |
| 40 | `test_process_tables_created_count` | `result.tables_created == 1` for single sheet |
| 41 | `test_process_chunks_created_count_schema_only` | With row_serialization_limit=0, only schema chunk counted |
| 42 | `test_process_tenant_id_passed_through` | ChunkMetadata.tenant_id and result.tenant_id match config |

#### Section: Row Serialization Integration (triggers / skips)

| # | Test name | What it verifies |
|---|---|---|
| 43 | `test_process_row_serialization_below_limit` | With 3 rows and limit=5000, schema + 3 row chunks = 4 total |
| 44 | `test_process_row_serialization_above_limit` | With 100 rows and limit=50, only schema chunk = 1 total |
| 45 | `test_process_row_serialization_batch_embedding` | With 130 rows and batch_size=64, embedder called 3 times (1 schema + 2 row batches) |

#### Section: Multi-Sheet Processing

| # | Test name | What it verifies |
|---|---|---|
| 46 | `test_process_multi_sheet` | 2 sheets -> 2 tables, 2+ chunks, both table names in written |
| 47 | `test_process_multi_sheet_chunk_index_global` | chunk_index is global across sheets (sheet2 schema chunk continues from sheet1 last index) |
| 48 | `test_process_duplicate_sheet_names_deduped` | Two sheets named "Data" -> tables "data" and "data_1" |

#### Section: Sheet Skipping

| # | Test name | What it verifies |
|---|---|---|
| 49 | `test_process_skips_hidden_sheet` | Hidden sheet skipped, warning `"W_SHEET_SKIPPED_HIDDEN"` added |
| 50 | `test_process_skips_oversized_sheet` | Sheet with row_count > max_rows_in_memory skipped, warning added |
| 51 | `test_process_all_sheets_skipped_empty_result` | All sheets skipped -> tables_created=0, chunks_created=0 |

#### Section: Error Handling

| # | Test name | What it verifies |
|---|---|---|
| 52 | `test_process_db_failure_records_error` | DB exception -> errors list contains error code, error_details has IngestError |
| 53 | `test_process_db_failure_continues_to_next_sheet` | Sheet 1 DB fails, Sheet 2 succeeds -> 1 table, 1 error |
| 54 | `test_process_embed_failure_records_error` | Embedder exception -> error recorded |
| 55 | `test_process_error_details_stage_is_process` | IngestError.stage == "process" |
| 56 | `test_process_error_details_has_sheet_name` | IngestError.sheet_name populated |

#### Section: Config Interactions

| # | Test name | What it verifies |
|---|---|---|
| 57 | `test_process_clean_column_names_disabled` | config.clean_column_names=False -> columns NOT cleaned |
| 58 | `test_process_custom_collection` | config.default_collection="custom" -> used in ensure_collection and upserts |

### 7.6 Test implementation pattern for `process()` tests

All `process()` tests must mock `pd.read_excel` since they do not use real files:

```python
@patch("ingestkit_excel.processors.structured_db.pd.read_excel")
def test_process_single_sheet_happy_path(
    mock_read_excel, processor, mock_db, mock_vector_store, mock_embedder,
):
    df = pd.DataFrame({"id": [1, 2, 3], "name": ["Alice", "Bob", "Carol"]})
    mock_read_excel.return_value = df

    sheet = _make_sheet_profile(name="Employees", row_count=3, col_count=2)
    profile = _make_file_profile([sheet])
    parse_result = _make_parse_result()
    classification_stage = _make_classification_stage_result()
    classification = _make_classification_result()

    result = processor.process(
        file_path="/tmp/test.xlsx",
        profile=profile,
        ingest_key="abc123" * 10 + "abcd",
        ingest_run_id="550e8400-e29b-41d4-a716-446655440000",
        parse_result=parse_result,
        classification_result=classification_stage,
        classification=classification,
    )

    # Verify DB interaction
    assert len(mock_db.tables_created) == 1
    assert mock_db.tables_created[0][0] == "employees"

    # Verify vector store interaction
    assert len(mock_vector_store.upserted) >= 1
    assert mock_vector_store.collections_ensured[0] == ("helpdesk", 768)

    # Verify result structure
    assert result.ingestion_method == IngestionMethod.SQL_AGENT
    assert result.tables_created == 1
    assert result.tables == ["employees"]
    assert result.written.db_table_names == ["employees"]
    assert len(result.written.vector_point_ids) >= 1
    assert result.written.vector_collection == "helpdesk"
    assert result.errors == []
    assert result.processing_time_seconds > 0
```

---

## 8. Implementation Order

1. **Create `src/ingestkit_excel/processors/__init__.py`** -- minimal, just the re-export
2. **Create `src/ingestkit_excel/processors/structured_db.py`** -- module-level helpers first (`clean_name`, `deduplicate_names`), then the class with all methods
3. **Create `tests/test_structured_db.py`** -- all tests
4. **Update `src/ingestkit_excel/__init__.py`** -- add `StructuredDBProcessor` to imports and `__all__`
5. **Run tests** -- `pytest tests/test_structured_db.py -v`

---

## 9. Critical Implementation Notes

### 9.1 ENUM_VALUE Pattern Prevention

- `ChunkMetadata.ingestion_method` MUST be `IngestionMethod.SQL_AGENT.value` which is `"sql_agent"` -- NOT `"SQL_AGENT"`
- `ProcessingResult.ingestion_method` MUST be `IngestionMethod.SQL_AGENT` (the enum member itself) -- the field type is `IngestionMethod`, not `str`
- `ChunkMetadata.parser_used` MUST use `sheet.parser_used.value` (e.g. `"openpyxl"`) -- NOT `"OPENPYXL"`
- Error/warning strings in `ProcessingResult.errors` and `ProcessingResult.warnings` MUST use `ErrorCode.*.value` (e.g. `"W_SHEET_SKIPPED_HIDDEN"`)

### 9.2 COMPONENT_API Pattern Prevention

- `VectorStoreBackend.upsert_chunks(collection, chunks)` -- NOT `upsert(collection, chunks)`
- `EmbeddingBackend.embed(texts, timeout=...)` -- returns `list[list[float]]`, NOT `list[float]`
- `StructuredDBBackend.create_table_from_dataframe(table_name, df)` -- returns `None`
- `EmbeddingBackend.dimension()` -- method call with parens, NOT property

### 9.3 VERIFICATION_GAP Prevention

- `ProcessingResult` requires `parse_result` and `classification_result` (no defaults) -- the processor MUST accept and pass them through
- `ProcessingResult.classification` is a separate field from `classification_result` -- it uses `ClassificationResult` (simpler model), not `ClassificationStageResult`
- `WrittenArtifacts.vector_point_ids` is `list[str]`, append individual `str` IDs
- `SheetProfile.header_row_index` is `int | None` -- use it as the `header` arg for `pd.read_excel`
- `ChunkPayload.vector` is `list[float]` -- cannot be `None`, use `[]` as placeholder for row chunks before embedding

### 9.4 pandas Version Note

The project uses Python 3.12 which means pandas >= 2.0 is required. Use `format="mixed"` instead of the deprecated `infer_datetime_format=True` parameter in `pd.to_datetime()`.

### 9.5 source_uri Convention

Use `f"file://{Path(file_path).resolve().as_posix()}"` to match the SPEC example format `"file:///path/to/roster.xlsx"`. This differs slightly from `idempotency.py` which uses just the POSIX path without `file://`, but the processor follows the SPEC section 10.1 convention for ChunkMetadata.

### 9.6 Table Name Deduplication

Table names must be deduplicated across sheets within a single `process()` call. If two sheets both clean to the same table name, append `_1`, `_2`, etc. This uses a simpler approach than `deduplicate_names` since it is across a growing list rather than a batch.

### 9.7 Global chunk_index

The `chunk_index` counter is global across all sheets in a single `process()` invocation. It does NOT reset per sheet. This ensures every chunk has a unique index within the result.
