---
issue: 38
agent: PLAN
date: 2026-02-14
complexity: COMPLEX
stack: backend
depends_on: map-38-021426.md
files_to_create:
  - packages/ingestkit-pdf/src/ingestkit_pdf/processors/text_extractor.py
  - packages/ingestkit-pdf/tests/test_text_extractor.py
files_to_modify:
  - packages/ingestkit-pdf/src/ingestkit_pdf/processors/__init__.py
  - packages/ingestkit-pdf/src/ingestkit_pdf/__init__.py
status: COMPLETE
---

# PLAN Artifact: Issue #38 -- Path A Text Extractor

## Executive Summary

Issue #38 implements `processors/text_extractor.py` (~350 lines) containing the `TextExtractor` class -- the fast-path processor for text-native PDFs (Type A). The 10-step pipeline extracts markdown via pymupdf4llm, assesses quality with block-level fallback, strips headers/footers, skips TOC/blank pages, detects headings, extracts metadata, chunks text, embeds chunks, and upserts to the vector store. Tests (~450 lines) mock `fitz`, `pymupdf4llm`, and all backends. The implementation follows the Excel `StructuredDBProcessor` pattern (DI, error accumulation, batch embedding, `WrittenArtifacts` tracking).

## File-by-File Implementation

---

### 1. `packages/ingestkit-pdf/src/ingestkit_pdf/processors/text_extractor.py` (~350 lines)

#### Imports

```python
from __future__ import annotations

import hashlib
import logging
import re
import time
import uuid
from pathlib import Path
from typing import TYPE_CHECKING

from ingestkit_core.models import ChunkPayload, EmbedStageResult, WrittenArtifacts
from ingestkit_pdf.config import PDFProcessorConfig
from ingestkit_pdf.errors import ErrorCode, IngestError
from ingestkit_pdf.models import (
    ClassificationResult,
    ClassificationStageResult,
    DocumentProfile,
    ExtractionQuality,
    ExtractionQualityGrade,
    IngestionMethod,
    ParseStageResult,
    PDFChunkMetadata,
    ProcessingResult,
)
from ingestkit_pdf.quality import QualityAssessor
from ingestkit_pdf.utils.chunker import PDFChunker
from ingestkit_pdf.utils.header_footer import HeaderFooterDetector
from ingestkit_pdf.utils.heading_detector import HeadingDetector
from ingestkit_pdf.utils.language import detect_language

if TYPE_CHECKING:
    from ingestkit_core.protocols import EmbeddingBackend, VectorStoreBackend
```

Logger: `logger = logging.getLogger("ingestkit_pdf.processors.text_extractor")`

#### 1a. `TextExtractor.__init__()` Constructor

```python
class TextExtractor:
    """Path A processor: text-native PDF extraction via pymupdf4llm."""

    def __init__(
        self,
        vector_store: VectorStoreBackend,
        embedder: EmbeddingBackend,
        config: PDFProcessorConfig,
    ) -> None:
        self._vector_store = vector_store
        self._embedder = embedder
        self._config = config
```

Follows the SPEC signature exactly. No `StructuredDBBackend` or `LLMBackend` needed for Path A.

#### 1b. `TextExtractor.process()` Method -- Main Orchestrator

Signature follows MAP Decision D1 (Option A) -- extended to accept classification parameters like the Excel pattern:

```python
def process(
    self,
    file_path: str,
    profile: DocumentProfile,
    ingest_key: str,
    ingest_run_id: str,
    parse_result: ParseStageResult,
    classification_result: ClassificationStageResult,
    classification: ClassificationResult,
) -> ProcessingResult:
```

**Implementation flow:**

```python
start_time = time.monotonic()
config = self._config
collection = config.default_collection
source_uri = f"file://{Path(file_path).resolve().as_posix()}"

errors: list[str] = []
warnings: list[str] = []
error_details: list[IngestError] = []
written = WrittenArtifacts(vector_collection=collection)

# Step 1-5: Extract text per page
page_texts, page_boundaries, extraction_warnings, extraction_errors = (
    self._extract_pages(file_path, config, warnings, error_details)
)
warnings.extend(extraction_warnings)

# If no text extracted at all, return early with zero chunks
if not page_texts:
    # ... assemble empty ProcessingResult
    # See section 1k for early-return assembly

# Step 6: Heading detection
import fitz
with fitz.open(file_path) as doc:
    heading_detector = HeadingDetector(config)
    raw_headings = heading_detector.detect(doc)  # (level, title, page_number)

# Convert page-number-based headings to char-offset-based headings (MAP D4)
headings = self._convert_headings_to_offsets(raw_headings, page_boundaries)

# Step 7: Language detection
language = config.default_language
if config.enable_language_detection and page_texts:
    first_text = next((t for t in page_texts.values() if t.strip()), "")
    if first_text:
        language, _ = detect_language(first_text, default_language=config.default_language)

# Document metadata from profile
doc_title = profile.metadata.title
doc_author = profile.metadata.author
doc_date = profile.metadata.creation_date

# Concatenate all page texts into single document text
full_text, final_page_boundaries = self._concatenate_pages(page_texts)

# Step 8: Chunking
chunker = PDFChunker(config)
chunk_dicts = chunker.chunk(full_text, headings, final_page_boundaries)

# Steps 9-10: Embed and upsert
embed_result, chunk_count = self._embed_and_upsert(
    chunk_dicts=chunk_dicts,
    source_uri=source_uri,
    ingest_key=ingest_key,
    ingest_run_id=ingest_run_id,
    collection=collection,
    doc_title=doc_title,
    doc_author=doc_author,
    doc_date=doc_date,
    language=language,
    written=written,
    errors=errors,
    warnings=warnings,
    error_details=error_details,
)

elapsed = time.monotonic() - start_time

return ProcessingResult(
    file_path=file_path,
    ingest_key=ingest_key,
    ingest_run_id=ingest_run_id,
    tenant_id=config.tenant_id,
    parse_result=parse_result,
    classification_result=classification_result,
    embed_result=embed_result,
    classification=classification,
    ingestion_method=IngestionMethod.TEXT_EXTRACTION,
    chunks_created=chunk_count,
    tables_created=0,
    tables=[],
    written=written,
    errors=errors,
    warnings=warnings,
    error_details=error_details,
    processing_time_seconds=elapsed,
)
```

#### 1c. `_extract_pages()` Private Method -- Steps 1-5

Returns `(page_texts: dict[int, str], page_boundaries: list[int], warnings: list[str], error_details: list[IngestError])`.

`page_texts` is a dict mapping 1-based page number to cleaned text. Pages skipped (TOC, blank) are excluded.

```python
def _extract_pages(
    self,
    file_path: str,
    config: PDFProcessorConfig,
    warnings: list[str],
    error_details: list[IngestError],
) -> tuple[dict[int, str], list[int], list[str], list[IngestError]]:
    import fitz
    import pymupdf4llm

    extra_warnings: list[str] = []
    extra_errors: list[IngestError] = []
    page_texts: dict[int, str] = {}
    quality_assessor = QualityAssessor(config)

    with fitz.open(file_path) as doc:
        # Step 1: Extract markdown per page
        page_chunks = pymupdf4llm.to_markdown(
            doc, page_chunks=True, header=False, footer=False
        )

        # Step 3: Header/footer detection (needs full doc)
        hf_detector = HeaderFooterDetector(config)
        try:
            header_patterns, footer_patterns = hf_detector.detect(doc)
        except Exception as exc:
            extra_warnings.append(ErrorCode.E_PROCESS_HEADER_FOOTER.value)
            extra_errors.append(IngestError(
                code=ErrorCode.E_PROCESS_HEADER_FOOTER,
                message=str(exc),
                stage="process",
                recoverable=True,
            ))
            header_patterns, footer_patterns = [], []

        for chunk in page_chunks:
            metadata = chunk.get("metadata", {})
            page_number = metadata.get("page", 0) + 1  # pymupdf4llm uses 0-based
            text = chunk.get("text", "")

            # Step 2: Quality assessment with block-level fallback
            page_quality = quality_assessor.assess_page(text, page_number - 1)
            if quality_assessor.needs_ocr_fallback(page_quality) and config.auto_ocr_fallback:
                extra_warnings.append(ErrorCode.W_QUALITY_LOW_NATIVE.value)
                # Block-level fallback (MAP D5)
                page_obj = doc[page_number - 1]
                blocks = page_obj.get_text("blocks")
                block_texts = [
                    b[4].strip() for b in blocks
                    if b[6] == 0 and b[4].strip()  # type 0 = text
                ]
                fallback_text = "\n".join(block_texts)
                # Re-assess
                fb_quality = quality_assessor.assess_page(fallback_text, page_number - 1)
                if fb_quality.grade != ExtractionQualityGrade.LOW:
                    text = fallback_text
                else:
                    extra_warnings.append(ErrorCode.W_OCR_FALLBACK.value)
                    # Keep original text even if still low quality

            # Step 3 (continued): Strip headers/footers from this page
            text = hf_detector.strip(text, page_number, header_patterns, footer_patterns)

            # Step 4: TOC page detection (MAP D2)
            if self._is_toc_page(text):
                extra_warnings.append(ErrorCode.W_PAGE_SKIPPED_TOC.value)
                logger.info("Skipping TOC page %d", page_number)
                continue

            # Step 5: Blank page detection (MAP D3)
            if self._is_blank_page(text, config):
                extra_warnings.append(ErrorCode.W_PAGE_SKIPPED_BLANK.value)
                logger.info("Skipping blank page %d", page_number)
                continue

            page_texts[page_number] = text

    return page_texts, list(page_texts.keys()), extra_warnings, extra_errors
```

#### 1d. `_is_toc_page()` Private Static Method

Per MAP Decision D2: a page is TOC if >30% of non-empty lines match dot-leader + page-number patterns.

```python
_TOC_PATTERN = re.compile(r"\.{2,}\s*\d+\s*$")

@staticmethod
def _is_toc_page(text: str) -> bool:
    """Return True if the page appears to be a Table of Contents."""
    lines = [line.strip() for line in text.splitlines() if line.strip()]
    if len(lines) < 3:
        return False
    toc_lines = sum(1 for line in lines if TextExtractor._TOC_PATTERN.search(line))
    return toc_lines / len(lines) > 0.3
```

#### 1e. `_is_blank_page()` Private Static Method

Per MAP Decision D3.

```python
@staticmethod
def _is_blank_page(text: str, config: PDFProcessorConfig) -> bool:
    """Return True if the page is effectively blank."""
    stripped = text.strip()
    if not stripped:
        return True
    return len(stripped.split()) < config.quality_min_words_per_page
```

#### 1f. `_convert_headings_to_offsets()` Private Method

Per MAP Decision D4: converts `(level, title, page_number)` tuples from `HeadingDetector` to `(level, title, char_offset)` tuples for `PDFChunker`.

```python
@staticmethod
def _convert_headings_to_offsets(
    raw_headings: list[tuple[int, str, int]],
    page_boundaries: dict[int, int],  # page_number -> char_offset in concatenated text
) -> list[tuple[int, str, int]]:
    """Map page-number-based headings to character-offset-based headings."""
    result: list[tuple[int, str, int]] = []
    for level, title, page_num in raw_headings:
        char_offset = page_boundaries.get(page_num, 0)
        result.append((level, title, char_offset))
    return result
```

Note: `page_boundaries` here is computed during concatenation (section 1g).

#### 1g. `_concatenate_pages()` Private Method

Joins page texts in page-number order. Returns the full document text and a list of character offsets where each page starts (for the chunker's `page_boundaries` parameter).

```python
@staticmethod
def _concatenate_pages(
    page_texts: dict[int, str],
) -> tuple[str, list[int]]:
    """Concatenate page texts and compute page boundary offsets."""
    sorted_pages = sorted(page_texts.keys())
    parts: list[str] = []
    page_boundary_list: list[int] = []  # char offset for start of each page
    offset = 0

    for page_num in sorted_pages:
        page_boundary_list.append(offset)
        text = page_texts[page_num]
        parts.append(text)
        offset += len(text) + 1  # +1 for joining newline

    full_text = "\n".join(parts)
    return full_text, page_boundary_list
```

**Important**: The `_convert_headings_to_offsets()` method needs a mapping from page number to char offset. This is built from the concatenation step. The actual implementation will compute `page_offset_map: dict[int, int]` mapping `page_number -> char_offset` during concatenation, and pass that to `_convert_headings_to_offsets()`.

Revised flow in `process()`:
```python
full_text, page_boundary_list, page_offset_map = self._concatenate_pages(page_texts)
headings = self._convert_headings_to_offsets(raw_headings, page_offset_map)
chunk_dicts = chunker.chunk(full_text, headings, page_boundary_list)
```

Where `_concatenate_pages` returns an additional `dict[int, int]` mapping page numbers to their character offsets.

#### 1h. `_embed_and_upsert()` Private Method -- Steps 9-10

Follows the Excel batch embedding pattern.

```python
def _embed_and_upsert(
    self,
    chunk_dicts: list[dict],
    source_uri: str,
    ingest_key: str,
    ingest_run_id: str,
    collection: str,
    doc_title: str | None,
    doc_author: str | None,
    doc_date: str | None,
    language: str,
    written: WrittenArtifacts,
    errors: list[str],
    warnings: list[str],
    error_details: list[IngestError],
) -> tuple[EmbedStageResult | None, int]:
    config = self._config
    total_chunks = 0
    total_texts_embedded = 0
    embed_duration = 0.0

    # Ensure collection exists
    vector_size = self._embedder.dimension()
    self._vector_store.ensure_collection(collection, vector_size)

    # Build ChunkPayload list (vectors empty initially)
    payloads: list[ChunkPayload] = []
    for cd in chunk_dicts:
        chunk_hash = cd["chunk_hash"]
        chunk_id = str(uuid.uuid5(uuid.NAMESPACE_URL, f"{ingest_key}:{chunk_hash}"))

        metadata = PDFChunkMetadata(
            source_uri=source_uri,
            source_format="pdf",
            page_numbers=cd["page_numbers"],
            ingestion_method=IngestionMethod.TEXT_EXTRACTION.value,
            parser_version=config.parser_version,
            chunk_index=cd["chunk_index"],
            chunk_hash=chunk_hash,
            ingest_key=ingest_key,
            ingest_run_id=ingest_run_id,
            tenant_id=config.tenant_id,
            heading_path=cd["heading_path"],
            content_type=cd["content_type"],
            doc_title=doc_title,
            doc_author=doc_author,
            doc_date=doc_date,
            language=language,
        )
        payloads.append(ChunkPayload(
            id=chunk_id,
            text=cd["text"],
            vector=[],  # placeholder
            metadata=metadata,
        ))

    # Batch embed and upsert
    for batch_start in range(0, len(payloads), config.embedding_batch_size):
        batch = payloads[batch_start : batch_start + config.embedding_batch_size]
        texts = [p.text for p in batch]

        try:
            embed_start = time.monotonic()
            vectors = self._embedder.embed(texts, timeout=config.backend_timeout_seconds)
            embed_duration += time.monotonic() - embed_start
            total_texts_embedded += len(texts)

            for payload, vec in zip(batch, vectors):
                payload.vector = vec

            self._vector_store.upsert_chunks(collection, batch)
            for payload in batch:
                written.vector_point_ids.append(payload.id)
            total_chunks += len(batch)

        except Exception as exc:
            error_code = self._classify_backend_error(exc)
            errors.append(error_code.value)
            error_details.append(IngestError(
                code=error_code,
                message=str(exc),
                stage="process",
                recoverable=False,
            ))
            logger.exception("Embedding/upsert batch failed: %s", exc)
            continue  # skip this batch, try next

    embed_result = None
    if total_texts_embedded > 0:
        embed_result = EmbedStageResult(
            texts_embedded=total_texts_embedded,
            embedding_dimension=self._embedder.dimension(),
            embed_duration_seconds=embed_duration,
        )

    return embed_result, total_chunks
```

#### 1i. `_classify_backend_error()` Private Static Method

Same pattern as Excel's `StructuredDBProcessor._classify_backend_error()` but without DB error codes (Path A doesn't use DB).

```python
@staticmethod
def _classify_backend_error(exc: Exception) -> ErrorCode:
    """Map an exception to the most appropriate ErrorCode."""
    msg = str(exc).lower()
    if "timeout" in msg or "timed out" in msg:
        if "embed" in msg:
            return ErrorCode.E_BACKEND_EMBED_TIMEOUT
        return ErrorCode.E_BACKEND_VECTOR_TIMEOUT
    if "connect" in msg or "connection" in msg:
        if "embed" in msg:
            return ErrorCode.E_BACKEND_EMBED_CONNECT
        return ErrorCode.E_BACKEND_VECTOR_CONNECT
    return ErrorCode.E_PROCESS_CHUNK
```

#### 1j. PII-Safe Logging

Throughout the implementation:
- Use `logger.info` for page skip messages (page numbers only, no content)
- Use `logger.debug` for chunk counts and timing
- Never log raw page text unless `config.log_sample_text is True`
- Never log chunk previews unless `config.log_chunk_previews is True`

```python
if config.log_sample_text:
    logger.debug("Page %d text sample: %s...", page_number, text[:100])
```

#### 1k. Early Return for Empty Document

If `_extract_pages()` returns no text (all pages blank/TOC), return a `ProcessingResult` with zero chunks:

```python
if not page_texts:
    elapsed = time.monotonic() - start_time
    return ProcessingResult(
        file_path=file_path,
        ingest_key=ingest_key,
        ingest_run_id=ingest_run_id,
        tenant_id=config.tenant_id,
        parse_result=parse_result,
        classification_result=classification_result,
        embed_result=None,
        classification=classification,
        ingestion_method=IngestionMethod.TEXT_EXTRACTION,
        chunks_created=0,
        tables_created=0,
        tables=[],
        written=WrittenArtifacts(vector_collection=config.default_collection),
        errors=errors,
        warnings=warnings,
        error_details=error_details,
        processing_time_seconds=elapsed,
    )
```

---

### 2. `packages/ingestkit-pdf/src/ingestkit_pdf/processors/__init__.py` (Modify)

Current file is empty (1 line). Add:

```python
"""Processing path implementations for ingestkit-pdf."""

from ingestkit_pdf.processors.text_extractor import TextExtractor

__all__ = ["TextExtractor"]
```

---

### 3. `packages/ingestkit-pdf/src/ingestkit_pdf/__init__.py` (Modify)

Add `TextExtractor` to the public API. Append to existing imports and `__all__`:

```python
from ingestkit_pdf.processors.text_extractor import TextExtractor

__all__ = [
    "LLMClassificationResponse",
    "PDFLLMClassifier",
    "TextExtractor",
]
```

---

### 4. `packages/ingestkit-pdf/tests/test_text_extractor.py` (~450 lines)

All tests use `@pytest.mark.unit`. All external dependencies (fitz, pymupdf4llm, backends) are mocked.

#### Imports

```python
from __future__ import annotations

import hashlib
import uuid
from typing import Any
from unittest.mock import MagicMock, patch

import pytest

from ingestkit_core.models import ChunkPayload, EmbedStageResult, WrittenArtifacts
from ingestkit_pdf.config import PDFProcessorConfig
from ingestkit_pdf.errors import ErrorCode
from ingestkit_pdf.models import (
    ClassificationResult,
    ClassificationStageResult,
    DocumentProfile,
    ExtractionQuality,
    ExtractionQualityGrade,
    IngestionMethod,
    PDFChunkMetadata,
    PDFType,
    PageType,
    ParseStageResult,
    ProcessingResult,
)
from ingestkit_pdf.processors.text_extractor import TextExtractor

from conftest import _make_document_profile, _make_page_profile
```

#### Mock Fixtures

```python
@pytest.fixture()
def mock_vector_store():
    store = MagicMock()
    store.ensure_collection = MagicMock()
    store.upsert_chunks = MagicMock(return_value=1)
    return store

@pytest.fixture()
def mock_embedder():
    embedder = MagicMock()
    embedder.dimension.return_value = 768
    embedder.embed.return_value = [[0.1] * 768]
    return embedder

@pytest.fixture()
def config():
    return PDFProcessorConfig()

@pytest.fixture()
def extractor(mock_vector_store, mock_embedder, config):
    return TextExtractor(mock_vector_store, mock_embedder, config)
```

**Shared helpers for building process() arguments:**

```python
def _make_parse_result(**overrides):
    defaults = dict(
        pages_extracted=5,
        pages_skipped=0,
        skipped_reasons={},
        extraction_method="native",
        overall_quality=ExtractionQuality(
            printable_ratio=0.95, avg_words_per_page=300.0,
            pages_with_text=5, total_pages=5, extraction_method="native",
        ),
        parse_duration_seconds=0.5,
    )
    defaults.update(overrides)
    return ParseStageResult(**defaults)

def _make_classification_result(**overrides):
    defaults = dict(
        pdf_type=PDFType.TEXT_NATIVE,
        confidence=0.95,
        tier_used="rule_based",
        reasoning="Text-native PDF",
        per_page_types={1: PageType.TEXT},
    )
    defaults.update(overrides)
    return ClassificationResult(**defaults)

def _make_classification_stage_result(**overrides):
    defaults = dict(
        tier_used="rule_based",
        pdf_type=PDFType.TEXT_NATIVE,
        confidence=0.95,
        reasoning="Text-native PDF",
        per_page_types={1: PageType.TEXT},
        classification_duration_seconds=0.1,
    )
    defaults.update(overrides)
    return ClassificationStageResult(**defaults)

def _mock_pymupdf4llm_output(page_texts: dict[int, str]):
    """Build mock pymupdf4llm.to_markdown output from page_number->text dict."""
    return [
        {"text": text, "metadata": {"page": page_num - 1}}
        for page_num, text in sorted(page_texts.items())
    ]
```

#### 4a. `TestTextExtractorInit` (2 tests)

| Test | What It Tests |
|---|---|
| `test_constructor_stores_dependencies` | Verify `_vector_store`, `_embedder`, `_config` are stored |
| `test_constructor_accepts_protocol_types` | MagicMock satisfies protocol (no type error) |

#### 4b. `TestProcessHappyPath` (5 tests)

All mock `fitz.open()` and `pymupdf4llm.to_markdown()`.

| Test | What It Tests |
|---|---|
| `test_basic_extraction_returns_processing_result` | Single page, simple text -> returns `ProcessingResult` with `chunks_created >= 1` |
| `test_ingestion_method_is_text_extraction` | Result has `ingestion_method == IngestionMethod.TEXT_EXTRACTION` |
| `test_written_artifacts_tracked` | `written.vector_point_ids` is non-empty, `written.vector_collection == "helpdesk"` |
| `test_embed_result_populated` | `embed_result` is not None, has correct `texts_embedded` and `embedding_dimension` |
| `test_processing_time_recorded` | `processing_time_seconds > 0` |

Mock setup pattern for happy path:
```python
@patch("ingestkit_pdf.processors.text_extractor.pymupdf4llm")
@patch("ingestkit_pdf.processors.text_extractor.fitz")
def test_basic_extraction(mock_fitz, mock_pymupdf4llm, extractor, ...):
    mock_doc = MagicMock()
    mock_doc.__len__ = MagicMock(return_value=1)
    mock_doc.__enter__ = MagicMock(return_value=mock_doc)
    mock_doc.__exit__ = MagicMock(return_value=False)
    mock_doc.get_toc.return_value = []
    mock_fitz.open.return_value = mock_doc

    mock_pymupdf4llm.to_markdown.return_value = [
        {"text": "Some meaningful text content here ...", "metadata": {"page": 0}}
    ]
    # ... call process() and assert
```

#### 4c. `TestTOCPageDetection` (3 tests)

| Test | What It Tests |
|---|---|
| `test_toc_page_skipped_with_warning` | Page with >30% dot-leader lines -> skipped, `W_PAGE_SKIPPED_TOC` in warnings |
| `test_non_toc_page_not_skipped` | Normal text page -> not skipped |
| `test_toc_detection_threshold` | Page with exactly 30% dot-leaders -> NOT skipped (>30% required) |

TOC page text fixture:
```python
_TOC_TEXT = """Table of Contents
Chapter 1 .............. 1
Chapter 2 .............. 15
Chapter 3 .............. 28
Chapter 4 .............. 42
"""
```

#### 4d. `TestBlankPageDetection` (3 tests)

| Test | What It Tests |
|---|---|
| `test_empty_page_skipped` | Empty string page -> skipped, `W_PAGE_SKIPPED_BLANK` in warnings |
| `test_whitespace_only_page_skipped` | `"   \n  \n  "` -> skipped |
| `test_few_words_page_skipped` | Page with fewer words than `quality_min_words_per_page` -> skipped |

#### 4e. `TestHeaderFooterStripping` (2 tests)

| Test | What It Tests |
|---|---|
| `test_headers_stripped_from_pages` | Mock `HeaderFooterDetector.detect()` returns patterns, verify `strip()` is called per page |
| `test_header_footer_error_handled_gracefully` | `detect()` raises -> warning logged, processing continues |

#### 4f. `TestQualityFallback` (3 tests)

| Test | What It Tests |
|---|---|
| `test_low_quality_triggers_block_fallback` | Garbled text -> `W_QUALITY_LOW_NATIVE` warning, block fallback attempted |
| `test_block_fallback_improves_quality` | Fallback text has better quality -> used instead |
| `test_block_fallback_still_low_adds_ocr_warning` | Both texts low quality -> `W_OCR_FALLBACK` warning, original kept |

#### 4g. `TestHeadingDetection` (2 tests)

| Test | What It Tests |
|---|---|
| `test_headings_propagated_to_chunks` | Mock headings -> chunk metadata has correct `heading_path` |
| `test_heading_offset_conversion` | `_convert_headings_to_offsets()` correctly maps page numbers to char offsets |

#### 4h. `TestChunkMetadata` (4 tests)

| Test | What It Tests |
|---|---|
| `test_chunk_metadata_has_all_fields` | Every `PDFChunkMetadata` field is populated correctly |
| `test_doc_title_propagated` | `doc_title` from profile metadata appears in chunk metadata |
| `test_language_propagated` | Detected language appears in chunk metadata |
| `test_tenant_id_propagated` | `config.tenant_id` flows through to chunk metadata |

#### 4i. `TestEmbeddingAndUpsert` (4 tests)

| Test | What It Tests |
|---|---|
| `test_embedder_called_with_chunk_texts` | `embedder.embed()` called with correct text batches |
| `test_batch_size_respected` | With 150 chunks and batch_size=64, `embed()` called 3 times |
| `test_vector_store_upsert_called` | `vector_store.upsert_chunks()` called with `ChunkPayload` list |
| `test_ensure_collection_called_first` | `ensure_collection()` called before any `upsert_chunks()` |

#### 4j. `TestErrorHandling` (4 tests)

| Test | What It Tests |
|---|---|
| `test_embed_timeout_classified_correctly` | Exception with "timeout" + "embed" -> `E_BACKEND_EMBED_TIMEOUT` |
| `test_vector_connect_error_classified` | Exception with "connection" -> `E_BACKEND_VECTOR_CONNECT` |
| `test_batch_failure_continues_to_next` | First batch fails, second batch succeeds -> partial results |
| `test_error_details_populated` | `error_details` contains `IngestError` with correct code and message |

#### 4k. `TestProcessingResultAssembly` (3 tests)

| Test | What It Tests |
|---|---|
| `test_empty_document_returns_zero_chunks` | All pages blank -> `chunks_created=0`, no errors |
| `test_result_fields_match_inputs` | `file_path`, `ingest_key`, `ingest_run_id`, `tenant_id` passed through |
| `test_tables_always_empty_for_path_a` | `tables_created=0`, `tables=[]` always |

#### 4l. `TestLanguageDetection` (2 tests)

| Test | What It Tests |
|---|---|
| `test_language_detection_enabled` | With `enable_language_detection=True`, `detect_language()` called |
| `test_language_detection_disabled_uses_default` | With `enable_language_detection=False`, default language used |

**Total test count:** ~37 tests across 12 test classes.

---

## Acceptance Criteria

- [ ] `TextExtractor` class with `__init__(vector_store, embedder, config)` constructor
- [ ] `process()` method returns `ProcessingResult` with all required fields
- [ ] Step 1: `pymupdf4llm.to_markdown()` called with `page_chunks=True, header=False, footer=False`
- [ ] Step 2: Quality assessment triggers block-level fallback when grade is LOW
- [ ] Step 3: `HeaderFooterDetector.detect()` and `.strip()` applied per page
- [ ] Step 4: TOC pages detected (>30% dot-leader lines) and skipped with `W_PAGE_SKIPPED_TOC`
- [ ] Step 5: Blank pages skipped with `W_PAGE_SKIPPED_BLANK`
- [ ] Step 6: `HeadingDetector.detect()` called; headings converted from page-number to char-offset format
- [ ] Step 7: Document metadata (title, author, date) propagated to `PDFChunkMetadata`
- [ ] Step 8: `PDFChunker.chunk()` called with converted headings and page boundaries
- [ ] Step 9: Batch embedding via `EmbeddingBackend.embed()` with `config.embedding_batch_size`
- [ ] Step 10: `VectorStoreBackend.upsert_chunks()` called with `PDFChunkMetadata`
- [ ] Language detection integrated when `enable_language_detection=True`
- [ ] `tenant_id` propagated from config to chunk metadata
- [ ] PII-safe logging (no raw text unless `log_sample_text=True`)
- [ ] Error handling follows Excel pattern (`_classify_backend_error`)
- [ ] `WrittenArtifacts` tracks all upserted point IDs and collection
- [ ] Empty document returns `ProcessingResult` with zero chunks (no crash)
- [ ] Logger name is `"ingestkit_pdf.processors.text_extractor"`
- [ ] Exports added to `processors/__init__.py` and `__init__.py`
- [ ] All unit tests pass with `pytest -m unit` (all mocked)
- [ ] No regressions in existing tests

## Verification Gates (PROVE)

```bash
# All unit tests pass
pytest packages/ingestkit-pdf/tests/test_text_extractor.py -v -m unit

# No regressions
pytest packages/ingestkit-pdf/tests/ -v

# Import works
python -c "from ingestkit_pdf.processors.text_extractor import TextExtractor; print('OK')"

# Public API export works
python -c "from ingestkit_pdf import TextExtractor; print('OK')"
```

AGENT_RETURN: .agents/outputs/plan-38-021426.md
