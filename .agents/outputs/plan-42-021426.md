---
issue: 42
agent: PLAN
date: 2026-02-14
complexity: COMPLEX
stack: backend
spec_sections: "5.2, 17.1, 17.2, 17.3, 18.1, 20, 21.1"
estimated_lines: ~900
files_to_create: 2
files_to_modify: 1
---

# PLAN: Issue #42 -- PDFRouter orchestrator and public API

## Executive Summary

Implement `PDFRouter` in `router.py` as the top-level orchestrator for the ingestkit-pdf pipeline. The router executes a 15-step `process()` flow (security scan, ingest key, document profiling, tiered classification with LLM outage resilience, processor routing, result assembly, PII-safe logging). It also provides `process_batch()` with `ProcessPoolExecutor` isolation, a `create_default_router()` factory, and updates `__init__.py` with all SPEC section 21.1 exports. The MAP identified one critical gap: `ComplexProcessor` does not exist yet and must be handled gracefully.

---

## 1. Files to Create / Modify

| # | File | Action | Purpose |
|---|------|--------|---------|
| 1 | `packages/ingestkit-pdf/src/ingestkit_pdf/router.py` | CREATE | PDFRouter class + create_default_router factory |
| 2 | `packages/ingestkit-pdf/tests/test_router.py` | CREATE | Unit tests for router |
| 3 | `packages/ingestkit-pdf/src/ingestkit_pdf/__init__.py` | MODIFY | Add SPEC 21.1 public API exports |

---

## 2. PDFRouter Class -- Full Method Signatures

```python
class PDFRouter:
    """Top-level orchestrator for the ingestkit-pdf pipeline."""

    def __init__(
        self,
        vector_store: VectorStoreBackend,
        structured_db: StructuredDBBackend,
        llm: LLMBackend,
        embedder: EmbeddingBackend,
        config: PDFProcessorConfig | None = None,
    ) -> None: ...

    # -- Public API --

    def can_handle(self, file_path: str) -> bool:
        """True if file_path ends with .pdf (case-insensitive)."""

    def process(
        self,
        file_path: str,
        source_uri: str | None = None,
    ) -> ProcessingResult:
        """Classify and process a single PDF. Synchronous."""

    def process_batch(
        self,
        file_paths: list[str],
    ) -> list[ProcessingResult]:
        """Process multiple PDFs with process isolation and per-document timeout."""

    # -- Private methods --

    def _build_document_profile(
        self,
        file_path: str,
        doc: fitz.Document,
        metadata: DocumentMetadata,
        security_warnings: list[str],
    ) -> DocumentProfile: ...

    def _build_page_profile(
        self,
        page: fitz.Page,
        page_number: int,
    ) -> PageProfile: ...

    def _classify(
        self,
        profile: DocumentProfile,
    ) -> tuple[ClassificationResult, list[str], list[str], list[IngestError]]:
        """Tiered classification with LLM outage resilience.
        Returns (classification, errors, warnings, error_details)."""

    def _build_parse_stage_result(
        self,
        profile: DocumentProfile,
        parse_duration: float,
    ) -> ParseStageResult: ...

    def _build_error_result(
        self,
        file_path: str,
        ingest_key: str,
        ingest_run_id: str,
        errors: list[str],
        warnings: list[str],
        error_details: list[IngestError],
        elapsed: float,
    ) -> ProcessingResult: ...

    @staticmethod
    def _merge_errors(
        result: ProcessingResult,
        errors: list[str],
        warnings: list[str],
        error_details: list[IngestError],
    ) -> None: ...

    @staticmethod
    def _pdf_type_to_ingestion_method(pdf_type: PDFType) -> IngestionMethod: ...

    @staticmethod
    def _pdf_type_to_path_name(pdf_type: PDFType) -> str: ...
```

---

## 3. `__init__()` Implementation

```python
def __init__(self, vector_store, structured_db, llm, embedder, config=None):
    self._config = config or PDFProcessorConfig()

    # Security
    self._security_scanner = PDFSecurityScanner(self._config)

    # Classification
    self._inspector = PDFInspector(self._config)
    self._llm_classifier = PDFLLMClassifier(llm, self._config)

    # Quality
    self._quality_assessor = QualityAssessor(self._config)

    # Layout analysis
    self._layout_analyzer = LayoutAnalyzer(self._config)

    # Processors
    self._text_extractor = TextExtractor(
        vector_store=vector_store,
        embedder=embedder,
        config=self._config,
    )
    self._ocr_processor = OCRProcessor(
        vector_store=vector_store,
        embedder=embedder,
        llm=llm,
        config=self._config,
    )
    # ComplexProcessor not yet implemented -- accept None
    self._complex_processor = None  # future: ComplexProcessor(...)

    # Store backends for process_batch worker recreation
    self._vector_store = vector_store
    self._structured_db = structured_db
    self._llm = llm
    self._embedder = embedder
```

**Verification note**: Must verify OCRProcessor `__init__` signature. Check if it takes `llm` parameter.

---

## 4. `process()` -- 15-Step Flow

### Step 1: Security Scan
```python
metadata, security_errors = self._security_scanner.scan(file_path)
fatal_errors = [e for e in security_errors if e.code.value.startswith("E_")]
if fatal_errors:
    return self._build_error_result(
        file_path, ingest_key="", ingest_run_id=ingest_run_id,
        errors=[e.code.value for e in fatal_errors],
        warnings=[e.code.value for e in security_errors if not e.code.value.startswith("E_")],
        error_details=security_errors, elapsed=time.monotonic() - start,
    )
```
**Note**: Security scan runs BEFORE ingest_key computation because the file may be invalid. Compute ingest_key only after security passes.

### Step 2: Compute Ingest Key
```python
from ingestkit_core.idempotency import compute_ingest_key
ingest_key_obj = compute_ingest_key(
    file_path=file_path,
    parser_version=config.parser_version,
    tenant_id=config.tenant_id,
    source_uri=source_uri,
)
ingest_key = ingest_key_obj.key
```

### Step 3: Generate Ingest Run ID
```python
ingest_run_id = str(uuid.uuid4())
```

### Step 4: Open Document with PyMuPDF
```python
try:
    doc = fitz.open(file_path)
except Exception:
    # Attempt repair
    try:
        doc = fitz.open(file_path, filetype="pdf")
        # If that also fails, try with repair
        # fitz.open returns a doc; if corrupt, we can try Document.tobytes(garbage=3)
    except Exception as exc:
        logger.error(...)
        return self._build_error_result(... E_PARSE_CORRUPT ...)
```
**Implementation detail**: PyMuPDF's `fitz.open()` with `filetype="pdf"` forces PDF interpretation. For repair, use `doc.save(doc.tobytes(garbage=3))` approach. If all fail, return `E_PARSE_CORRUPT`.

### Step 5: Extract Document Profile
```python
profile = self._build_document_profile(file_path, doc, metadata, security_warnings)
```

### Step 6: Detect Language (if enabled)
Language detection happens inside `_build_document_profile` as part of profiling. Sample text from first N pages with extractable text, call `detect_language()`.

### Step 7-9: Tiered Classification
```python
classification, classify_errors, classify_warnings, classify_error_details = self._classify(profile)
```

### Step 10: Fail-Closed Check
```python
if classification.confidence == 0.0:
    # E_CLASSIFY_INCONCLUSIVE -- zero chunks
    return self._build_error_result(...)
```

### Step 11: Route to Processor
```python
if classification.pdf_type == PDFType.TEXT_NATIVE:
    result = self._text_extractor.process(
        file_path=file_path, profile=profile,
        ingest_key=ingest_key, ingest_run_id=ingest_run_id,
        parse_result=parse_result,
        classification_result=classification_stage_result,
        classification=classification,
    )
elif classification.pdf_type == PDFType.SCANNED:
    result = self._ocr_processor.process(
        file_path=file_path, profile=profile,
        pages=None,  # full-document OCR
        ingest_key=ingest_key, ingest_run_id=ingest_run_id,
        parse_result=parse_result,
        classification_result=classification_stage_result,
        classification=classification,
    )
elif classification.pdf_type == PDFType.COMPLEX:
    if self._complex_processor is None:
        # ComplexProcessor not available -- return error result
        return self._build_error_result(
            ..., errors=["E_PROCESS_NOT_AVAILABLE"],
            ...
        )
```
**Critical API difference**: OCRProcessor.process() takes an extra `pages: list[int] | None` parameter. Pass `None` for full-document OCR.

### Steps 12-13: Collect Artifacts + Assemble Result
Merge classification warnings/errors into the processor's result. Override `processing_time_seconds` with full pipeline time.

### Step 14: PII-Safe Logging
```python
logger.info(
    "ingestkit_pdf | file=%s | ingest_key=%s | tier=%s | type=%s | "
    "confidence=%.2f | degraded=%s | path=%s | pages=%d | chunks=%d | "
    "tables=%d | ocr_pages=%d | time=%.1fs",
    filename, ingest_key[:8], tier_used.value, pdf_type.value,
    confidence, degraded, path_name, page_count, chunks, tables, ocr_pages, elapsed,
)
```

### Step 15: Return Result

---

## 5. `_classify()` -- Tiered Classification with LLM Outage Resilience

Implementation follows SPEC section 5.2 exactly:

```python
def _classify(self, profile):
    errors, warnings, error_details = [], [], []

    # Step 7: Tier 1 -- ALWAYS runs, zero external dependencies
    tier1_result = self._inspector.classify(profile)

    # Check if Tier 1 is high-confidence (SPEC: tier1_high_confidence_signals/5)
    # Default: 4/5 = 0.8
    high_conf = self._config.tier1_high_confidence_signals / 5
    if tier1_result.confidence >= high_conf:
        return tier1_result, errors, warnings, error_details

    # If Tier 1 confidence == 0.0 (inconclusive) OR below high threshold,
    # attempt Tier 2
    try:
        tier2_result = self._llm_classifier.classify(
            profile, ClassificationTier.LLM_BASIC
        )
        if tier2_result.confidence >= self._config.tier2_confidence_threshold:
            return tier2_result, errors, warnings, error_details

        # Step 9: Tier 3 escalation
        if self._config.enable_tier3:
            try:
                tier3_result = self._llm_classifier.classify(
                    profile, ClassificationTier.LLM_REASONING
                )
                return tier3_result, errors, warnings, error_details
            except (ConnectionError, TimeoutError) as exc:
                # Tier 3 failed -- fall through to degrade check
                pass

        # Tier 2 returned but low confidence, Tier 3 disabled or failed
        # If Tier 2 confidence > 0 use it, otherwise fall back to Tier 1
        if tier2_result.confidence > 0.0:
            return tier2_result, errors, warnings, error_details
        # Fall through to Tier 1 degraded

    except (ConnectionError, TimeoutError) as exc:
        # LLM outage -- degrade to Tier 1
        warnings.extend([
            ErrorCode.W_LLM_UNAVAILABLE.value,
            ErrorCode.W_CLASSIFICATION_DEGRADED.value,
        ])
        error_details.append(IngestError(
            code=ErrorCode.W_LLM_UNAVAILABLE,
            message=f"LLM backend unreachable ({type(exc).__name__}), degraded to Tier 1",
            stage="classify",
            recoverable=True,
        ))
        degraded = tier1_result.model_copy(update={"degraded": True})
        return degraded, errors, warnings, error_details

    # If we get here: Tier 1 was low-confidence AND LLM tiers didn't help
    # Use Tier 1 result as-is (may have confidence 0.0 -> E_CLASSIFY_INCONCLUSIVE upstream)
    return tier1_result, errors, warnings, error_details
```

**Key invariant** (SPEC 5.2): `E_CLASSIFY_INCONCLUSIVE` only when Tier 1 confidence == 0.0 AND all LLM tiers also fail. LLM outage alone NEVER causes zero-chunk results -- Tier 1 always provides a usable result with `degraded=True`.

---

## 6. `_build_document_profile()` -- Document Profiling

This is the router's responsibility (no dedicated profiler module exists). Key operations:

```python
def _build_document_profile(self, file_path, doc, metadata, security_warnings):
    content_hash = hashlib.sha256(Path(file_path).read_bytes()).hexdigest()

    pages = []
    page_qualities = []
    for page_num in range(doc.page_count):
        page = doc[page_num]
        page_profile = self._build_page_profile(page, page_num + 1)
        pages.append(page_profile)
        page_qualities.append(page_profile.extraction_quality)

    # Page type distribution
    distribution = {}
    for p in pages:
        distribution[p.page_type.value] = distribution.get(p.page_type.value, 0) + 1

    # Language detection
    detected_languages = []
    if self._config.enable_language_detection:
        sample_text = " ".join(
            page.get_text() for page in [doc[i] for i in range(min(5, doc.page_count))]
            if len(page.get_text().strip()) > 50
        )
        if sample_text:
            lang, conf = detect_language(sample_text, default_language=self._config.default_language)
            detected_languages = [lang]

    # TOC
    toc = doc.get_toc()  # [(level, title, page_number), ...]
    has_toc = len(toc) > 0
    toc_entries = [(lvl, title, page) for lvl, title, page in toc] if toc else None

    # Overall quality
    overall_quality = self._quality_assessor.assess_document(page_qualities)

    return DocumentProfile(
        file_path=file_path, file_size_bytes=metadata.file_size_bytes,
        page_count=doc.page_count, content_hash=content_hash,
        metadata=metadata, pages=pages,
        page_type_distribution=distribution,
        detected_languages=detected_languages,
        has_toc=has_toc, toc_entries=toc_entries,
        overall_quality=overall_quality,
        security_warnings=[e for e in security_warnings],
    )
```

### `_build_page_profile()` per-page logic:

```python
def _build_page_profile(self, page, page_number):
    text = page.get_text()
    words = text.split()

    # Images
    images = page.get_images(full=True)
    image_count = len(images)
    # Image coverage: sum of image block areas / page area
    page_rect = page.rect
    page_area = page_rect.width * page_rect.height
    image_area = 0.0
    for img_block in page.get_image_info():
        bbox = img_block.get("bbox", (0, 0, 0, 0))
        w = bbox[2] - bbox[0]
        h = bbox[3] - bbox[1]
        image_area += w * h
    image_coverage = image_area / max(page_area, 1.0)

    # Fonts
    fonts = page.get_fonts()
    font_names = list({f[3] for f in fonts if f[3]})
    font_count = len(font_names)

    # Tables (heuristic: count table-like structures)
    # Use simple line-counting heuristic or check for table patterns
    tables = page.find_tables() if hasattr(page, "find_tables") else []
    table_count = len(tables) if tables else 0

    # Form fields
    widgets = page.widgets()
    has_form_fields = widgets is not None and len(list(widgets)) > 0

    # Multi-column detection
    layout_result = self._layout_analyzer.detect_columns(page)
    is_multi_column = layout_result.column_count > 1

    # Quality assessment
    quality = self._quality_assessor.assess_page(text, page_number)

    # Preliminary page type classification
    page_type = self._determine_page_type(
        text_length=len(text), word_count=len(words),
        image_coverage=image_coverage, font_count=font_count,
        table_count=table_count, has_form_fields=has_form_fields,
        is_multi_column=is_multi_column,
    )

    return PageProfile(
        page_number=page_number, text_length=len(text),
        word_count=len(words), image_count=image_count,
        image_coverage_ratio=min(image_coverage, 1.0),
        table_count=table_count, font_count=font_count,
        font_names=font_names, has_form_fields=has_form_fields,
        is_multi_column=is_multi_column, page_type=page_type,
        extraction_quality=quality,
    )
```

### `_determine_page_type()` -- preliminary heuristic:

Blank: text_length < 10 and image_count == 0
Scanned: text_length < 50 and image_coverage > 0.7
Table_heavy: table_count >= 1
Form: has_form_fields
Mixed: is_multi_column or (has tables and has text)
Text: default for pages with sufficient text

---

## 7. `process_batch()` -- ProcessPoolExecutor

```python
def process_batch(self, file_paths: list[str]) -> list[ProcessingResult]:
    """Process multiple files with process isolation per SPEC 17.2."""
    if not file_paths:
        return []

    results: dict[int, ProcessingResult] = {}
    timeout = self._config.per_document_timeout_seconds

    with ProcessPoolExecutor(max_workers=min(len(file_paths), 4)) as executor:
        future_to_idx = {
            executor.submit(
                _process_single_file,
                fp,
                self._config.model_dump(),  # serializable config
            ): idx
            for idx, fp in enumerate(file_paths)
        }
        for future in as_completed(future_to_idx):
            idx = future_to_idx[future]
            fp = file_paths[idx]
            try:
                result = future.result(timeout=timeout)
                results[idx] = result
            except TimeoutError:
                results[idx] = self._build_error_result(
                    fp, ingest_key="", ingest_run_id=str(uuid.uuid4()),
                    errors=["E_PROCESS_TIMEOUT"],
                    warnings=[], error_details=[], elapsed=float(timeout),
                )
            except Exception as exc:
                results[idx] = self._build_error_result(
                    fp, ingest_key="", ingest_run_id=str(uuid.uuid4()),
                    errors=[f"E_PROCESS_UNKNOWN: {exc}"],
                    warnings=[], error_details=[], elapsed=0.0,
                )

    return [results[i] for i in range(len(file_paths))]
```

### Module-level worker function (must be picklable):

```python
def _process_single_file(file_path: str, config_dict: dict) -> ProcessingResult:
    """Worker function for process_batch -- runs in a child process.

    Creates fresh backends and router per-worker to avoid cross-process
    sharing issues (SPEC 18.1).
    """
    config = PDFProcessorConfig(**config_dict)
    router = create_default_router(config=config)
    return router.process(file_path)
```

**Note**: This requires `create_default_router` to be available. If backends are not installed, `process_batch` will fail with `ImportError` in the worker. This is acceptable -- `process_batch` requires default backends. For custom backends, callers should iterate and call `process()` directly.

---

## 8. `create_default_router()` Factory

```python
def create_default_router(**overrides) -> PDFRouter:
    """Create a PDFRouter with default backends (Qdrant, SQLite, Ollama).

    Mirrors the ExcelRouter factory pattern. All defaults can be overridden.
    """
    # Lazy imports -- avoid hard dependency on backend packages
    from ingestkit_excel.backends import (
        OllamaEmbedding,
        OllamaLLM,
        QdrantVectorStore,
        SQLiteStructuredDB,
    )

    router_keys = {"vector_store", "structured_db", "llm", "embedder", "config"}
    router_kwargs = {k: v for k, v in overrides.items() if k in router_keys}
    config_kwargs = {k: v for k, v in overrides.items() if k not in router_keys}

    config = router_kwargs.pop("config", None)
    if config is None and config_kwargs:
        config = PDFProcessorConfig(**config_kwargs)
    elif config is None:
        config = PDFProcessorConfig()

    vector_store = router_kwargs.pop("vector_store", None) or QdrantVectorStore()
    structured_db = router_kwargs.pop("structured_db", None) or SQLiteStructuredDB()
    llm = router_kwargs.pop("llm", None) or OllamaLLM()
    embedder = router_kwargs.pop("embedder", None) or OllamaEmbedding(model=config.embedding_model)

    return PDFRouter(
        vector_store=vector_store,
        structured_db=structured_db,
        llm=llm,
        embedder=embedder,
        config=config,
    )
```

**Decision**: Import backends from `ingestkit_excel.backends` since no PDF-specific backends exist. This is the pragmatic choice -- the backends are protocol-conformant and backend-agnostic. If/when backends are extracted to `ingestkit-core`, this import changes.

---

## 9. `__init__.py` Public API Exports

Replace the current `__init__.py` with SPEC 21.1 exports:

```python
"""ingestkit-pdf -- Tiered PDF file processing for RAG pipelines."""

from ingestkit_pdf.config import PDFProcessorConfig
from ingestkit_pdf.errors import ErrorCode, IngestError
from ingestkit_pdf.inspector import PDFInspector
from ingestkit_pdf.llm_classifier import LLMClassificationResponse, PDFLLMClassifier
from ingestkit_pdf.models import (
    ClassificationResult,
    ClassificationStageResult,
    ClassificationTier,
    ChunkPayload,
    DocumentMetadata,
    DocumentProfile,
    EmbedStageResult,
    ExtractionQuality,
    IngestionMethod,
    OCRResult,
    OCRStageResult,
    PageProfile,
    PageType,
    PDFChunkMetadata,
    PDFType,
    ParseStageResult,
    ProcessingResult,
    TableResult,
    WrittenArtifacts,
)
from ingestkit_pdf.processors.ocr_processor import OCRProcessor
from ingestkit_pdf.processors.text_extractor import TextExtractor
from ingestkit_pdf.router import PDFRouter, create_default_router

__all__ = [
    # Router
    "PDFRouter",
    "create_default_router",
    # Config
    "PDFProcessorConfig",
    # Models -- enums
    "PDFType",
    "PageType",
    "ClassificationTier",
    "IngestionMethod",
    # Models -- data
    "ClassificationResult",
    "ProcessingResult",
    "ChunkPayload",
    "PDFChunkMetadata",
    "DocumentProfile",
    "DocumentMetadata",
    "PageProfile",
    "ExtractionQuality",
    "OCRResult",
    "TableResult",
    "WrittenArtifacts",
    # Stage results
    "ParseStageResult",
    "ClassificationStageResult",
    "OCRStageResult",
    "EmbedStageResult",
    # Errors
    "ErrorCode",
    "IngestError",
    # Classifiers
    "PDFInspector",
    "PDFLLMClassifier",
    "LLMClassificationResponse",
    # Processors
    "TextExtractor",
    "OCRProcessor",
]
```

---

## 10. Test Plan -- `test_router.py`

### 10.1 Test Fixtures (add to conftest.py or inline)

```python
@pytest.fixture()
def pdf_router(mock_vector_store, mock_structured_db, mock_llm, mock_embedder, pdf_config):
    """PDFRouter with all mock backends."""
    return PDFRouter(
        vector_store=mock_vector_store,
        structured_db=mock_structured_db,
        llm=mock_llm,
        embedder=mock_embedder,
        config=pdf_config,
    )
```

### 10.2 Test Classes and Methods

All tests marked `@pytest.mark.unit`.

```python
class TestCanHandle:
    def test_pdf_extension_lowercase(self, pdf_router): ...
    def test_pdf_extension_uppercase(self, pdf_router): ...
    def test_pdf_extension_mixed_case(self, pdf_router): ...
    def test_non_pdf_xlsx(self, pdf_router): ...
    def test_non_pdf_docx(self, pdf_router): ...
    def test_non_pdf_no_extension(self, pdf_router): ...

class TestClassify:
    """Tests for the _classify() private method via process()."""

    def test_tier1_high_confidence_skips_llm(self): ...
    def test_tier1_inconclusive_escalates_to_tier2(self): ...
    def test_tier2_low_confidence_escalates_to_tier3(self): ...
    def test_tier3_disabled_uses_tier2_result(self): ...

class TestLLMOutageResilience:
    """SPEC 5.2 test contract."""

    def test_llm_connection_error_degrades_to_tier1(self):
        """LLM ConnectionError -> degraded=True, W_LLM_UNAVAILABLE, chunks > 0."""
        ...

    def test_llm_timeout_degrades_to_tier1(self):
        """LLM TimeoutError -> same degraded behavior."""
        ...

    def test_degraded_result_has_correct_warnings(self):
        """W_LLM_UNAVAILABLE and W_CLASSIFICATION_DEGRADED in warnings."""
        ...

    def test_degraded_result_still_processes(self):
        """chunks_created > 0 even when degraded."""
        ...

    def test_inconclusive_only_when_all_tiers_fail(self):
        """E_CLASSIFY_INCONCLUSIVE only when Tier 1 conf==0 AND LLM fails."""
        ...

class TestSecurityScan:
    def test_fatal_security_error_returns_immediately(self): ...
    def test_security_warning_continues_processing(self): ...

class TestDocumentProfiling:
    def test_build_page_profile_text_page(self): ...
    def test_build_page_profile_scanned_page(self): ...
    def test_build_page_profile_blank_page(self): ...
    def test_build_document_profile_aggregation(self): ...
    def test_language_detection_called_when_enabled(self): ...
    def test_toc_extraction(self): ...

class TestProcessFlow:
    def test_text_native_routes_to_text_extractor(self): ...
    def test_scanned_routes_to_ocr_processor(self): ...
    def test_complex_no_processor_returns_error(self): ...
    def test_corrupt_pdf_repair_attempt(self): ...
    def test_fail_closed_zero_chunks(self): ...
    def test_processing_time_includes_full_pipeline(self): ...

class TestIngestKey:
    def test_deterministic_same_file(self): ...
    def test_tenant_id_affects_key(self): ...
    def test_source_uri_override(self): ...

class TestProcessBatch:
    def test_returns_results_in_order(self): ...
    def test_timeout_produces_error_result(self): ...
    def test_empty_batch_returns_empty(self): ...

class TestCreateDefaultRouter:
    def test_creates_valid_router(self): ...
    def test_config_overrides_applied(self): ...

class TestLogging:
    def test_info_log_no_raw_text(self, caplog): ...
    def test_warning_log_for_llm_outage(self, caplog): ...
    def test_error_log_for_corrupt_file(self, caplog): ...
```

### 10.3 Mock Strategy

- **Security scanner**: `unittest.mock.patch("ingestkit_pdf.router.PDFSecurityScanner.scan")` to return controlled `(metadata, errors)` tuples.
- **fitz.open**: `unittest.mock.patch("ingestkit_pdf.router.fitz.open")` to return mock documents with controlled page data.
- **Processors**: `unittest.mock.patch.object(router, "_text_extractor")` / `"_ocr_processor"` to return controlled `ProcessingResult`.
- **Inspector / LLM classifier**: Let real objects run with mock backends where possible; patch only for specific outage tests.
- **Ingest key**: `unittest.mock.patch("ingestkit_pdf.router.compute_ingest_key")` to return deterministic values.

### 10.4 Test Fixtures for Mock PyMuPDF

```python
class MockPage:
    """Minimal mock for fitz.Page."""
    def __init__(self, text="Sample text...", images=None, fonts=None, ...): ...
    def get_text(self): return self._text
    def get_images(self, full=True): return self._images
    def get_fonts(self): return self._fonts
    def get_image_info(self): return self._image_info
    @property
    def rect(self): return MockRect(0, 0, 612, 792)
    @property
    def widgets(self): return iter([])

class MockDocument:
    """Minimal mock for fitz.Document."""
    def __init__(self, pages=None): ...
    def __getitem__(self, idx): return self._pages[idx]
    @property
    def page_count(self): return len(self._pages)
    def get_toc(self): return []
    def close(self): pass
```

---

## 11. Imports for `router.py`

```python
from __future__ import annotations

import hashlib
import logging
import os
import time
import uuid
from concurrent.futures import ProcessPoolExecutor, as_completed
from pathlib import Path

import fitz  # PyMuPDF

from ingestkit_core.idempotency import compute_ingest_key
from ingestkit_core.models import ClassificationTier, EmbedStageResult, WrittenArtifacts

from ingestkit_pdf.config import PDFProcessorConfig
from ingestkit_pdf.errors import ErrorCode, IngestError
from ingestkit_pdf.inspector import PDFInspector
from ingestkit_pdf.llm_classifier import PDFLLMClassifier
from ingestkit_pdf.models import (
    ClassificationResult,
    ClassificationStageResult,
    DocumentMetadata,
    DocumentProfile,
    ExtractionQuality,
    IngestionMethod,
    OCRStageResult,
    PageProfile,
    PageType,
    ParseStageResult,
    PDFType,
    ProcessingResult,
)
from ingestkit_pdf.processors.ocr_processor import OCRProcessor
from ingestkit_pdf.processors.text_extractor import TextExtractor
from ingestkit_pdf.protocols import (
    EmbeddingBackend,
    LLMBackend,
    StructuredDBBackend,
    VectorStoreBackend,
)
from ingestkit_pdf.quality import QualityAssessor
from ingestkit_pdf.security import PDFSecurityScanner
from ingestkit_pdf.utils.language import detect_language
from ingestkit_pdf.utils.layout_analysis import LayoutAnalyzer
```

---

## 12. Critical Decisions

### 12.1 ComplexProcessor Not Implemented
**Decision**: When `PDFType.COMPLEX` is classified and no ComplexProcessor exists, return a `ProcessingResult` with zero chunks and a warning `W_COMPLEX_NOT_AVAILABLE` (or use a generic error message). This avoids blocking the router on an unimplemented dependency. The router is forward-compatible -- when ComplexProcessor is added, it plugs in via `__init__`.

### 12.2 Document Profiling in Router
**Decision**: Implement as `_build_document_profile()` and `_build_page_profile()` private methods on PDFRouter. Can be extracted to a `profiler.py` module later if it grows too large.

### 12.3 Backend Imports for Factory
**Decision**: `create_default_router()` imports from `ingestkit_excel.backends` since no PDF-specific backends exist. The backends are protocol-conformant and reusable. Raises `ImportError` with clear message if not available.

### 12.4 process_batch() Simplification Path
**Decision**: Implement full `ProcessPoolExecutor` per SPEC 17.2. If pickling issues arise during implementation, document them and fall back to sequential processing with a `TODO` for process isolation.

---

## 13. Acceptance Criteria

- [ ] `PDFRouter` class created in `router.py` with `can_handle()`, `process()`, `process_batch()`
- [ ] `process()` implements all 15 SPEC 17.1 steps in order
- [ ] Security scan runs first; fatal errors return immediately
- [ ] Ingest key computed via `ingestkit_core.idempotency.compute_ingest_key()`
- [ ] Document profiling builds `DocumentProfile` from PyMuPDF
- [ ] Tiered classification: Tier 1 always runs, escalates to Tier 2/3 when needed
- [ ] LLM outage resilience per SPEC 5.2: degrades to Tier 1 with `degraded=True`
- [ ] `E_CLASSIFY_INCONCLUSIVE` only when Tier 1 confidence==0 AND all LLM tiers fail
- [ ] TEXT_NATIVE routes to TextExtractor, SCANNED to OCRProcessor
- [ ] COMPLEX type handled gracefully (error when ComplexProcessor unavailable)
- [ ] OCRProcessor called with extra `pages=None` parameter
- [ ] `process_batch()` uses ProcessPoolExecutor with per-document timeout
- [ ] `create_default_router()` factory created
- [ ] `__init__.py` updated with all SPEC 21.1 exports
- [ ] PII-safe logging at INFO/WARNING/ERROR levels per SPEC 20
- [ ] `tenant_id` propagated through all stages
- [ ] All new code has unit tests
- [ ] All existing tests pass (no regressions)
- [ ] `test_llm_outage_degrades_to_tier1` matches SPEC 5.2 test contract exactly

---

AGENT_RETURN: .agents/outputs/plan-42-021426.md
