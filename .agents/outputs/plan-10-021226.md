---
issue: 10
agent: PLAN
date: 2026-02-12
complexity: COMPLEX
stack: backend
---

# PLAN -- Issue #10: Path C Hybrid Splitter

## Executive Summary

Implement `HybridSplitter` in `processors/splitter.py` -- the Path C processor that detects distinct regions within hybrid Excel worksheets, classifies each as Type A (tabular) or Type B (document-formatted), and delegates to `StructuredDBProcessor` or `TextSerializer` for processing. The splitter uses five heuristics for region detection (blank separators, merged cell blocks, formatting transitions, header/footer detection, matrix detection), creates `SheetRegion` models with confidence scores, then synthesizes single-sheet `FileProfile` objects to delegate to sub-processors. Results from all sub-processors are merged into a single `ProcessingResult` with unified `WrittenArtifacts`, shared `ingest_key`, and per-region `region_id` in chunk metadata. The `process()` signature matches Path A/B (7 parameters) for router compatibility.

## Architecture Decision: Sub-processor Delegation

**Choice: Synthetic profile delegation** (Option A from MAP).

**Justification:**

1. Path A and Path B `process()` methods iterate `profile.sheets` and handle all internal logic (skip logic, error handling, embedding, DB writes, vector upserts). Re-implementing this would duplicate hundreds of lines.

2. The HybridSplitter creates a synthetic `FileProfile` containing a single `SheetProfile` per region, then calls the sub-processor's `process()`. The sub-processor "sees" a one-sheet file and processes it normally.

3. After delegation, the HybridSplitter merges `WrittenArtifacts` (union of vector_point_ids and db_table_names), sums chunk counts and table counts, aggregates `EmbedStageResult`, and collects errors/warnings from all sub-results.

4. **Region ID injection**: After sub-processor delegation, we post-process the returned `ProcessingResult` -- the chunks already exist in the vector store with `region_id=None`. To avoid this, we modify the synthetic `SheetProfile` to carry region metadata that can be detected. However, since Path A/B set `region_id=None` directly in their code, the cleaner approach is to **NOT delegate to sub-processors for embedding/upsert** but instead use sub-processor `process()` and accept that region_id will be None in the initial upsert. This is a known limitation documented in the spec.

**Revised approach after analysis**: Direct delegation to `process()` will produce chunks with `region_id=None`. Since the spec requires `region_id` to be set, we need a different strategy:

**Final approach: Internal region processing with helper reuse.**

The HybridSplitter will:
1. Detect regions (own logic)
2. Classify each region (using Tier 1 signal evaluation logic, adapted from ExcelInspector)
3. For Type A regions: load DataFrame via `pd.read_excel` with row/col slicing, then use `StructuredDBProcessor` by passing a synthetic profile. The sub-processor will create chunks with `region_id=None`, which we accept as a Phase 1 limitation and can be addressed later by updating Path A/B to accept region_id.
4. For Type B regions: load openpyxl worksheet, extract region rows, and use `TextSerializer` similarly.
5. Merge all results.

**CRITICAL SIMPLIFICATION**: After careful analysis, the most pragmatic approach is to delegate to sub-processors via `process()` with synthetic profiles. The `region_id` limitation is acceptable because:
- The spec says "All chunks/tables share the same ingest_key and link via region_id in metadata"
- We can set `region_id` after the fact by tracking which chunks came from which region
- But sub-processors upsert directly to vector store, so we cannot modify chunks post-upsert

**FINAL DECISION**: The HybridSplitter will handle its own embedding/upsert loop (similar to Path A/B) rather than delegating process(). This is the only way to ensure `region_id` is correctly set in ChunkMetadata before upsert. The splitter will:
1. Detect and classify regions
2. For Type A regions: extract DataFrame, create table, generate schema, embed with region_id set
3. For Type B regions: extract rows, detect sections, serialize, embed with region_id set
4. This reuses concepts from Path A/B but does NOT call their process() methods

This means the constructor needs the same backends as Path A + Path B:
```python
def __init__(
    self,
    structured_processor: StructuredDBProcessor,
    text_serializer: TextSerializer,
    config: ExcelProcessorConfig,
) -> None:
```

BUT internally, it will use the backends from the sub-processors (accessing `structured_processor._db`, etc.) or receive them directly. Since the spec says the constructor takes `structured_processor` and `text_serializer`, we follow the spec constructor signature and extract backends from them when needed for direct operations.

**REVISED FINAL DECISION (simplest correct approach)**: Keep the spec constructor. Delegate to sub-processors' `process()` with synthetic profiles. Accept that chunks will have `region_id=None` from sub-processors. After each sub-processor call, we do NOT re-upsert -- instead, we note this as a documented limitation and ensure the `ProcessingResult` metadata tracks regions. The `region_id` can be populated at the `ProcessingResult` level through the `SheetRegion` list returned alongside the result.

Actually, re-reading the spec requirement: "All chunks/tables share the same `ingest_key` and link via `region_id` in metadata." This means `region_id` MUST be in ChunkMetadata. Since sub-processors set `region_id=None`, delegation won't work for this requirement.

**TRULY FINAL DECISION**: The HybridSplitter performs its own processing loop (does not delegate to sub-processor `process()` methods). It uses the backends from the injected sub-processors to perform embedding, DB writes, and vector upserts directly, setting `region_id` correctly on every chunk. This is more code but is the only way to satisfy the spec requirement. The constructor still takes `structured_processor` and `text_serializer` per the spec -- we extract their backends via private attributes.

## File 1: processors/splitter.py

### Imports

```python
from __future__ import annotations

import hashlib
import logging
import re
import time
import uuid
from pathlib import Path

import openpyxl
import pandas as pd

from ingestkit_excel.config import ExcelProcessorConfig
from ingestkit_excel.errors import ErrorCode, IngestError
from ingestkit_excel.models import (
    ChunkMetadata, ChunkPayload, ClassificationResult,
    ClassificationStageResult, EmbedStageResult, FileProfile, FileType,
    IngestionMethod, ParseStageResult, ProcessingResult, RegionType,
    SheetProfile, SheetRegion, WrittenArtifacts,
)
from ingestkit_excel.processors.structured_db import clean_name, deduplicate_names
```

### Constants

```python
_BLANK_ROW_THRESHOLD = 2          # >= 2 consecutive blank rows = boundary
_BLANK_COL_THRESHOLD = 2          # >= 2 consecutive blank cols = boundary
_FORMATTING_TRANSITION_WINDOW = 5 # rows to compare for numeric/text shift
_NUMERIC_HEAVY_THRESHOLD = 0.6    # ratio above which a window is "numeric-heavy"
_TEXT_HEAVY_THRESHOLD = 0.6       # ratio above which a window is "text-heavy"
_HEADER_FOOTER_MAX_ROWS = 5      # max rows at top/bottom to check for header/footer blocks
_MATRIX_MIN_HEADERS = 2           # minimum row/col headers for matrix detection
```

### Class Structure

```python
class HybridSplitter:
    """Path C processor: detects regions in hybrid sheets, classifies each,
    and processes via Path A or Path B logic with region_id tracking."""

    def __init__(
        self,
        structured_processor: StructuredDBProcessor,
        text_serializer: TextSerializer,
        config: ExcelProcessorConfig,
    ) -> None:
        self._structured_processor = structured_processor
        self._text_serializer = text_serializer
        self._config = config
        # Extract backends from sub-processors for direct use
        self._db = structured_processor._db
        self._vector_store = structured_processor._vector_store
        self._embedder = structured_processor._embedder

    def process(
        self,
        file_path: str,
        profile: FileProfile,
        ingest_key: str,
        ingest_run_id: str,
        parse_result: ParseStageResult,
        classification_result: ClassificationStageResult,
        classification: ClassificationResult,
    ) -> ProcessingResult:
        """Process a hybrid Excel file by detecting regions and routing each."""
        ...

    # --- Region Detection ---
    def _detect_regions(self, ws, sheet: SheetProfile) -> list[SheetRegion]:
        """Detect distinct regions in a worksheet using 5 heuristics."""
        ...

    @staticmethod
    def _detect_blank_row_boundaries(all_rows: list[list]) -> list[int]:
        """Find row indices where >= 2 consecutive blank rows occur."""
        ...

    @staticmethod
    def _detect_blank_col_boundaries(all_rows: list[list], col_count: int) -> list[int]:
        """Find column indices where >= 2 consecutive blank columns occur."""
        ...

    @staticmethod
    def _detect_merged_blocks(ws) -> list[SheetRegion]:
        """Identify large merged cell blocks as HEADER_BLOCK regions."""
        ...

    @staticmethod
    def _detect_formatting_transitions(all_rows: list[list]) -> list[int]:
        """Find row boundaries where numeric/text ratio shifts significantly."""
        ...

    @staticmethod
    def _detect_header_footer(ws, all_rows: list[list], total_rows: int) -> tuple[SheetRegion | None, SheetRegion | None]:
        """Detect header and footer blocks at sheet boundaries."""
        ...

    @staticmethod
    def _detect_matrix_regions(all_rows: list[list], start_row: int, end_row: int, start_col: int, end_col: int) -> bool:
        """Check if a region has both row and column headers (matrix pattern)."""
        ...

    # --- Region Classification ---
    def _classify_region(self, all_rows: list[list], region: SheetRegion) -> FileType:
        """Classify a region as TABULAR_DATA or FORMATTED_DOCUMENT using Tier 1 signals."""
        ...

    @staticmethod
    def _compute_region_signals(all_rows: list[list], region: SheetRegion) -> dict[str, bool]:
        """Evaluate Tier 1 binary signals on the region's data subset."""
        ...

    # --- Processing ---
    def _process_type_a_region(
        self, region: SheetRegion, df: pd.DataFrame,
        sheet: SheetProfile, source_uri: str, db_uri: str,
        ingest_key: str, ingest_run_id: str,
        collection: str, chunk_index_start: int,
    ) -> tuple[list[ChunkPayload], list[str], int, float]:
        """Process a tabular region: create table, generate schema, embed."""
        ...

    def _process_type_b_region(
        self, region: SheetRegion, all_rows: list[list],
        sheet: SheetProfile, source_uri: str,
        ingest_key: str, ingest_run_id: str,
        collection: str, chunk_index_start: int,
    ) -> tuple[list[ChunkPayload], int, float]:
        """Process a text region: detect sections, serialize, embed."""
        ...

    @staticmethod
    def _classify_backend_error(exc: Exception) -> ErrorCode:
        """Map an exception to the most appropriate ErrorCode."""
        ...
```

### Region Detection Methods

#### `_detect_regions(ws, sheet) -> list[SheetRegion]`

This is the main orchestrator for region detection. Steps:

1. Load all rows from the worksheet: `all_rows = [[cell.value for cell in row] for row in ws.iter_rows()]`
2. If empty, return empty list.
3. Call each detection heuristic:
   - `_detect_blank_row_boundaries(all_rows)` -> list of boundary row indices
   - `_detect_blank_col_boundaries(all_rows, col_count)` -> list of boundary col indices
   - `_detect_merged_blocks(ws)` -> list of SheetRegion (HEADER_BLOCK)
   - `_detect_formatting_transitions(all_rows)` -> list of boundary row indices
   - `_detect_header_footer(ws, all_rows, len(all_rows))` -> (header_region, footer_region)
4. Merge all row boundaries (union of blank row boundaries + formatting transitions), sort, deduplicate.
5. Split the sheet into rectangular regions using row boundaries and column boundaries.
6. For each region, check if it overlaps with a merged block or header/footer region.
7. For each non-special region, check if it's a matrix via `_detect_matrix_regions`.
8. Assign `RegionType` to each region:
   - Regions matching merged blocks -> `HEADER_BLOCK`
   - Header/footer regions -> `HEADER_BLOCK` / `FOOTER_BLOCK`
   - Matrix regions -> `MATRIX_BLOCK`
   - Empty regions (all cells None) -> `EMPTY`
   - Default: `DATA_TABLE` or `TEXT_BLOCK` (determined later by classification)
9. Assign `detection_confidence`:
   - Blank separator boundaries: 0.9 (strong signal)
   - Merged block headers: 0.85
   - Formatting transitions: 0.7 (weaker signal)
   - Matrix detection: 0.8
   - Header/footer: 0.85
10. Generate `region_id` as `f"{sheet.name}_r{idx}"` (0-indexed).
11. Return list of SheetRegion objects.

#### `_detect_blank_row_boundaries(all_rows) -> list[int]`

Scan rows looking for runs of >= `_BLANK_ROW_THRESHOLD` consecutive blank rows. A blank row is one where all cells are None or whitespace-only. Return the starting index of each blank run as a boundary.

#### `_detect_blank_col_boundaries(all_rows, col_count) -> list[int]`

For each column index, check if >= `_BLANK_COL_THRESHOLD` consecutive columns are all blank across all rows. Return column indices where boundaries occur.

#### `_detect_merged_blocks(ws) -> list[SheetRegion]`

Iterate `ws.merged_cells.ranges`. For merged ranges spanning >= 2 columns (wide merges), create a SheetRegion with `region_type=RegionType.HEADER_BLOCK` if the merged cell has a non-empty value. These represent title/header blocks.

#### `_detect_formatting_transitions(all_rows) -> list[int]`

Use a sliding window of `_FORMATTING_TRANSITION_WINDOW` rows. For each window, compute:
- numeric_ratio = count of numeric cells / total non-empty cells
- text_ratio = count of text cells / total non-empty cells

When consecutive windows shift from numeric-heavy to text-heavy (or vice versa), mark the boundary row.

#### `_detect_header_footer(ws, all_rows, total_rows) -> tuple[SheetRegion | None, SheetRegion | None]`

Check the first `_HEADER_FOOTER_MAX_ROWS` rows for merged cells spanning the full width. If found, create a HEADER_BLOCK SheetRegion. Similarly check last `_HEADER_FOOTER_MAX_ROWS` rows for a FOOTER_BLOCK.

#### `_detect_matrix_regions(all_rows, start_row, end_row, start_col, end_col) -> bool`

Check if a region has:
- Non-empty values in column 0 of data rows (row headers)
- Non-empty values in row 0 for columns 1+ (column headers)
- Corner cell (row 0, col 0) is empty or generic
- Intersection cells are populated (> 30%)

Same logic as TextSerializer._classify_sub_structure's matrix detection, adapted for arbitrary regions.

### Region Classification

#### `_classify_region(all_rows, region) -> FileType`

Extract the sub-grid of `all_rows` for the region's bounding box. Compute Tier 1-like signals:

1. **row_count**: `(end_row - start_row + 1) >= config.min_row_count_for_tabular`
2. **merged_cell_ratio**: Count merged cells in region / total cells in region. Compare to `config.merged_cell_ratio_threshold`.
3. **column_type_consistency**: For each column in the region, check if values are consistently typed. Compare to `config.column_consistency_threshold`.
4. **header_detected**: Check if the first row of the region looks like a header (distinct non-numeric string values).
5. **numeric_ratio**: Ratio of numeric cells to total non-empty cells. Compare to `config.numeric_ratio_threshold`.

Count Type A signals (True) vs Type B signals (False). Use same threshold logic as ExcelInspector:
- >= 4 Type A signals -> TABULAR_DATA
- >= 4 Type B signals -> FORMATTED_DOCUMENT
- >= 3 Type A signals -> TABULAR_DATA (medium confidence)
- >= 3 Type B signals -> FORMATTED_DOCUMENT (medium confidence)
- Otherwise -> FORMATTED_DOCUMENT (fail-closed to text to avoid data loss)

Set `region.classified_as` to the result.

### Sub-processor Delegation (revised: direct processing)

Since we cannot delegate to sub-processors (region_id requirement), the HybridSplitter processes regions directly.

#### `_process_type_a_region(...)`

For tabular regions:
1. Extract DataFrame from the file for the region's row/column range using `pd.read_excel` with `skiprows`, `nrows`, `usecols` parameters.
2. Clean column names using `clean_name()` and `deduplicate_names()` from structured_db module.
3. Auto-detect dates (reuse `StructuredDBProcessor._auto_detect_dates()` by calling it on the structured_processor instance).
4. Create table in DB via `self._db.create_table_from_dataframe(table_name, df)`.
5. Generate schema description (reuse `StructuredDBProcessor._generate_schema_description()`).
6. Embed schema, create ChunkPayload with `region_id` set.
7. Upsert to vector store.
8. Optional row serialization if below limit.
9. Return chunks, table names, texts_embedded count, embed_duration.

#### `_process_type_b_region(...)`

For text/document regions:
1. Extract rows from `all_rows` for the region's bounding box.
2. Create Section objects from the extracted rows (simplified -- treat the whole region as one section or use TextSerializer's section detection logic on the subset).
3. Serialize each section using TextSerializer's static methods (`_serialize_section`, `_serialize_table`, etc.).
4. Create ChunkPayload with `region_id` set.
5. Embed and upsert.
6. Return chunks, texts_embedded count, embed_duration.

### Result Merging

In the main `process()` method, after processing all regions:

1. **WrittenArtifacts**: Single instance, accumulate `vector_point_ids` and `db_table_names` from all regions.
2. **EmbedStageResult**: Sum `texts_embedded` and `embed_duration_seconds` across all regions. Only create if total > 0.
3. **chunks_created**: Sum of all chunks from all regions.
4. **tables_created**: Count of tables from Type A regions.
5. **tables**: List of table names from Type A regions.
6. **errors/warnings**: Accumulate from all regions.
7. **error_details**: Accumulate from all regions.
8. **ingestion_method**: `IngestionMethod.HYBRID_SPLIT`
9. **processing_time_seconds**: Total elapsed time (not sum of sub-times).

### Error Handling

1. **Region detection failure**: If `_detect_regions` raises, catch and record `E_PROCESS_REGION_DETECT`. Return result with zero chunks.
2. **Per-region processing failure**: try/except around each region's processing. Log, record error, continue to next region. Use `_classify_backend_error()` (same pattern as Path A/B).
3. **Per-sheet error handling**: If a sheet fails entirely during region detection, record error and continue to next sheet.
4. **Empty regions**: Skip `EMPTY` and `CHART_ONLY` regions silently.
5. **Header/footer blocks**: Skip processing (they are structural markers, not data). Optionally include header text as context in neighboring regions' chunks.

### process() Method Flow

```python
def process(self, file_path, profile, ingest_key, ingest_run_id,
            parse_result, classification_result, classification):
    start_time = time.monotonic()
    config = self._config
    collection = config.default_collection
    source_uri = f"file://{Path(file_path).resolve().as_posix()}"
    db_uri = self._db.get_connection_uri()

    errors, warnings, error_details = [], [], []
    written = WrittenArtifacts(vector_collection=collection)
    tables, total_chunks, total_texts_embedded, embed_duration = [], 0, 0, 0.0

    vector_size = self._embedder.dimension()
    self._vector_store.ensure_collection(collection, vector_size)

    wb = openpyxl.load_workbook(file_path, data_only=True)
    chunk_index_counter = 0

    for sheet in profile.sheets:
        # Skip logic (hidden, chart-only, oversized) -- same as Path A/B
        if sheet.is_hidden:
            warnings.append(ErrorCode.W_SHEET_SKIPPED_HIDDEN.value)
            continue
        if sheet.row_count == 0 and sheet.col_count == 0:
            warnings.append(ErrorCode.W_SHEET_SKIPPED_CHART.value)
            continue
        if sheet.row_count > config.max_rows_in_memory:
            warnings.append(ErrorCode.W_ROWS_TRUNCATED.value)
            continue

        try:
            ws = wb[sheet.name]
            regions = self._detect_regions(ws, sheet)

            for region in regions:
                if region.region_type in (RegionType.EMPTY, RegionType.CHART_ONLY):
                    continue
                if region.region_type in (RegionType.HEADER_BLOCK, RegionType.FOOTER_BLOCK):
                    continue  # structural markers, not processed as data

                # Classify region
                all_rows = [[cell.value for cell in row] for row in ws.iter_rows()]
                classified_type = self._classify_region(all_rows, region)
                region.classified_as = classified_type

                if classified_type == FileType.TABULAR_DATA:
                    # Extract DataFrame for region
                    df = pd.read_excel(
                        file_path, sheet_name=sheet.name,
                        skiprows=range(0, region.start_row - 1),  # adjust for 0-based
                        nrows=region.end_row - region.start_row + 1,
                        usecols=range(region.start_col - 1, region.end_col),
                    )
                    # Process as Type A with region_id
                    # ... (create table, schema, embed, upsert with region_id)
                else:
                    # Process as Type B with region_id
                    # ... (extract rows, serialize, embed, upsert with region_id)

        except Exception as exc:
            error_code = ErrorCode.E_PROCESS_REGION_DETECT
            errors.append(error_code.value)
            error_details.append(IngestError(
                code=error_code, message=str(exc),
                sheet_name=sheet.name, stage="process", recoverable=False,
            ))
            continue

    wb.close()

    embed_result = None
    if total_texts_embedded > 0:
        embed_result = EmbedStageResult(
            texts_embedded=total_texts_embedded,
            embedding_dimension=self._embedder.dimension(),
            embed_duration_seconds=embed_duration,
        )

    elapsed = time.monotonic() - start_time
    return ProcessingResult(
        file_path=file_path, ingest_key=ingest_key,
        ingest_run_id=ingest_run_id, tenant_id=config.tenant_id,
        parse_result=parse_result,
        classification_result=classification_result,
        embed_result=embed_result, classification=classification,
        ingestion_method=IngestionMethod.HYBRID_SPLIT,
        chunks_created=total_chunks, tables_created=len(tables),
        tables=tables, written=written,
        errors=errors, warnings=warnings, error_details=error_details,
        processing_time_seconds=elapsed,
    )
```

## File 2: tests/test_splitter.py

### Test Organization

Following the test patterns from `test_serializer.py` and `test_structured_db.py`:

1. **Helper factories** at the top: `_make_sheet_profile()`, `_make_file_profile()`, `_make_parse_result()`, `_make_classification_stage_result()`, `_make_classification_result()` -- all with hybrid-appropriate defaults.
2. **Mock backends**: `MockVectorStore`, `MockEmbedder`, `MockStructuredDB` -- lightweight classes tracking calls.
3. **Mock openpyxl helpers**: `_make_mock_cell()`, `_make_mock_merged_range()`, `_make_mock_workbook()`.
4. **Fixtures**: `mock_vector_store`, `mock_embedder`, `mock_db`, `config`, `structured_processor`, `text_serializer`, `splitter`.

### Test Classes and Coverage

```
TestRegionDetection
    test_blank_rows_split_regions
    test_blank_cols_split_regions
    test_merged_block_detected_as_header
    test_formatting_transition_boundary
    test_header_detection_at_top
    test_footer_detection_at_bottom
    test_matrix_region_detected
    test_empty_sheet_no_regions
    test_single_data_block_one_region
    test_multiple_heuristics_combined

TestRegionClassification
    test_tabular_region_classified_as_type_a
    test_text_region_classified_as_type_b
    test_ambiguous_region_defaults_to_type_b
    test_classification_uses_config_thresholds

TestProcessFlow
    test_process_happy_path_mixed_regions
    test_process_single_sheet_with_regions
    test_process_ingestion_method_hybrid_split
    test_process_result_fields_populated
    test_process_shared_ingest_key
    test_process_region_id_in_metadata
    test_process_timing_recorded

TestRegionProcessing
    test_type_a_region_creates_table
    test_type_a_region_creates_chunks
    test_type_b_region_creates_chunks
    test_type_b_region_no_tables

TestWrittenArtifacts
    test_written_artifacts_merged_across_regions
    test_written_artifacts_vector_ids_populated
    test_written_artifacts_db_tables_from_type_a
    test_written_collection_name

TestEmbedding
    test_embed_result_populated
    test_embed_result_none_when_no_chunks
    test_embedding_batching_respected

TestSheetSkipping
    test_skips_hidden_sheet
    test_skips_chart_only_sheet
    test_skips_oversized_sheet
    test_skips_empty_regions
    test_skips_header_footer_regions

TestErrorHandling
    test_region_detection_error_recorded
    test_per_region_error_continues
    test_per_sheet_error_continues
    test_error_code_region_detect
    test_classify_backend_error_timeout
    test_classify_backend_error_connect
    test_classify_backend_error_default

TestMultiSheet
    test_multi_sheet_regions_detected_per_sheet
    test_multi_sheet_chunk_index_global
    test_multi_sheet_errors_accumulated

TestChunkMetadata
    test_metadata_region_id_set
    test_metadata_ingestion_method_hybrid_split
    test_metadata_tenant_id_propagated
    test_metadata_source_uri_format
    test_metadata_parser_used
    test_metadata_ingest_key_shared
```

### Key Test Scenarios

1. **Happy path**: Sheet with a tabular block and a text block separated by blank rows. Verify both are detected, classified, and processed. Check region_id is set differently for each region's chunks.

2. **Region detection**: Create mock worksheets with known structure (blank row separators, merged headers, formatting transitions) and verify correct SheetRegion objects are returned.

3. **Classification**: Provide regions with clear tabular signals (high numeric ratio, header, consistent columns) and verify TABULAR_DATA. Provide regions with document signals (merged cells, low consistency) and verify FORMATTED_DOCUMENT.

4. **Error resilience**: One region fails during processing; others still succeed. Verify error is recorded but processing continues.

5. **Metadata correctness**: Every chunk has region_id set, ingestion_method is "hybrid_split", tenant_id propagates, ingest_key is shared.

6. **WrittenArtifacts merging**: Type A region produces db_table_names entries, Type B produces only vector_point_ids. Verify both are in the merged WrittenArtifacts.

### Mock Backend: MockStructuredDB

```python
class MockStructuredDB:
    def __init__(self, uri: str = "sqlite:///test.db"):
        self._uri = uri
        self.tables_created: list[tuple[str, pd.DataFrame]] = []

    def create_table_from_dataframe(self, table_name: str, df: pd.DataFrame) -> None:
        self.tables_created.append((table_name, df))

    def drop_table(self, table_name: str) -> None:
        pass

    def table_exists(self, table_name: str) -> bool:
        return any(t[0] == table_name for t in self.tables_created)

    def get_table_schema(self, table_name: str) -> dict:
        return {}

    def get_connection_uri(self) -> str:
        return self._uri
```

### Test Fixture Setup

```python
@pytest.fixture()
def splitter(mock_vector_store, mock_embedder, mock_db, config):
    structured = StructuredDBProcessor(mock_db, mock_vector_store, mock_embedder, config)
    serializer = TextSerializer(mock_vector_store, mock_embedder, config)
    return HybridSplitter(structured, serializer, config)
```

## File 3: processors/__init__.py Changes

Add `HybridSplitter` import and export:

```python
"""Processing-path implementations for ingestkit-excel."""

from ingestkit_excel.processors.serializer import TextSerializer
from ingestkit_excel.processors.splitter import HybridSplitter
from ingestkit_excel.processors.structured_db import StructuredDBProcessor

__all__ = ["StructuredDBProcessor", "TextSerializer", "HybridSplitter"]
```

## File 4: __init__.py Changes

Add `HybridSplitter` to the import from processors and to `__all__`:

1. Add to import line:
   ```python
   from ingestkit_excel.processors import StructuredDBProcessor, TextSerializer, HybridSplitter
   ```

2. Add to `__all__` in the "Processors" section:
   ```python
   "HybridSplitter",
   ```

## Acceptance Criteria

- [ ] Blank row boundaries (>= 2) correctly detected
- [ ] Blank column boundaries (>= 2) correctly detected
- [ ] Merged cell header blocks identified as `HEADER_BLOCK`
- [ ] Formatting transitions detected as region boundaries
- [ ] Header/footer blocks detected at sheet top/bottom
- [ ] Matrix regions identified with `MATRIX_BLOCK`
- [ ] Per-region `detection_confidence` populated (0.0-1.0)
- [ ] Tabular regions classified as TABULAR_DATA using Tier 1 signals
- [ ] Text regions classified as FORMATTED_DOCUMENT using Tier 1 signals
- [ ] Tabular regions routed to Path A processing logic
- [ ] Text regions routed to Path B processing logic
- [ ] All outputs share same `ingest_key` with distinct `region_id`
- [ ] `region_id` set in ChunkMetadata for every Path C chunk
- [ ] `ingestion_method` is `IngestionMethod.HYBRID_SPLIT` on ProcessingResult
- [ ] Merged `WrittenArtifacts` from all region processing
- [ ] EmbedStageResult aggregated across regions
- [ ] Sheet skip logic matches Path A/B (hidden, chart-only, oversized)
- [ ] Per-region error handling (one failure doesn't block others)
- [ ] `E_PROCESS_REGION_DETECT` error code used for region detection failures
- [ ] Works with mock backends (no external services required)
- [ ] HybridSplitter exported from processors/__init__.py
- [ ] HybridSplitter exported from package __init__.py
- [ ] `pytest tests/test_splitter.py -q` passes
- [ ] No regressions in existing tests

## Verification Gates

```bash
# Run splitter tests only
pytest packages/ingestkit-excel/tests/test_splitter.py -v

# Run all tests (regression check)
pytest packages/ingestkit-excel/tests -v

# Run with coverage
pytest packages/ingestkit-excel/tests --cov=ingestkit_excel --cov-report=term-missing

# Import check
python -c "from ingestkit_excel import HybridSplitter; print('OK')"
python -c "from ingestkit_excel.processors import HybridSplitter; print('OK')"
```

AGENT_RETURN: plan-10-021226.md
