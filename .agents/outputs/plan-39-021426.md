---
issue: 39
agent: plan
date: 2026-02-14
complexity: COMPLEX
stack: [python, pydantic, pytest, ocr, multiprocessing]
title: "Implement Path B OCR Processor (processors/ocr_processor.py)"
depends_on: map-39-021426.md
---

# PLAN Artifact: Issue #39 -- Path B OCR Processor

## Executive Summary

This plan implements `OCRProcessor` in `processors/ocr_processor.py` (SPEC 11.2) -- the scanned-PDF processing path. The processor orchestrates 9 pipeline steps: page rendering, preprocessing, language detection, parallel OCR, result collection, postprocessing, optional LLM cleanup, low-confidence flagging, and downstream chunking/embedding/upsert. A module-level worker function handles ProcessPoolExecutor pickling constraints. Sequential fallback when `ocr_max_workers <= 1` enables straightforward unit testing. All utility modules are already implemented; this is purely orchestration + tests.

## File-by-File Implementation Plan

### File 1: `packages/ingestkit-pdf/src/ingestkit_pdf/processors/ocr_processor.py` (CREATE, ~420 lines)

#### Module-Level Worker Function: `_ocr_single_page()`

```python
def _ocr_single_page(
    file_path: str,
    page_number: int,      # 1-based
    ocr_dpi: int,
    preprocessing_steps: list[str],
    ocr_engine_name: str,  # "tesseract" or "paddleocr"
    ocr_language: str,     # ISO 639-1
    enable_language_detection: bool,
    default_language: str,
) -> OCRResult | tuple[int, str]:
```

**Design rationale:** Must be module-level (not a method) for `ProcessPoolExecutor` pickling. Receives only serializable primitives. Worker internally:
1. Opens PDF via `fitz.open(file_path)`, gets page `[page_number - 1]`
2. Creates `PageRenderer` with a minimal config (just `ocr_dpi` + `preprocessing_steps`)
3. Calls `renderer.render_page(page)` -> PIL Image
4. Calls `renderer.preprocess(image)` -> preprocessed PIL Image
5. If `enable_language_detection`, runs initial OCR for text sample, then `detect_language(sample_text)` -> `(lang, conf)`, then `map_language_to_ocr(lang, engine)` -> engine-specific lang
6. Creates OCR engine via `create_ocr_engine()` with a minimal config
7. Calls `engine.recognize(image, language)` -> `OCRPageResult`
8. Calls `postprocess_ocr_text(result.text)` -> cleaned text
9. Returns `OCRResult(page_number=page_number, text=cleaned_text, confidence=result.confidence, engine_used=OCREngine(ocr_engine_name), dpi=ocr_dpi, preprocessing_steps=preprocessing_steps, language_detected=lang_or_None)`

**Error handling:** Wraps everything in try/except. On failure, returns `(page_number, str(error))` tuple instead of raising (so the parent process can handle per-page errors without losing the future).

**Config reconstruction in worker:** The worker cannot receive a `PDFProcessorConfig` (Pydantic model may have issues with pickling across processes). Instead, it receives primitive arguments and constructs a minimal config: `PDFProcessorConfig(ocr_dpi=ocr_dpi, ocr_preprocessing_steps=preprocessing_steps, ocr_engine=OCREngine(ocr_engine_name), ocr_language=ocr_language)`.

#### Class: `OCRProcessor`

**Constructor:**
```python
def __init__(
    self,
    vector_store: VectorStoreBackend,
    embedder: EmbeddingBackend,
    llm: LLMBackend | None,
    config: PDFProcessorConfig,
) -> None:
```
Stores all four as private attributes. Logger: `logging.getLogger("ingestkit_pdf.processors.ocr_processor")`.

**Public method: `process()`**
```python
def process(
    self,
    file_path: str,
    profile: DocumentProfile,
    pages: list[int] | None,
    ingest_key: str,
    ingest_run_id: str,
) -> ProcessingResult:
```

**Implementation steps (numbered to match SPEC 11.2):**

**Step 0 -- Setup:**
- `start_time = time.monotonic()`
- Initialize accumulators: `errors: list[str]`, `warnings: list[str]`, `error_details: list[IngestError]`, `ocr_results: list[OCRResult]`, `written = WrittenArtifacts(vector_collection=config.default_collection)`
- `source_uri = f"file://{Path(file_path).resolve().as_posix()}"`

**Step 0.5 -- Page filtering via `_select_pages()`:**
- If `pages` is not None, use only those page numbers
- If `pages` is None, iterate `profile.pages` and include only pages whose `page_type` is `SCANNED` or `MIXED`
- Skip `BLANK` pages -> append `W_PAGE_SKIPPED_BLANK` warning
- Skip `TOC` pages -> append `W_PAGE_SKIPPED_TOC` warning
- Skip `VECTOR_ONLY` pages -> append `W_PAGE_SKIPPED_VECTOR_ONLY` warning
- `TEXT` pages: include them (they may have been routed here by OCR fallback)
- `TABLE_HEAVY`, `FORM`: include them (handle gracefully)
- Return `list[int]` of 1-based page numbers to process

**Steps 1-6 -- OCR via `_ocr_pages()`:**
- Delegates to `_ocr_pages_parallel()` or `_ocr_pages_sequential()` based on `config.ocr_max_workers`
- If `ocr_max_workers <= 1` or only 1 page: use sequential
- Otherwise: use parallel

**`_ocr_pages_parallel(page_numbers, file_path)` -> `list[OCRResult]`:**
- Creates `ProcessPoolExecutor(max_workers=config.ocr_max_workers)`
- Submits `_ocr_single_page(...)` for each page number
- Uses `as_completed()` with per-page timeout `config.ocr_per_page_timeout_seconds`
- For each completed future:
  - If result is `OCRResult`: append to results
  - If result is `(page_number, error_str)` tuple: record `E_OCR_FAILED` error, continue
  - If `TimeoutError`: record `E_OCR_TIMEOUT` for that page, continue
  - If other exception: record `E_OCR_FAILED`, continue
- Returns collected `OCRResult` list (may be fewer than pages submitted)

**`_ocr_pages_sequential(page_numbers, file_path)` -> `list[OCRResult]`:**
- Same logic but calls `_ocr_single_page()` directly in a loop
- Per-page timeout not enforced in sequential mode (simplification)
- Same error handling: catch exceptions per page, record error, continue

**Step 7 -- LLM cleanup via `_llm_cleanup(ocr_results)` -> `list[OCRResult]`:**
- If `config.enable_ocr_cleanup is False`: return results unchanged
- If `self._llm is None`: log warning "LLM cleanup enabled but no LLM backend provided", return unchanged
- For each `OCRResult` with non-empty text:
  - Build prompt: `"Fix obvious OCR errors in the following text while preserving its meaning and structure. Do not add, remove, or rephrase content. Return only the corrected text.\n\n{result.text}"`
  - Call `self._llm.generate(prompt=prompt, model=config.ocr_cleanup_model, temperature=0.1, timeout=config.backend_timeout_seconds)`
  - If response is non-empty: replace `result.text` with response
  - If LLM raises exception: log warning, keep original text, continue
- Return modified results

**Step 8 -- Low-confidence flagging via `_flag_low_confidence(ocr_results)`:**
- For each result where `result.confidence < config.ocr_confidence_threshold`:
  - Append `W_PAGE_LOW_OCR_CONFIDENCE` to warnings
  - Record in `low_confidence_pages` list

**Step 9 -- Downstream pipeline via `_chunk_and_embed(ocr_results, file_path, ...)`:**

Sub-step 9a -- Header/footer detection:
- Open PDF with `fitz.open(file_path)`
- `HeaderFooterDetector(config).detect(doc)` -> `(headers, footers)`
- For each OCR result, strip headers/footers: `detector.strip(result.text, result.page_number, headers, footers)`

Sub-step 9b -- Text assembly:
- Concatenate all page texts with `\n\n` separator
- Track `page_boundaries: list[int]` (character offset where each page starts)
- If all texts are empty: return early with zero chunks

Sub-step 9c -- Heading detection:
- `HeadingDetector(config).detect(doc)` -> `list[(level, title, page)]`
- Convert to `(level, title, char_offset)` format needed by chunker (map page-based headings to character offsets using page_boundaries)
- If no headings found: use empty list (expected for scanned PDFs)

Sub-step 9d -- Chunking:
- `PDFChunker(config).chunk(full_text, headings_with_offsets, page_boundaries)` -> `list[dict]`
- Each dict has: text, page_numbers, heading_path, content_type, chunk_index, chunk_hash

Sub-step 9e -- Build ChunkPayloads:
- For each chunk dict:
  - `chunk_id = str(uuid.uuid5(uuid.NAMESPACE_URL, f"{ingest_key}:{chunk['chunk_hash']}"))`
  - Compute average OCR confidence across the chunk's pages (from ocr_results matching page_numbers)
  - Build `PDFChunkMetadata`:
    - `source_uri`, `source_format="pdf"`, `ingestion_method=IngestionMethod.OCR_PIPELINE.value`
    - `parser_version=config.parser_version`, `chunk_index`, `chunk_hash`, `ingest_key`, `ingest_run_id`
    - `tenant_id=config.tenant_id`
    - `page_numbers=chunk["page_numbers"]`, `heading_path=chunk["heading_path"]`
    - `content_type=chunk["content_type"]`
    - `doc_title=profile.metadata.title`, `doc_author=profile.metadata.author`
    - `doc_date=profile.metadata.creation_date`
    - `ocr_engine=config.ocr_engine.value`, `ocr_confidence=avg_confidence`
    - `ocr_dpi=config.ocr_dpi`, `ocr_preprocessing=config.ocr_preprocessing_steps`
    - `language=detected_language` (from OCR results for those pages)
  - Create `ChunkPayload(id=chunk_id, text=chunk["text"], vector=[], metadata=metadata)`

Sub-step 9f -- Batch embed:
- `self._vector_store.ensure_collection(collection, self._embedder.dimension())`
- Process chunks in batches of `config.embedding_batch_size`:
  - `vectors = self._embedder.embed(texts, timeout=config.backend_timeout_seconds)`
  - Assign vectors to chunk payloads
  - Track embed duration
  - Track `total_texts_embedded`

Sub-step 9g -- Upsert:
- `self._vector_store.upsert_chunks(collection, batch_chunks)`
- Append chunk IDs to `written.vector_point_ids`

**Error handling for sub-steps 9e-9g:** Wrap embed + upsert in try/except per batch. On failure, record appropriate error code via `_classify_backend_error()`, continue with remaining batches.

**Final assembly:**
- Build `OCRStageResult`:
  - `pages_ocrd=len(ocr_results)`
  - `engine_used=config.ocr_engine`
  - `avg_confidence=mean(r.confidence for r in ocr_results)` or 0.0 if empty
  - `low_confidence_pages=low_confidence_page_numbers`
  - `ocr_duration_seconds=ocr_elapsed`
  - `engine_fallback_used=any("W_OCR_ENGINE_FALLBACK" in w for w in warnings)`
- Build `EmbedStageResult` if any embeddings were done
- Return `ProcessingResult` with all fields populated

**Note on `parse_result` and `classification_result`:** The `process()` signature does NOT receive these (unlike Excel's StructuredDBProcessor). The PDF `ProcessingResult` requires them. Two options:
- Option A: Add them as parameters to `process()` -- matches Excel pattern
- Option B: Build placeholder stubs inside the processor

**Decision:** Option A. Add `parse_result: ParseStageResult` and `classification_result: ClassificationStageResult` and `classification: ClassificationResult` as parameters to `process()`. This matches the Excel pattern exactly. The updated signature:

```python
def process(
    self,
    file_path: str,
    profile: DocumentProfile,
    pages: list[int] | None,
    ingest_key: str,
    ingest_run_id: str,
    parse_result: ParseStageResult,
    classification_result: ClassificationStageResult,
    classification: ClassificationResult,
) -> ProcessingResult:
```

**Private helper: `_classify_backend_error(exc)` -> `ErrorCode`:**
- Same pattern as Excel StructuredDBProcessor
- Maps timeout/connect exceptions to appropriate E_BACKEND_* codes
- Default: `E_OCR_FAILED`

---

### File 2: `packages/ingestkit-pdf/tests/test_ocr_processor.py` (CREATE, ~650 lines)

All tests use `@pytest.mark.unit`. All external dependencies are mocked (no real Tesseract, no real PDFs, no real fitz).

**Mock strategy:**
- Mock `fitz.open()` to return a mock document with mock pages
- Mock `PageRenderer` to return deterministic PIL images
- Mock `create_ocr_engine()` to return a mock engine
- Mock `postprocess_ocr_text()` to pass through or return fixed text
- Mock `detect_language()` to return `("en", 0.95)`
- Use `MockVectorStoreBackend`, `MockEmbeddingBackend`, updated `MockLLMBackend` from conftest
- Patch `_ocr_single_page` at module level for orchestration tests
- Test `_ocr_single_page` directly (with mocked internals) for unit tests

**Test classes and cases:**

1. **`TestOCRSinglePageWorker`** (~80 lines)
   - `test_successful_ocr_single_page`: Mock fitz, PageRenderer, engine -> returns OCRResult with correct fields
   - `test_worker_returns_error_tuple_on_failure`: Simulate engine crash -> returns (page_number, error_string)
   - `test_worker_with_language_detection`: Enable lang detection -> correct language in OCRResult
   - `test_worker_without_language_detection`: Disable -> uses default language

2. **`TestOCRProcessorInit`** (~20 lines)
   - `test_constructor_accepts_all_backends`: Verify stores backends + config
   - `test_constructor_accepts_none_llm`: LLM can be None

3. **`TestPageFiltering`** (~60 lines)
   - `test_filters_blank_pages_with_warning`: BLANK pages skipped, W_PAGE_SKIPPED_BLANK emitted
   - `test_filters_toc_pages_with_warning`: TOC pages skipped, W_PAGE_SKIPPED_TOC emitted
   - `test_filters_vector_only_with_warning`: VECTOR_ONLY skipped, W_PAGE_SKIPPED_VECTOR_ONLY emitted
   - `test_includes_scanned_and_mixed_pages`: SCANNED and MIXED pages processed
   - `test_explicit_pages_parameter`: Only specified page numbers processed
   - `test_text_pages_included_for_ocr_fallback`: TEXT pages are not skipped

4. **`TestSinglePageOCR`** (~60 lines)
   - `test_single_scanned_page_end_to_end`: 1 page -> OCR -> chunk in vector store, correct metadata
   - `test_chunk_metadata_has_ocr_fields`: Verify ocr_engine, ocr_confidence, ocr_dpi, ocr_preprocessing, ingestion_method="ocr_pipeline"
   - `test_source_uri_format`: Verify `file://` prefix

5. **`TestMultiPageOCR`** (~50 lines)
   - `test_multi_page_produces_correct_page_boundaries`: 3 pages -> correct page_numbers on chunks
   - `test_metadata_propagation_across_pages`: tenant_id, ingest_key, ingest_run_id on all chunks

6. **`TestSequentialVsParallel`** (~50 lines)
   - `test_sequential_when_max_workers_1`: Verify `_ocr_pages_sequential` called when workers=1
   - `test_sequential_when_single_page`: Even if workers>1, use sequential for 1 page
   - `test_parallel_path_selected`: Verify `_ocr_pages_parallel` called when workers>1 and multiple pages
   - Note: Actually testing ProcessPoolExecutor is an integration test concern. Unit tests mock/patch the parallel path.

7. **`TestPerPageErrorIsolation`** (~50 lines)
   - `test_page_ocr_failure_continues_remaining`: Page 2 fails, pages 1 and 3 succeed -> 2 OCRResults + 1 error
   - `test_timeout_error_records_E_OCR_TIMEOUT`: Simulated timeout -> E_OCR_TIMEOUT in errors
   - `test_general_failure_records_E_OCR_FAILED`: General exception -> E_OCR_FAILED
   - `test_all_pages_fail_produces_zero_chunks`: All pages fail -> ProcessingResult with chunks_created=0

8. **`TestLowConfidenceWarning`** (~30 lines)
   - `test_page_below_threshold_flagged`: confidence 0.5 < threshold 0.7 -> W_PAGE_LOW_OCR_CONFIDENCE
   - `test_page_above_threshold_not_flagged`: confidence 0.85 >= 0.7 -> no warning
   - `test_low_confidence_pages_in_ocr_stage_result`: Correct page numbers in `ocr_result.low_confidence_pages`

9. **`TestLLMCleanup`** (~60 lines)
   - `test_cleanup_called_when_enabled`: enable_ocr_cleanup=True, LLM mock returns cleaned text -> text replaced
   - `test_cleanup_skipped_when_disabled`: enable_ocr_cleanup=False -> LLM never called
   - `test_cleanup_skipped_when_llm_is_none`: llm=None, enable=True -> warning logged, text unchanged
   - `test_cleanup_failure_uses_original_text`: LLM raises -> original postprocessed text kept, warning
   - `test_cleanup_uses_correct_model`: Verify model=config.ocr_cleanup_model passed to generate()

10. **`TestOCRStageResult`** (~30 lines)
    - `test_stage_result_fields`: pages_ocrd, avg_confidence, engine_used match
    - `test_engine_fallback_tracked`: W_OCR_ENGINE_FALLBACK in warnings -> engine_fallback_used=True
    - `test_avg_confidence_calculation`: Multiple pages -> correct average

11. **`TestHeaderFooterStripping`** (~30 lines)
    - `test_headers_stripped_from_ocr_text`: Mock HeaderFooterDetector to return patterns -> text cleaned
    - `test_no_stripping_when_no_patterns`: Empty patterns -> text unchanged

12. **`TestHeadingDetection`** (~30 lines)
    - `test_headings_from_pdf_outline`: Mock HeadingDetector returns headings -> chunk heading_path populated
    - `test_no_headings_produces_empty_heading_path`: No headings -> heading_path is None or []

13. **`TestBatchEmbedding`** (~40 lines)
    - `test_chunks_embedded_in_batches`: 10 chunks, batch_size=3 -> 4 embed calls (3+3+3+1)
    - `test_ensure_collection_called`: Verify ensure_collection called before upsert

14. **`TestProcessingResult`** (~30 lines)
    - `test_full_result_assembly`: All fields present, correct types
    - `test_ingestion_method_is_ocr_pipeline`: ingestion_method == IngestionMethod.OCR_PIPELINE
    - `test_processing_time_recorded`: processing_time_seconds > 0

15. **`TestEngineUnavailable`** (~20 lines)
    - `test_engine_unavailable_error`: create_ocr_engine raises EngineUnavailableError -> E_OCR_ENGINE_UNAVAILABLE

16. **`TestEmptyOCROutput`** (~20 lines)
    - `test_all_pages_empty_text`: OCR returns empty text for all pages -> zero chunks, no embed calls
    - `test_no_pages_to_process`: All pages filtered out -> zero chunks

---

### File 3: `packages/ingestkit-pdf/tests/conftest.py` (MODIFY, ~60 lines added)

**Add `MockVectorStoreBackend`:**
```python
class MockVectorStoreBackend:
    def __init__(self):
        self.collections_ensured: list[tuple[str, int]] = []
        self.upserted: list[tuple[str, list]] = []  # (collection, chunks)
        self.payload_indices: list[tuple[str, str, str]] = []
        self.deleted: list[tuple[str, list[str]]] = []

    def upsert_chunks(self, collection, chunks):
        self.upserted.append((collection, list(chunks)))
        return len(chunks)

    def ensure_collection(self, collection, vector_size):
        self.collections_ensured.append((collection, vector_size))

    def create_payload_index(self, collection, field, field_type):
        self.payload_indices.append((collection, field, field_type))

    def delete_by_ids(self, collection, ids):
        self.deleted.append((collection, ids))
        return len(ids)
```

**Add `MockEmbeddingBackend`:**
```python
class MockEmbeddingBackend:
    def __init__(self, dimension: int = 768):
        self._dimension = dimension
        self.calls: list[list[str]] = []

    def embed(self, texts, timeout=None):
        self.calls.append(list(texts))
        return [[0.1] * self._dimension for _ in texts]

    def dimension(self):
        return self._dimension
```

**Update `MockLLMBackend.generate()`:**
Replace the `NotImplementedError` with functional mock behavior matching the `classify()` pattern:
```python
def generate(self, prompt, model, temperature=0.7, timeout=None):
    self.calls.append({"prompt": prompt, "model": model, "temperature": temperature, "timeout": timeout})
    if not self._responses:
        raise RuntimeError("MockLLMBackend: no more responses configured")
    response = self._responses.pop(0)
    if isinstance(response, Exception):
        raise response
    return str(response)
```

**Add fixtures:**
```python
@pytest.fixture()
def mock_vector_store():
    return MockVectorStoreBackend()

@pytest.fixture()
def mock_embedder():
    return MockEmbeddingBackend()

@pytest.fixture()
def mock_llm():
    return MockLLMBackend()
```

---

### File 4: `packages/ingestkit-pdf/src/ingestkit_pdf/processors/__init__.py` (MODIFY, ~3 lines)

Add export:
```python
from ingestkit_pdf.processors.ocr_processor import OCRProcessor

__all__ = ["OCRProcessor"]
```

---

### File 5: `packages/ingestkit-pdf/src/ingestkit_pdf/__init__.py` (MODIFY, ~3 lines)

Add `OCRProcessor` to public exports:
```python
from ingestkit_pdf.processors import OCRProcessor

__all__ = [
    "LLMClassificationResponse",
    "PDFLLMClassifier",
    "OCRProcessor",
]
```

---

## Implementation Order

1. **conftest.py** -- Add mock backends first (tests depend on them)
2. **ocr_processor.py** -- Core implementation
3. **processors/__init__.py** -- Export
4. **__init__.py** -- Public export
5. **test_ocr_processor.py** -- Full test suite

## Key Design Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| Worker function location | Module-level `_ocr_single_page()` | ProcessPoolExecutor requires picklable callables |
| Sequential fallback | `ocr_max_workers <= 1` triggers `_ocr_pages_sequential()` | Enables unit testing without multiprocessing |
| Error return from worker | `OCRResult \| tuple[int, str]` union | Avoids exception propagation across process boundaries |
| Config in worker | Reconstruct from primitives | PDFProcessorConfig may not pickle cleanly across processes |
| LLM cleanup method | `LLMBackend.generate()` (not `classify()`) | Returns free text, not JSON |
| process() signature | Includes parse_result, classification_result, classification | Matches Excel StructuredDBProcessor pattern; ProcessingResult requires these fields |
| Heading detection for scanned PDFs | Attempt PDF outline first, accept empty | Scanned PDFs rarely have heading metadata |
| Language detection source | First OCR pass gives text sample, then detect_language() on it | Cannot detect language from image alone; need text first |

## Acceptance Criteria

- [ ] `OCRProcessor` class with correct constructor signature (SPEC 11.2)
- [ ] `process()` returns fully-assembled `ProcessingResult` with all fields
- [ ] Module-level `_ocr_single_page()` worker handles render, preprocess, OCR, postprocess
- [ ] Parallel OCR via ProcessPoolExecutor when `ocr_max_workers > 1`
- [ ] Sequential fallback when `ocr_max_workers <= 1`
- [ ] Per-page error isolation: single page failure does not abort document
- [ ] Per-page timeout handling with `E_OCR_TIMEOUT`
- [ ] Page filtering: skips BLANK/TOC/VECTOR_ONLY with correct warnings
- [ ] Language detection per page when `enable_language_detection=True`
- [ ] OCR postprocessing via `postprocess_ocr_text()`
- [ ] LLM cleanup when `enable_ocr_cleanup=True` via `generate()`
- [ ] LLM cleanup graceful degradation (None LLM, LLM failure)
- [ ] Low-confidence pages flagged with `W_PAGE_LOW_OCR_CONFIDENCE`
- [ ] `OCRStageResult` with correct aggregated metrics
- [ ] Header/footer detection and stripping on OCR text
- [ ] Heading detection attempted (outline strategy)
- [ ] Chunking via `PDFChunker` with page boundaries
- [ ] `PDFChunkMetadata` with all OCR-specific fields
- [ ] Batch embedding with `embedding_batch_size`
- [ ] Vector upsert with `WrittenArtifacts` tracking
- [ ] `ingestion_method = IngestionMethod.OCR_PIPELINE`
- [ ] `tenant_id` propagated through all metadata
- [ ] PII-safe logging (no raw OCR text unless `log_ocr_output=True`)
- [ ] `OCRProcessor` exported from `processors/__init__.py` and `__init__.py`
- [ ] All unit tests pass, all mocked (no external dependencies)
- [ ] Mock backends added to conftest.py (MockVectorStoreBackend, MockEmbeddingBackend)
- [ ] `MockLLMBackend.generate()` functional (not NotImplementedError)

## Verification Gates (for PROVE)

```bash
# Unit tests
pytest packages/ingestkit-pdf/tests/test_ocr_processor.py -v -m unit

# All existing tests still pass
pytest packages/ingestkit-pdf/tests/ -v

# Import check
python -c "from ingestkit_pdf import OCRProcessor; print('OK')"
python -c "from ingestkit_pdf.processors import OCRProcessor; print('OK')"
```

AGENT_RETURN: .agents/outputs/plan-39-021426.md
